"70701035","DODO BRANDS (Dodo Pizza, Drinkit, Doner 42)","Data Engineer Lead","True","280000","350000","Более 6 лет","Удаленная работа","['Python', 'Spark', 'SaaS', 'Databricks', 'Data Analysis']","Dodo Engineering - это IT-подразделение компании Dodo Brands. У нас 3 бренда:  Додо Пицца: более 830 пиццерий в 16 странах Дринкит: 5 кофеен в Москве и Самаре Донер 42: 6 донерных в Москве, Краснодаре, Казани и, конечно, в Санкт-Петербурге.  Хотим предложить тебе прокачать наш проект сквозной аналитики - аналитики на основе всех данных из всех источников. Мы хотим лучше понимать, как затраты на рекламные кампании конвертируются в продажи и новых клиентов. Выводы, которые мы получим благодаря сквозной аналитике, повлияют на все маркетинговые коммуникации, — а значит в итоге повлияют на привлечение клиентов и бизнес-результат всего Додо.Задачи проекта:  Помочь маркетингу стать действительно дата-дривен и прозрачным для всех, Оптимизировать свою работу и расходы, Отслеживать эффективность каналов и прочих используемых подходов (например, активностей CVM), Вдобавок научиться агрегировать региональные рекламные активности партнёров и видеть полную картину, Ну и по пути поднять выручку за счет оптимизации расходов и более эффективного привлечения клиентов.  Требования:  Опыт разработки с амбициями и желанием в Big Data; Хорошее знание баз данных; Знание устройства Azure, чтобы разбираться с доступами и окружениями; Умение менеджерить себя и других;  Нужно будет работать с внешними SaaS-решениями для сбора маркетинговых кампаний. Наш внутренний стек Data платформы: Spark, BI-инструменты, Python, Databricks Jobs. Условия:  Удаленная работа; Стабильная, официальная заработная плата. Финальную сумму обсуждаем с каждым кандидатом; ДМС; Оплату профильных конференций; Оплату профильных курсов; Покупку профессиональной литературы; Скидку на уроки английского языка в Skyeng; Митапы, лекции, воркшопы и интенсивы по твоему направлению; Прокачку навыков публичных выступлений (сделаем из тебя крутого спикера); Помощь в написании профессиональных статей и раскрутку тебя, как автора, на профильных ресурсах (Хабр, VC). "
"70735115","Mindbox","Senior Data Scientist / ML Engineer","True","300000","450000","От 1 года до 3 лет","Гибкий график","['Docker', 'Python', 'SQL', 'Pandas', 'Numpy', 'Scipy', 'Scikit', 'Lightgbm', 'Fastcluster', 'Implicit', 'Spacy']","Mindbox — крупнейшая в России облачная платформа автоматизации маркетинга, в десятке B2B SaaS России. Помогаем компаниям — от ДоДо Пиццы и МВидео до ПИК и МИФ расти за счет эффективного маркетинга. Без спама. Ищем data science инженера с опытом полного цикла разработки моделей. Предстоит вместе с ML-лидом построить команду и конвейер моделей, снижающих спам за счет предсказания поведения и look alike, рекомендующих полезное, автоматически сегментирующих и любых других, если они зарабатывают клиентам деньги. Также, помогать развивать функционал и математику А-Б тестирования. Ожидаем, что вы разбираетесь в бизнесе, тщательно подбираете фичи, стремитесь к explainable ML. Проектируете и анализируете А-Б тесты. Совместно с data-инженерами разрабатываете и сопровождаете модели в проде.У нас Десять минут от правки до прода, сотни А-Б тестов параллельно, 100+ TB чистых данных из десятков индустрий, топовые технологии и большая нагрузка. Особенная культура: открытые финансы и зарплаты, право принять любое решение у каждого. А значит — прямая обратная связь от клиентов, возможность выбирать технологии и влиять на бэклог. Детали  DS-stack: docker, python 3.7, sql, pandas, scipy, numpy, scikit, lightgbm, fastcluster, implicit, spacy. Прод на Airflow over K8S Нейронки, контент-бейзд рекомендации, кросс-доменные модели, многорукие бандиты 115+ человек в разработке, есть очень умные люди, с кем поговорить (МФТИ, МГУ, Бауманка, ВШЭ). Докладчики DotNext, Yandex.Scale, AgileDays (подробнее об инженерных командах: https://jobs.mindbox.ru/dev/) 2000 пересчетов моделей в день, 2000 RPS сервиса рекомендаций. 50 миллионов транзакций в день, 600+ миллионов профилей в базах, пятьдесят миллиардов фактов.  Условия Удаленно / гибрид / офис в Москве или Ереване. Релокация, бонус за выход, опцион.  Офисы для людей: еда, кухня, комнаты по 6-10 человек, комнаты для зума, забота и комфорт Десктоп i9 с SSD или MacBook Pro, 2x4K монитора. В т.ч. для удаленных Спорт, квесты, настолки, ЧГК, VR и караоке Безлимитные отпуска, саббатикал, болезни без справок, свободный график 300 000 ₽ в год на софинансирование образования, медицины, спорта, путешествий. Книги бесплатно "
"69811783","Апптимизм","Data Engineer","False","None","None","Нет опыта","Удаленная работа","['SQL', 'Python', 'SCALA', 'Hadoop', 'Kafka']","Крупнейшая компания FMCG в поиске Data Engineer. Сама техническая команда проекта небольшая. Планируется в 3 DS и 1 DE. Пока только 1 DS lead. Но внутри компании есть еще DS. Обязанности:  Дизайн и разработка витрин в Azure; Дизайн, разработка и поддержка ETL процессов для загрузки данных в/из Data Lake; Контроль качества и полноты данных (ручные и автоматические DQ тесты); Интеграции новых источников с DataLake; Написание документаций; Работать в связке с аналитиками и экспертами для получения end-to-end продукта    Мы ожидаем:  Знание принципов построения DWH и баз данных; Опыт работы с технологией Hadoop (HDInsight, Spark, Hive, Scala, etc.); Уверенное знание SQL, Python/Scala or Java; 1+ лет опыта с Azure / Yandex Cloud Platform; Опыт работы с Greenplum будет плюсом; Опыт работы с Nifi, Kafka, Airflow будет плюсом; "
"69658229","Антиплагиат","Data Engineer (Middle)","True","130000","180000","От 1 года до 3 лет","Полный день","['Работа в команде', 'Python', 'SQL', 'NoSQL', 'Работа с базами данных', 'работа с данными']","Компания «Антиплагиат» занимается разработкой корпоративных продуктов по анализу текстов, в том числе крупнейшего в стране одноименного продукта по поиску текстовых заимствований в студенческих работах.Задачи:• Писать краулеры сайтов;• Разрабатывать службы для получения данных из клиентских API;• Писать сервисы для внутренних нужд команды;• Поддерживать полноту и целостность текущих данных системы. Основные требования:• Уверенное знание Python;• Опыт работы с большими данными;• Умение обернуть написанный код в Docker-контейнер;• Опыт работы с SQL/NoSQL БД;• Знание Linux. Используем debian/ubuntu;• Английский достаточный для чтения технической литературы.• Высшее техническое образование; Плюсик в карму за:- Опыт планирования микросервисной/распределенной инфраструктуры;- Опыт работы с облаками (AWS, Яндекс.Облако);- Знакомство с C#;- Знание технологий из стека, указанного ниже. С чем придется работать:- Библиотеки Python: scrapy, beautiful soup, pandas;- Взаимодействие с сервисами по протоколам: gRPC, REST, FTP, etc;- Обработка данных в форматах: xml/html, json, plain text, etc;- Инфраструктура: docker, mongodb, rabbitmq, postgresql, redis;- Мониторинг: graphite, grafana, zabbix, ELK. Мы предлагаем: • Работу в компании, использующей современные подходы и технологии в разработке ПО;• Оформление согласно ТК РФ и официальная система оплаты труда (оклад + годовой бонус);• Уровень заработной платы обсуждается индивидуально;• Добровольное Медицинское Страхование;• Полную «удалёнку» (Комфортный офис м. Нагатинская\ мцк Верхние Котлы у нас тоже есть);• Возможность профессионального и карьерного роста;• Оплату курсов, тренингов и конференций;• 4 дополнительных выходных дня в течение года."
"69464622","Offer Now","Data Engineer (Middle/Senior)","True","None","400000","От 1 года до 3 лет","Полный день","[]","Обязанности:  Разрабатывать и поддерживать микросервисы Разрабатывать и оптимизировать пайплайны Разработка мониторинга и алертинга  Требования:  Python уровня senior Sql на уровне сложных запросов Опыт работы с luigi/airflow Опыт работы с aws Опыт работы с hadoop Опыт работы с clickhouse Высшее образование - Математическое/Техническое  Условия:  Оформление по ТК РФ Конкурентная заработная плата Удаленный формат работы Возможности для роста в компании "
"70739897","Синергетик","Data engineer/инженер данных/дата-инженер","True","150000","None","От 1 года до 3 лет","Полный день","['Python', 'Hadoop', 'SQL', 'ETL', 'PostgreSQL', 'Опытный пользователь ПК', 'Умение принимать решения', 'MySQL']","Synergetic – лидер российского рынка по производству экологичных товаров бытовой химии. Наша продукция представлена во многих странах мира. Мы постоянно работаем над новыми направлениями и следующий шаг для нас - выпуск косметических продуктов. Мы планируем создать множество косметических средств, для этого нам необходим опытный технолог-разработчик косметических продуктов с опытом внедрения продуктов на производственную линию. Если Вы ищете работу в уютном офисе и дружном коллективе, в надежной развивающейся компании - эта вакансия для Вас!   Обязанности:    Построение ETL/ELT процессов (Nifi), решение задач по организации сбора данных из различных источников в Data Lake/DWH Организация хранения данных с использованием инфраструктуры Hadoop (HDFS, Hive), оптимизация хранимых данных, Разработка и автоматизация процессов преобразования данных внутри хранилища Сборка витрин данных (Clickhouse, PostgreSQL) Построение визуализационных моделей данных, отчетов и дашбордов (Grafana, Zeppelin, Jupyter, PowerBI)  Требования:    Хорошие знания SQL, опыт работы с одной из реляционной БД - PostgreSQL/MySQL/MS SQL Понимание принципов проектирования хранилищ данных Опыт работы с системами визуализации данных (Grafana, PowerBI, Zeppelin) Базовые знания Python  Будет плюсом:  Наличие опыта работы с экосистемой Hadoop (HDFS, Hive, HBase, Yarn, Kafka, NiFi, Airflow); Навыки работы с Unix shell, git Опыт работы с гибкими методологиями и инструментами разработки (Agile) Опыт работы с не реляционными БД    Условия  Белая заработная плата и трудоустройство согласно ТК РФ Уютный офис возле м. Цветной Бульвар/Трубная Обучение, конференции, выставки — по желанию; Молодой и интересный коллектив; "
"69710334","Aviasales.ru","Data Engineer","False","None","None","От 3 до 6 лет","Полный день","[]","Наша команда инфраструктуры аналитики помогает аналитикам делать свою работу. Мы обеспечиваем доставку данных в единое хранилище для аналитики и поддерживаем инструменты для их обработки и презентации. В компании много самодостаточных команд. В каждой команде свои сервисы, свои разработчики и аналитики. Так что у нас достаточно потоков данных самого разнообразного формата и объема. Это данные о букингах, на которые смотрит вся компания, и каждая запись проверяется чуть ли не руками, а еще — данные о билетах объемом более терабайта в сутки. Это требует разных подходов с нашей стороны. Стек: Мы используем Apache Impala на базе Apache Hadoop для хранения и обработки данных. Основной источник данных – Kafka. Для доставки и оркестрации данных мы используем самописные сервисы на Python 3 + PostgreSQL. Аналитики строят отчеты в Redash и Apache Superset. Языки программирования: Основной язык — это Python 3.8. Важно знать SQL, он нужен для Impala и PostgreSQL. У нашего сервиса есть админка, она написана на TS (React) + MobX. Иногда полезно знание С++, Scala, Go, чтобы посмотреть на исходный код используемых сервисов, как open source, так и тех, что пишут другие команды в компании. Что нужно будет делать:  Основная задача группы инфраструктуры аналитики — поддерживать ее работоспособность. В это входит много разных задач:   подключение новых данных; мониторинг актуальности; исследование производительности отдельных запросов и базы в целом; разработка инструментов для запуска агрегатов разных форматов и так далее;     есть пара хранилищ, которые оптимизированы для каких-то специальных целей. Например для команды саппорта. Там основная задача в том, чтобы данные не расходились с основным хранилищем. Поддерживаем open source продукты, которые связаны с аналитикой; Superset и Redash для отчетов. Snowplow для сбора клиентских событий.     Основной код, с которым надо работать — это репозиторий для софта, который выполняет доставку данных в кластер и последующую оркестрацию задач. Там порядка 40 тысяч строк на Python 3.8 и 3 тысячи на TypeScript (это админка). Критическая функциональность покрыта тестами, и юнит и приемочными. 80% кода покрыто тестами. Тестирование и сборка производятся на CI сервере (Jenkins). Деплой в Kubernetes. Настройкой серверов и разворачиванием баз мы не занимаемся. Это делает отдельная команда инфраструктуры. Поступающие задачи можно поделить на две группы: текучка и развитие платформы.   Текучка бывает разная:   Нужно подключить новые данные или изменить формат уже подключенных. Нужно помочь разобраться, почему не работает SQL-запрос или BI-система.     Произошел какой-то инцидент и нужно найти корень проблемы и починить или донести до тех, кто может починить. Как правило такие штуки появляются как запросы через Slack, которые мы регистрируем в JIRA и выполняем обычно в течение одного-двух дней.     Развитие платформы — это проекты от двух недель до нескольких месяцев, которые дают какие-то новые возможности: бэкапы в AWS S3 Glacier; оптимизированные агрегаты; автоматическая выгрузка результатов расчетов во внешние базы. Большая часть таких задач приходит от команды аналитики. Сейчас мы планируем переезд на более современный стек. Но мы много выделяем времени на задачи, которые уменьшают количество текучки или облегчают её выполнение.   Что мы ждем от тебя:    опыт работы инженером данных от 3 лет;   отличное знание Python и SQL;   опыт с AWS или опыт с Apache Impala.     Что мы предлагаем:    сильную команду и возможность влиять как на технологические, так и на продуктовые решения;   возможность выбрать, где работать — удаленно или в одном из наших офисов (Пхукет, Москва или Петербург);   заботу о здоровье: компенсацию индивидуальной психотерапии, медицинскую страховку для тебя и твоей семьи;   поддержку твоих увлечений: компенсацию занятий спортом и изучения иностранных языков.  "
"70645700","QIWI","Data инженер","False","None","None","От 1 года до 3 лет","Полный день","['Spark', 'Hadoop', 'Python', 'SQL', 'dwh', 'airflow']","Ищем Middle Data Engineer в нашу команду Big data. Мы занимаемся построением единого хранилища данных (миллиарды транзакций) и формированием витрин для построения бизнес-моделей, направленных на увеличение прибыли компании. Наш стек технологий: Hadoop, Spark, Airflow, Cassandra, Kubernetes, Vertica, Oracle, Postgres и пишем на языках Python, Scala, Java. В команде BigData 2 data инженера, надеемся, ты станешь третьим).Чем предстоит заниматься:   Разработка архитектуры доставки, хранения и обработки данных вместе с техническими лидами проекта;   Разработка новых моделей данных;   Разработка и поддержка автоматизированных регулярных ETL процессов;   Разработка микросервисов, используя модели, подготовленные ML разработчиками.   Ожидаем от тебя:   Знаешь Python. Коммерческий опыт в приоритете, но и классные pet-проекты не исключаем;   Умеешь писать и оптимизировать сложные SQL запросы;   У тебя есть желание развиваться в сфере проектов с ML и Big Data;   Будет преимуществом:  Опыт работы с большими данными от года; Знания в области Big Data (Hadoop, Spark, Airflow, Kafka, etc); Знание Java или Scala; Навыки работы с Docker, Kubernetes;  Условия:  Оформление с первого дня по ТК, современная техника, быстрая настройка доступов. Удаленка, гибрид или работа в комфортном офисе - выбирайте сами. Офис у метро Чертановская 24/7. Ищем человека на 40-часовую рабочую неделю. Совмещать с учебой будет крайне сложно. Забота о сотрудниках в виде расширенного ДМС, страховки, английского, фитнеса, мобильной связи, доступов к библиотекам, персонального бюджета на обучение и не только. Возможность участвовать в митапах, хакатонах, конференциях QIWI. Уникальную атмосферу для продуктивной работы и развивающую среду, где можно найти единомышленников и научиться новому. Никакой бюрократии и важных боссов :-).  Этапы собеседования:1. Короткая беседа с рекрутером. Расскажу о нас, познакомлюсь с тобой, договоримся об основном собеседовании. Обычно это 30 минут.2. Техническое интервью на 90 минут. Обсудим твой опыт, интересы, порешаем типовые для этой вакансии задачки.3. Оффер."
"69633190","МТС","Middle Data Engineer (Process Mining)","False","None","None","От 1 года до 3 лет","Полный день","[]","МТС Digital – сердце цифровой экосистемы МТС. Облачные сервисы, суперкомпьютер, системы видеоаналитики, IoT, собственная лаборатория AI и 20+ петабайт данных, финтех, стриминг, гейминг, мобильные приложения. Каждый день мы работаем над тем, чтобы вывести мобильную и веб-разработку на новый уровень, благодаря сплоченным продуктовым командам и agile методологиям.Сейчас мы в поиске Data engineer на проект Process Mining.Process Mining - технология для повышения эффективности массовых, сложных, транзакционных бизнес-процессов, оказывающих существенное прямое влияние на бизнес компании. Система базируется на инструменте Celonis. Чем предстоит заниматься:   разрабатывать решения для пакетной и потоковой выгрузки данных из корпоративных систем в аналитическое хранилище (AirFlow); разрабатывать логическую и физическую модель ХД, готовить структуры данных, обрабатывать внутри ХД (DDL, DML); реализовывать средства мониторинга и автоматического тестирования разработанных компонентов; формировать требования для проверки данных (DQ), разрабатывать средства мониторинга и информирования о качестве данных.  Что мы ждем от кандидата:  владение Linux на базовом уровне; знание языков Python (уверенно); опыт работы с Greenplum; знание языков запросов БД (SQL) на продвинутом уровне; опыт работы с Docker (написание Dockerfile, docker-compose); опыт работы с Apache Airflow (разработка DAG-ов для загрузки данных в реляционное ХД, мониторинг); опыт работы с MPP СУБД (разработка структуры, обработка данных).  Будет плюсом:  опыт работы с одним из инструментов CI/CD (GitLab, Jenkins); опыт работы с Kafka; опыт использования инструментов управления конфигурацией (Ansible); опыт работы с различными инструментами Process Mining.  Что мы предлагаем:  собственную платформу MTS Ocean для получения ИТ-ресурсов, а это значит, что деплой, мониторинг, observability - не будут для вас проблемой, вы сможете сосредоточиться на фичах; профессиональные гильдии инженеров по направлениям, чтобы поддерживать друг друга и обмениваться опытом; внутреннюю площадку TechTalks для обмена опытом, дискуссий, развития навыков самопрезентации; участие во внешних IT конференциях. Мы выступаем на HighLoad++, DataFest, Mobius, Test Driven Conf, Joker, DevOps, Матемаркетинг и даже проводим собственную конференцию по архитектуре Hello, conference! полезные курсы и вебинары в корпоративном университете и электронные библиотеки.  А еще:  медицинскую страховку с 1 месяца со 100% покрытием расходов, включая стоматологию, страхование жизни и здоровья в поездках за рубеж. А еще можно застраховать родственников с корпоративной скидкой; доступ к сервису «Понимаю»: онлайн-консультации с психологом, юристом, экспертом по финансам или ЗОЖ; корпоративный и командный психолог в офисе и массажный кабинет;  единую подписку МТС Premium — KION light в онлайн-кинотеатре KION, сервис МТС Music, 30 дней бесплатного пользования подпиской OZON Premium; скидки и предложения от партнеров на фитнес, занятия английским и прочее.   "
"69475611","КИБЕР-РОМ","Data Engineer","True","200000","None","От 1 года до 3 лет","Полный день","['Python', 'ClickHouse', 'Elasticsearch', 'Data Analysis']","КИБЕР-РОМ — это неформальная обстановка, крутые ИТ-проекты в области медиаиндустрии и продукты, конкурирующие с лидерами рынка! У нас есть команда, которая создает ML решения, аналогов которых нет на рынке! Спроси себя, хочешь ли ты:  обрабатывать миллионы видео со стриминговых сервисов предоставлять пользователям релевантный контент искать структуры в неструктурированных данных бороться за каждую миллисекунду скорости делать продукт, который сделает лучше пользовательский опыт  Наша команда растет и мы ищем Mid+/Senior Data инженера, который сможет внести свой вклад в построении обработки данных для ML задач.В твоих силах сделать продукты, используя Hi-End решения, которыми воспользуются миллионы пользователей!BigData + Machine Learning + HighLoad &gt; Это про нас.Чем предстоит заниматься:  Построение высоконагруженных систем хранения больших данных; Выстраивание ETL процессов; Проектирование витрин данных; Выстраивание Pipeline разметки данных.  Что ожидаем от Вас:  Промышленный опыт работы с реалиционными базами данных (PostgreSQL / MySQL); Промышленный опыт работы c NoSQL Базами данных (ClickHouse, ElasticSearch, Redis); Опыт работы с очередями (Kafka / RabbitMQ); Уверенное владение Python; Опыт работы с AirFlow/Luigi; Уверенное владение PySpark.  Будет плюсом:  Опыт разворачивания ETL инфраструктуры; Уверенное владение Docker и промышленный опыт работы с K8S / Openshift / Heroku.  Бенефиты:  Стильный просторный лофт на территории Трехгорной мануфактуры. Тебе предстоит легкая 10 минутная пешая прогулка через парки от метро 1905 года и Краснопресненская; Сhill Lounge с пятой плойкой и капсулой для сна с массажем; Современное топовое оборудование, мощные ноутбуки; Комфортная кухня с вкусным кофе, чаем, какао, орешками, фруктами, снэками и прочими ништяками; Холодильник с колой и энергетиками; По пятницам в офисе пицца, роллы или грузинская кухня; Оформление по ТК РФ, конкурентная заработная плата; График работы 5\2, гибкое начало дня с 8-11 до 17-20 в офисе (возможен гибридный график работы); ДМС со стоматологией и госпитализацией в классных клиниках; Профессиональное обучение и конференции по запросу сотрудника; Годовая подписка на топовый онлайн кинотеатр.  Присутствие в офисе важно, рассматриваем гибридный вариант или полный офис.Готов создавать медиа сервисы будущего? Готов брать на себя ответственность за сложные вызовы? Если да, ждем твое резюме, рады будем видеть тебя в команде!    "
"69574814","Data Driven Lab","Data Engineer","False","None","None","От 1 года до 3 лет","Удаленная работа","['Python', 'SQL', 'SCALA', 'Databricks']","О нас: Data Driven Lab – высокотехнологичная продуктовая компания с головным офисом в Белграде. Сегодня наша команда насчитывает более 450 человек в 15 странах. Основные регионы присутствия DDL – Азия, Южная Америка, Африка, Европа и Океания. Мы используем передовые практики и реализуем масштабные проекты: наши продукты переведены на 21 язык и представлены более чем в 150 странах. Наши решения основаны на анализе данных, исследованиях и стремлении к тому, чтобы стать еще эффективнее. В своей работе мы используем современные аналитические инструменты, прогрессивные методы, передовые технологии и ML. Тебе предстоит:  Проектирование решения DWH / DataLake (на базе облачного Databricks), участие в проработке архитектуры и логики сервисов сбора данных совместно с командами разработки и аналитики Создание интеграционных сценариев для сбора, хранения, обработки, очистки и обогащения данных. Желательно иметь опыт с полным циклом работы с данными (CRISP-DM) Валидация, оптимизация и внедрение моделей расчета аналитических показателей Создание и поддержка ETL-процессов, настройка сопутствующего мониторинга и алертинга, контроль качества доставки данных Описание моделей данных, их происхождения; составление сопутствующей документации Контроль доступа к данным и мониторинг безопасности данных в облачном хранилище Разработка механизмов нормализации очистки/дедупликации необработанных данных на этапах ETL-трансформации или загрузки данных в хранилище Поддержка по части технических проблем и потребностей, связанных с данными облачного хранилища Активное взаимодействие с разными командами аналитиков внутри компании  Основная задача проекта: построение прогнозных моделей вероятностиконверсий в депозиты и размеров Revenue, LTV и CAC, а также других аналитическихрешений для финансовых KPI компании. Мы ожидаем от тебя:  Релевантный опыт работы от 2х лет в роли Data engineer Опыт работы с облачными решениями (GCP/Azure) и их администрирование (DevOps, Solution Architect) Опыт работы с базами данных и хранилищами широкого профиля (PostgreSQL, MySQL, MongoDB, Elasticsearch, Greenplum) Опыт применения языков программирования (Scala, Python) для обработки больших массивов данных (Jupyter Notebooks, Scala, Hadoop,BigQuery) Опыт преобразования сырых данных (событийных) согласно бизнес-логике заказчика в табличные данные для аналитики Экспертный уровень владения языком запросов SQL Опыт работы с брокерами сообщений (Kafka, RabbitMQ), понимание принципов построения отказоустойчивых кластеров Опыт построения и оптимизации ETL-процессов, включающих множество трансформаций Творческий и аналитический подход с сильными способностями к самостоятельному решению проблем с минимальным контролем со стороны руководителя  С каким стеком предстоит работать:   Databricks с интеграцией в инфраструктуру GCP PostgreSQL Apache Airflow, DBT Kafka, RabbitMQ Scala, Python, SQL Jira, Confluence, Git  Мы предлагаем:  Релокацию. Возможность релокации в Сербию (на нас: билеты, аренда жилья, подъемные, полное сопровождение официального оформления). Стабильность. Конкурентная з/п в евро, продвинутая система бенефитов (медицина, спорт). Развитие экспертизы. Возможности для нетворкинга, обмена опытом, внутреннего и внешнего обучения за счет компании.Возможность писать статьи и получать за это вознаграждение, выступать на профильных мероприятиях. Возможность влиять на результат. Отсутствие бюрократии и необходимости большого количества согласований. Можно легко повлиять на развитие и выстраивание глобальных процессов. Комфортные условия. Гибридный график – можно совмещать работу в офисе с удаленкой.   "
"70737845","ИЦ АЙ-ТЕКО","Data Engineer","False","None","None","От 1 года до 3 лет","Удаленная работа","['SQL', 'Hadoop', 'ETL']","Компания «АЙ-ТЕКО» — ведущий российский системный интегратор и поставщик информационных технологий для корпоративных заказчиков. Активно действует на рынке IT России с 1997 года, входит в ТОП-400 крупнейших российских компаний, ТОП-10 крупнейших IT-компаний России. Компания аккредитована Минцифры. В связи с активным развитием крупных, долгоиграющих проекта Сбера мы ищем DATA-РАЗРАБОТЧИКА (MIDDLE). Чем мы занимаемся: Аналитический инструмент для управления ресурсами Банка в периметре Agile. Основная задача инструмента — приоритизация продуктов Банка, с целью эффективного перераспределения ресурсов между продуктами (определения продуктов с нехваткой ресурсов и продуктов с излишком ресурсов). Целевая аудитория инструмента: топ-менеджмент, профильные сотрудники департамента финансов / департамента стратегии и развития, штабы трайбов. Задачи:  Подготовка витрины данных, создание выборок для обучения и тестирования моделей Организация процесса сбора данных из систем-источников (анализ атрибутного состава, качества данных, выбор систем-источников, выбор методов загрузки, реализация механизмов загрузки) Поддержка процесса наполнения витрины данными из систем-источников Разработка алгоритмов трансформации и очистки данных Проектирование логической/физической модели данных в СУБД  Наши ожидания от кандидата:  Опыт работы с перечисленными СУБД от 5-и лет Отличные знания SQL (SQL/PL/pgSQL) Знание принципов построения реляционных баз данных и витрин данных Опыт проектирования моделей хранения данных в СУБД Будет плюсом опыт работы с большими данными (Hadoop, Hive, Impala) Обязательно опыт работы c СУБД PostgreSQL Опыт работы с инструментами ETL (Informatica) Высшее техническое образование  УСЛОВИЯ:  Работа в стабильной компании, белая заработная плата График работы 5/2, фулл-тайм в комфортном офисе на Кутузовском проспекте с дальнейшим переходом на гибрид после прохождения испытательного срока Социальный пакет (медицинская страховка, включая стоматологию, собственная столовая) Корпоративный спорт: скидки на посещение фитнес-клубов, футбольная и волейбольная секции Работа в команде, использующей гибкий подход к разработке Оформление в соответствии с ТК РФ с первого дня работы Работа в развивающемся IT-проекте с командой специалистов высокого уровня, возможность развития и обмена опытом, корпоративное обучение "
"69755883","Самокат (ООО Умное пространство)","Data Engineer DWH","False","None","None","От 1 года до 3 лет","Удаленная работа","['Python', 'SQL', 'Data Vault', 'Spark', 'ETL']","Самокат — сервис #1 в России по числу заказов в сфере быстрой доставки Наша миссия — дарить людям время, которое они могут потратить на свои увлечения и близких. Мы создаем новый слой городской инфраструктуры с доставкой продуктов и товаров для дома за 15 минут. Позиция открыта в инфраструктурном продуктовом стриме - Data Platform. Data Platform отвечает за запуск data driven сервисов и продуктов, создание экосистемы и процессов по работе с данными в Самокате, а также за технологическое развитие платформы для решения новых задач компании. Мы в поисках Data Engineer, которому предстоит выполнять следующий блок задач:   Разработка хранилища данных на базе Greenplum   Построение ETL-процессов поставки данных с помощью DBT и Airflow   Расчет сложных аналитических показателей в витринах данных   Развитие кастомного адаптера для DBT под Greenplum   Участие в тестировании потоков с целью обеспечения их стабильной работы   Осуществление поддержки разработанных пайплайнов   Разработка технической документации   Внедрение реализованных решений   Пожелания к кандидатам:   Понимание архитектуры современных аналитических систем и принципов межсистемной интеграции   Понимание архитектуры хранилищ данных и методологий проектирования(Data Vault)   Знание Python   Знание SQL на высоком уровне и опыт оптимизации запросов на какой-либо СУБД   Опыт обработки данных в экосистеме Hadoop (Spark/PySpark, Hive)   Опыт работы с Apache Airflow   Опыт работы с СУБД Greenplum будет плюсом   Мы заботимся о своих сотрудниках, поэтому создаем максимально комфортные условия для реализации профессиональных амбиций:   Официальное трудоустройство   Гибридный формат работы (в офисе или удаленно)   ДМС со стоматологией   Возможность участвовать в профильных конференциях в качестве спикера или участника   Командообразующие мероприятия - дни рождения команд, митапы, презентации, неформальные встречи   Наш фокус это:   Работа в крутом проекте, востребованность которого уже очевидна и будет только увеличиваться   Пул задач, у которых нет тривиальных решений   Профессиональная команда, в которой ты сможешь реализовать свой потенциал   Если ты хочешь развивать продукт, который делает жизнь людей лучше и комфортнее – WELCOME!"
"70137785","Behavox","Senior Software Engineer - Data Integrations (to Canada/Serbia)","True","434999","562941","Более 6 лет","Полный день","['Java', 'Английский язык', 'Java SE', 'SCALA', 'Java Core', 'Spring Framework', 'Big Data']","About Behavox Behavox is shaping the future for how businesses harness their most important raw material - data. Our mission is bold: Organize enterprise data into actionable information that protects and promotes the business growth of multinational companies around the world. From managing enterprise risk and compliance to maximizing revenue and value, our data operating platform presents a widespread opportunity to build multilingual, AI/ML-based solutions that activate data for every function within a global enterprise. Our approach is unique, and it’s validated by our customers who tell us to keep forging ahead because no one else is aggregating, analyzing, and acting on data to uncover opportunities or solve problems quite the way we are. We are looking for fearless innovators who have an insatiable appetite for building what no one has built before. About the Role Behavox is an ecosystem of products with the common mission to organize all corporate communications and productivity data on Earth, and to make it useful to human organizations around the globe to become more compliant, improve conduct &amp; culture, automate their CRM functions and more. We use behavioral signals captured in internal data to help businesses achieve better outcomes by linking employee behavior to specific business processes, which helps organizations understand how they can unleash the collective power of their people. Integration Systems and Infrastructure is an essential part of the Platform Engineering team which is part of the broader CTO organization. As a Senior Software Engineer (Data Integrations), you will act as the owner and contributor for various data integration systems and plugins, partnering up with our Product, Sales, and Solution Engineering teams to solve complex integration challenges and ensure the delivery and maintenance of outstanding products for our clients, on time. Ideal candidates will love this opportunity because: 1. You will get rare expertise in integrations systems. Today, Behavox supports over 80 integrations and this number continues to grow as we expand our product portfolio beyond Compliance.2. Opportunity to tackle engineering problems of very high complexity. Work with high-load systems hosting petabytes of data and ingesting terabytes of unstructured data on a daily basis (Hadoop, HBase, ElasticSearch).3. You can promote your ideas from day one. We love competition. What You&#39;ll Bring  Proven experience of building or working with high-load data integration systems 7+ years of experience building scalable and reliable server-side Java-based applications Deep understanding of the JVM functioning, ability to troubleshoot performance problems Experience with microservice technologies (e.g. Kubernetes, Nomad, Docker, Istio, Envoy) Strong knowledge of core Java and experience with any other JVM-based language (Groovy, Kotlin, Scala)  What You&#39;ll Do  Develop complex scalable and reliable server-side Java-based applications Design and implement solutions to improve supportability and maintainability of the platform Write benchmarks and automated tests for critical features (unit, integration, end-to-end) to improve the overall product quality Take ownership of the complex components and features assigned to you, demonstrating the ability to be autonomous and learn fast Work collaboratively with the other team members (plannings, technical discussions, status meetings, code review) in order to deliver the high-quality solutions on time  What We Offer  A truly global mission with a passionate community in locations all over the world Huge impact and learning potential as our aspirations require bold innovation Highly competitive compensation with 100% bonus pay already integrated Benefits include fully covered health coverage for employee and family Generous time-off policy and flexible work schedule Great relocation package and Work Permit Sponsorship for the candidate and his family (relocation to our main R&amp;D center in Montreal is a MUST)  About Our Process We take Talent very seriously and we are building a community of extraordinary individuals working together in very high performing teams. We also know that the best Talent always has options so we believe that the process has to be a two way assessment - the company AND the candidate assessing the business needs alignment, the career next step alignment, and the cultural alignment.During the process we will begin by exploring the core factors regarding salary and location along with core experience and skills and values alignment. We will then deep dive explore the critical technical competencies we have identified for the role, and then we will deep dive in behavioral competencies.The most aligned candidate will then be asked to do a practical work task simulation activity so we can make sure that you will enjoy the kind of work the role requires, and this task will typically be presented and discussed with a group of colleagues and managers. Finally we will ask you to meet with a number of our senior leaders to make sure that you are making the most informed call possible. Please apply with your CV and cover letter in English. Our recruitment team won&#39;t be able to review the CVs in Russian. Please note that relocation to Canada is required"
"70697360","Bell Integrator","Data Engineer","False","None","None","От 1 года до 3 лет","Удаленная работа","['Hadoop', 'ETL', 'SQL', 'Spark', 'Java', 'SCALA', 'Big Data', 'informatica']","Проект: Развитие платформы больших данных. Мы ищем опытных специалистов, профессионалов в работе с данными. Сейчас нам нужен Разработчик ЕТL на создание аналитических платформ и data-продуктов для разных направлений корпоративно-инвестиционного бизнеса Банка. Команда платформы консолидирует данные из различных внешних и внутренних источников, запускает пилотные проекты и обогащает данными в соответствии с требованиями бизнес-заказчиков Основные задачи:   Миграционные задачи: перенос существующих витрин на фреймворк SDP Hadoop   Развитие и внедрение продуктов банка   Требования:   Опыт работы от 2х лет;   Навыки программирования на Java;   Наличие практического опыта работы с Hadoop (Hive, Impala, sqoop, oozie, HDFS, YARN), понимание парадигмы map-reduce и алгоритмов работы Spark;   Опыт работы с ETL-инструментами (Informatica BDM, ODI, Pentaho) и BI-средствами;   Опыт программирования и разработки алгоритмов и структур данных;   Понимание как оптимизировать ETL процессы;   Фундаментальные знания принципов построения распределенных систем хранения и обработки данных.   Желательно   Опыт работы с реляционными СУБД (Oracle, MS/SQL), навыки оптимизации запросов;   Опыт работы с NoSQL базам данных (HBase, Cassandra);   Опыт работы с Unix shell;   Jenkins;   Условия:  Возможность профессионального и карьерного роста в компании, возможность поучаствовать в разных проектах; Опыт работы в распределенной команде профессионалов; Уровень заработной платы обсуждается индивидуально; Возможность работать удаленно "
"69761586","Rubius","Data Engineer","True","150000","None","От 3 до 6 лет","Удаленная работа","[]","Rubius – IT-компания со смелым характером. Мы разрабатываем софт для клиентов из различных отраслей – от промышленности и нефтегаза до ритейла и медицины. Обосновались в Томске, работаем по всему миру: наше программное обеспечение используют в США, Европе и Азии. Наши решения используют Apple, Tesla, Kaspersky, Amazon, IBM, Uber, Netflix, Газпром, РЖД и другие. В группу компаний входят представительства в США (Нью-Йорк), Казахстане (Алматы, резиденты Astana hub), ОАЭ (Дубай). В нашем профиле на hh.ru мы постарались подробно рассказать о нас, обязательно загялните:) Одна из наших команд, которая занимается искусственным интеллектом, приглашает Data Engineer. Вам предстоит работать с большими данными совместно с аналитиками и командой ML. Мы ищем человека, который умеет собирать витрины, джойнить данные и проверять полученный результат. У нас есть экспертиза в сфере синтеза и анализа аудио и видео, предиктивной аналитики, распознавания разных метрик... Мы исследуем разные сферы от логистики до медицины, погружаемся в сферы наших крупных заказчиков с головой, чтобы помочь оптимизировать процессы. Чем предстоит заниматься:    проектировать и собирать витрины данных по разработанному ТЗ   проектировать, разрабатывать и поддерживать ETL-процессы для загрузки данных из/в Data Lake   тестировать результаты преобразования данных и проверять их целостность   писать документацию - комментировать код   работать с data-аналитиками для создания новых и оптимизации существующих витрин   Добро пожаловать к нам в команду, если есть:   понимание основных операций ДБ и DWH   опыт работы с Hadoop технологиями (Spark, Hive и тд)   хорошее знание SQL, Python   опыт работы с Azure/Yandex облачными платформами   опыт работы с Airflow, Kafka будем плюсом   Что мы предлагаем: Сотрудники компании – главная ценность Rubius. Мы поддерживаем свободу творчества и полёт инженерной мысли. Стремимся, чтобы каждый участник нашей команды раскрыл свой потенциал. Мы стараемся максимально заботиться о наших сотрудниках. Здесь удалённые и офисные команды чувствуют себя максимально комфортно. Про работу и оплату   белая и своевременная заработная плата в зависимости от компетенций и уровня   официальное трудоустройство   до 10% ежемесячной премии за хорошие результаты   помощь с home office   возможно трудоустройство в нашей компании в Казахстане (для желающих получить заветную карту Visa)   Про рост и развитие   индивидуальный трек развития по желанию   бесплатное обучение английскому языку   бонус за профессиональное развитие (курсы, подкасты, литература по хард и софт скиллам)   компенсация 50% за профессиональную сертификацию   внутренние митапы на разные темы   Про офис, плюшки и атмосферу   оплачиваемые занятия спортом (даже в домашних условиях)   программа ДМС после испытательного срока   скидка для вас и родственников в Rubius Academy   бонусы к рождению детей и свадьбе   классные корпоративы и активности   развитая и комфортная корпоративная культура, без иерархии и бюрократии   А ещё у нас есть лучший офис в Томске, где тебя всегда ждут, сообщества по интересам (футбол, теннис, своя музыкальная группа, шахматный клуб...) и коллектив, где прислушиваются к мнению каждого. Подробнее о нашей компании можно почитать в нашем профиле на hh.ru. Там же есть ссылочки на наши сайты и соцсети. Откликайтесь!  "
"70677068","СБЕР","Data Engineer (Greenplum)","False","None","None","От 1 года до 3 лет","Полный день","[]","Блок «Розничный бизнес» Сбера расширяет команду создания Дата-платформы. Мы создаем инструменты и базовые витрины данных для всего Розничного блока на различных платформах (Hadoop, Greenplum, Teradata, Kafka), работаем с огромными объёмами данных (десятки и сотни Тб) Перед нами стоят очень амбициозные задачи, и мы активно расширяемся. Сейчас мы ищем в команду человека, которому предстоит проектировать и разрабатывать решения на платформе MPP СУБД Greenplum Обязанности  Проектирование и разработка аналитических витрин данных для на базе СУБД Greenplum и вывод в промышленную эксплуатацию Реализация потоков поставки данных для потребителей Консалтинг пользователей (DS/DA/DE) по вопросам производительности и особенностей работы СУБД Greenplum Разработка ETL-процессов по преобразованию и загрузке данных Мониторинг и оптимизация процессов загрузки, преобразования данных и сборки витрин Создание инструментов для автоматизации рутинных задач, связанных с обработкой данных Создание инструментов для мониторинга и управления производительностью Greenplum Разработка и внедрение инструментов для расширения возможностей платформы Greenplum Разработка и поддержка сопроводительной документации и спецификаций данных, развитие и поддержка базы знаний  Требования  Высшее техническое образование Опыт работы не менее 2 лет в качестве Data Engineer / Data Analyst / ETL Developer Опыт работы с linux-системами Знание SQL на экспертном уровне (аналитические функции, подзапросы, хранимые процедуры, оптимизация запросов) Знание архитектуры МРР СУБД (Greenplum, Teradata, Exadata), опыт практической работы c MPP СУБД Опыт разработки ETL-процессов, построения хранилищ и витрин данных Английский язык на уровне свободного чтения технической документации Желание и возможность разбираться в сложных проблемах  Будет плюсом:  Опыт практической работы с СУБД Greenplum Знание Python/Java/Scala Опыт работы с Informatica Power Center  Условия  Полный рабочий день, но гибкий график работы (можно выбрать режим начала работы в промежутке 7:00 - 10:30) Профессиональный рост, инновационные, амбициозные проекты и задачи; Обучение, семинары, тренинги, конференции, корпоративная библиотека. Конкурентная компенсация (оклад и премии по результатам деятельности). ДМС, страхование жизни. Свободный дресс-код. Льготные кредиты и корпоративные скидки. Комфортный офис «Sbergile Home» с просторными опенспейсами, лаунж зонами, кафе, рестораном и оборудованными кухнями; Бесплатный фитнес-зал; Дисконт-программа от множества компаний партнеров. "
"70570775","QIWI","Data engineer (Python/Airflow)","False","None","None","От 3 до 6 лет","Удаленная работа","['Python', 'ETL', 'Data enineer', 'Airflow', 'ELT', 'Kafka']","Мы команда QIWI. Финтех компания, которая создает крутые финансовые it-продукты для бизнеса и обычных пользователей. Нас больше 1700 человек, больше 600 человек в it, у нас современный стек и большие планы на будущее. Сейчас мы ищем Data инженера для развития внутреннего продукта. О продукте: Мы разрабатываем свой (dev2dev) продукт. Его цель - увеличение производительности и упрощение жизни разработке. Одной из частей продукта является система, которая собирает информацию про микросервисы (их более 1000), разрабатываемые в компании, из множества разных источников: github, teamcity, grafanа. Она была написана как MVP на Рython, и сейчас нам сложно развивать её с нормальной скоростью. Поэтому мы ищем коллегу, знакомого с инструментами из мира Data Engineering, который поможет нам построить удобный для разработки и масштабируемый ETL/ELT data pipeline. В команде 6 человек: 4 фронтенда, 2 бэкенда на Java, но нет data инженера. Cтек: Python, Airflow, Postgres, Kafka, Gitlab, Gerrit, Jira. Работаем по Scrum. Как понять, что вакансия вам подходит:  Опыт работы в роли data-инженера не менее 2 лет. Умеете писать DAGи, понимаете, что такое ETL. Хорошо владеете ЯП Python. Здорово, если имели дело с большими объемами данных. Будет плюсом если вы работали с Kubernetes и graphql. Вы не боитесь ответственности за выбранные решения, не стесняетесь задавать вопросы коллегам.  Мы предлагаем:  Оформление с первого дня по ТК, современная техника, быстрая настройка доступов. Удаленка, гибрид или работа в комфортном офисе - выбирайте сами. Офис у метро Чертановская 24/7. Забота о сотрудниках в виде расширенного ДМС, страховки, английского, фитнеса, мобильной связи, доступов к библиотекам, персонального бюджета на обучение и не только. Возможность участвовать в митапах, хакатонах, конференциях QIWI. Уникальную атмосферу для продуктивной работы и развивающую среду, где можно найти единомышленников и научиться новому. Никакой бюрократии и важных боссов :-). Собеседование в 1 этап, без лайв кодинга. "
"70576738","МТС","Data engineer на продукт Антиспам (Big Data)","False","None","None","От 1 года до 3 лет","Удаленная работа","['Python', 'Git', 'Spark', 'PostgreSQL', 'Docker']","Big Data МТС – место, где телеком данные превращаются в реально работающие IT-продукты. Мы создали и протестировали несколько десятков сервисов. Самые успешные из них уже стали частью экосистемы МТС. Например, МТС Маркетолог, рекомендации в KION (МТС ТВ), услуга “Кто звонит?” или Спам blacklist. Кого мы ищем? Обязательно:  Понимание моделей данных и принципов устройства хранилищ данных Хорошее знание SQL, работа с хранимыми процедурами Опыт работы хотя бы с одной промышленной БД Знание стека Hadoop/Hive/Spark и опыт работы с большими объемами данных Знание Python, написание API Знакомство с CI/CD и Docker Умение вести проекты в GIT  Что предстоит делать?  Разработка и поддержка пайплайнов обработки данных и машинного обучения на Python и Spark с использованием Airflow, MLflow, а также собственных разработок Организация потоков данных в рамках микросервисной архитектуры платформы, реализация обработки данных в хранимых процедурах БД Интеграция с внешними системами (FTP, API) Поддержка разработанных решений и обеспечение качества данных  Сейчас мы ищем Data Engineer на продукт Антиспам  Антиспам - это база пользователей услуг на основе данных, которая уже превысила 20 млн, блокируется более 120 млн вызовов в месяц. При входящем вызове номера проверяются по справочнику организаций и по спам-базе, которая обновляется в онлайн-режиме с помощью технологий Big Data. Если номер принадлежит спамеру, он не сможет до вас дозвониться В зависимости от услуги, высветится наименование организации, его категория (при наличии информации), а ненужный звонок блокируется либо направляется на голосового бота или голосовую почту. Абонент затем увидит, какая организация ему звонила, может прослушать оставленное сообщение или прочитать его расшифровку Цель: защитить абонентов от назойливых нецелевых звонков спамеров, предоставить информации о звонящем номере и цели звонка Что вы найдете в команде Big Data? Стек:  Используем язык программирования Python Большие данные храним в Hadoop, обрабатываем на Spark (Pyspark, Scala) и SQL Также работаем с данными в PostgreSQL, Teradata, Greenplum Стек ML - Pandas, Scikit-learn, XGBoost, PyTorch, Transformers, BERT, MLflow Процессы автоматизируем в Airflow Контроль версии кода и доступность такового в Git Управляем задачами через Jira и Confluence Мониторинг: Grafana, Prometheus  Команда: в команде Data engineer сейчас 30 человек (во всей Big Data МТС более 300 человек). Все Data инженеры разработчики поделены на группы со своими лидами. Каждую неделю мы обмениваемся опытом на совместных синках. Data инженеры работают в продуктах со своей автономной командой, в которой есть все роли: аналитики, DS, разработчики, девопсы, менеджеры продукта. Условия: каждый месяц - аванс и зарплата, дважды в год - премия. ДМС + стоматология, корпоративная связь, специальные предложения от партнеров и друзей МТС, отпуск 31 день в год. Выдаем 16” MacBook Pro или Dell на выбор. Есть ли обучение?  Конференции, митапы Корпоративный университет МТС и масштабная виртуальная библиотека А ещё мы регулярно обмениваемся опытом на совместных синках с лидами экспертизы  Какой график? Гибкое начало рабочего дня в промежутке с 8 до 11. Есть возможность работать несколько дней вне офиса по договоренности с командой Сколько этапов при отборе? Не более трех:  HR + первое тех. интервью с лидом направления Тестовое задание/второе интервью - по необходимости Собеседование с PO и командой, выбор кандидатом проекта   "
"70145451","билайн","Data Engineer","False","None","None","От 1 года до 3 лет","Удаленная работа","['PostgreSQL', 'Python', 'SQL']","Наша команда каждый день работает над повышением качества связи и отвечает за стабильный сервис для тебя. Профессионалы билайн уверенно создают надежную связь по всей территории нашей страны: в небольших поселках и крупных городах, на вершинах гор и под землей.Если ты готовы решать сложные и масштабные задачи в команде экспертов — мы ждем твое резюме!Обязанности:   Анализ и разработка ETL-процедур в контуре систем по управлению данными  Управление качеством реализации ETL-процедур Автоматизация скриптов дата-аналитиков и обработка сырых данных на PostgreSQL  Требования:  Опыт разработки на PostgreSQL+оптимизация запросов и Python, Aiflow, Git Опыт разработки ETL-процессов Опыт работы с Hadoop (Spark, Hive) Опыт проведения бизнес/системного анализа  Будет плюсом:  Понимание командной строки unix ​​​​​​​Основы администрирования Postgres  ​​​​​"
"70656383","СБЕР","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'Hadoop', 'Spark', 'SCALA', 'Java']","Наша цель реализовать ПКАП (прикладное корпоративное аналитическое приложение) и перенести все сервисы в него из легаси систем, дополнив современными техническими возможностями, позволяющими ускорить, облегчить, автоматизировать работу пользователей с данными. Задачи:  сбор данных их различных источников Корпоративной Аналитической Платформы Банка (Облако Данных) проектирование и разработка ETL-потоков по преобразованию и загрузке данных поддержка внедрения функционала на этапах ПСИ и ОПЭ участие в тестировании и приёмке готового функционала. мониторинг и оптимизация процессов сборки витрин создание потоков поставки данных потребителям проектирование и разработка аналитических витрин данных управление качеством данных разработка API сервисов, интеграция ПКАП с банковскими сервисами и сервисами других участников экосистемы.  От тебя ждем:  отличное знание SQL (аналитические функции, подзапросы, хранимые процедуры, оптимизация запросов) общее понимание архитектуры хранилищ данных понимание принципов ETL-процессов навыки разработки витрин данных на Big Data с использованием описанного ниже стека инструментов и знанием соответствующего окружения и языков программирования:   экосистема Hadoop (Hive, Spark, Kafka, Oozie) языки программирования (Scala, Java, Python) инструменты DevOps (Git, Nexus, Jenkins)   опыт проектирования, разработки витрин данных, тестирования и вывода решений в промышленную эксплуатацию желательно опыт работы с Hadoop в роли Data инженера опыт разработки или администрирования PostgreSQL будет существенным преимуществом.  Почему Вам будет это интересно:  новый проект – миграция существующей функциональности на целевое решение наша команда работает по гибкой методологии и отвечает за полный цикл разработки продуктов, мы сами взаимодействуем с заказчиками, сами проектируем, разрабатываем и тестируем наши продукты.  Работа в Сбере - это:   стабильный оклад и социальная поддержка сотрудников   расширенный ДМС с первого дня работы для сотрудников и льготная медицинская страховка для близких   бесплатная подписка СберПрайм+, скидки на продукты компаний-партнеров, Сбер Маркет, Delivery Club, Самокат, Сбер Еаптека и других   корпоративная пенсионная программа   корпоративное обучение за счет компании   реферальная программа для сотрудников: можно пригласить в команду знакомых профессионалов и получить вознаграждение до 100 тыс. рублей   официальное оформление с первого дня   корпоративный спортзал и скидки в спортзалы-партнеры   скидки на продукты Сбера и компаний экосистемы   мощное железо, дополнительные мониторы и всё, что нужно для продуктивной работы   работу по Agile с лучшими из IT индустрии: 2000 продуктовых команд и возможность внутреннего перемещения  "
"70473067","Лига Цифровой Экономики","Data engineer","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'Python', 'Java', 'Linux', 'SCALA']","Проект по разработке системы для обезличивания данных, обеспечивающей безопасное хранение данных в тестовых контурах банка. Система обрабатывает большие объёмы данных. Твои задачи:  Участие в проработке архитектуры приложения; Разработка на Spark (Java/Scala), тесное сотрудничество с командой BigData; Работа с различными источниками данных, реляционными и нереляционными.  Мы ждем от тебя:  Умение работать с данными; Опыт работы с Linux-системами; Опыт работы с DWH; Знание SQL и опыт работы с реляционными СУБД; Знание ETL процессов; Умение выстраивать ETL процессы; Будет плюсом знание одного из языков программирования Python, Java, С++, Scala.  Условия:  Работа в офисе. "
"70566864","СБЕР","Data Engineer (GreenPlum)","False","None","None","От 1 года до 3 лет","Полный день","[]","Команда трайба «Особенные решения» ищет дата-инженера для развития инфраструктуры данных, предназначенной для построения, развертывания, использования моделей AI и аналитики по клиентским сегментам. Чем предстоит заниматься:  Проектированием и разработкой витрин данных для моделирования и аналитики. Участие в задачах по миграции и рефакторингу витрин данных на целевые платформы. Оптимизацией процессов загрузки, преобразования данных, мониторинг сборки витрин. Предоставлением экспертной поддержки по логике расчета витрин, атрибутов. Поддержкой сопроводительной документации, развитием базы знаний.  Мы ждем от тебя:  Не менее 2 лет работы в качестве Data Engineer/ETL Developer Знание SQL на продвинутом уровне (желательно в контексте Greenplum/Teradata), Опыт разработки витрин данных. Системный подход в анализе и интерпретации исходных данных из систем источников. Высшее техническое образование  Необязательно, но будет плюсом:  Знание Python Построение механизмов или работа с готовыми решениями контроля качества данных Понимания специфики данных розничного банковского бизнеса  С нашей стороны:  Возможности профессионального развития: регулярные митапы по новым решениям и технологиям, организация тренингов как внутри организации, так и от вендоров; В рамках текущих задач будет возможность получить или улучшить навыки работы с современным инструментарием для DE (GreenPlum, Hadoop, Airflow) Конкурентные условия труда (белая заработная плата, оклад, премии) Большой и комфортный офис с бесплатным спортзалом Программу по поддержанию и улучшению состояния здоровья - ДМС, страхование Льготное кредитование в Сбербанке - возможность пользоваться премиальными продуктами Банка на очень специальных условиях Широкий спектр скидок и привилегий от компаний-партнеров "
"70743616","МТС","Data engineer на продукт tNPS - измерение клиентского опыта (Big Data)","False","None","None","От 3 до 6 лет","Удаленная работа","['Spark', 'Python', 'Git', 'Big Data', 'Hadoop', 'Airflow', 'clickhouse', 'greenplum']","Big Data МТС – место, где телеком данные превращаются в реально работающие IT-продукты. Мы создали и протестировали несколько десятков сервисов. Самые успешные из них уже стали частью экосистемы МТС. Например, МТС Маркетолог, рекомендации в KION (МТС ТВ), услуга “Кто звонит?” или Спам blacklist. Кого мы ищем? Обязательно:   Опыт со стеком технологий Hadoop/Spark/Airflow   Опыт построения ETL процессов   Опыт написания кода на Python   Git (GitLab CI/CD)   *nix   Желательно:   Опыт работы с clickhouse или greenplum   Опыт построения пайплайнов с ML моделями   Опыт разработки REST API   Что предстоит делать?  Интеграция источников данных в контур BigData (реляционки + API) Разработка и поддержка ETL-процессов (ежедневные батчи, pyspark/airflow) Контроль качества загружаемых данных (внутренний DQ инструментарий) Сбор витрин данных для ML, первичная предобработка данных Продуктивизация ML-пайплайнов  Сейчас мы ищем Data engineer на продукт tNPS - измерение клиентского опыта Главная задача продукта - измерение клиентского опыта с помощью моделей данных Big Data МТС. Цели продукта:  не проводя опрос пользователя предсказывать класс пользователя (критик или лояльный пользователь) после обращения в точку контакта (прогнозирование tNPS) не проводя опрос пользователя понимать был ли решен вопрос пользователя при обращении  Что вы найдете в команде Big Data? Стек: SQL, Python, Spark, Hadoop, AirFlow, Confluence, MS Office, Git, Jira Команда: в команде Data engineer сейчас 30 человек (во всей Big Data МТС более 300 человек). Все Data инженеры разработчики поделены на группы со своими лидами. Каждую неделю мы обмениваемся опытом на совместных синках. Data инженеры работают в продуктах со своей автономной командой, в которой есть все роли: аналитики, DS, разработчики, девопсы, менеджеры продукта. Условия: каждый месяц - аванс и зарплата, дважды в год - премия. ДМС + стоматология, корпоративная связь, специальные предложения от партнеров и друзей МТС, отпуск 31 день в год. Выдаем 16” MacBook Pro или Dell на выбор. Есть ли обучение?  Конференции, митапы Корпоративный университет МТС и масштабная виртуальная библиотека А ещё мы регулярно обмениваемся опытом на совместных синках с лидами экспертизы  Какой график? Гибкое начало рабочего дня в промежутке с 8 до 11. Есть возможность работать несколько дней вне офиса по договоренности с командой Сколько этапов при отборе? Не более трех:  HR + первое тех. интервью с лидом направления Тестовое задание/второе интервью - по необходимости Собеседование с PO и командой, выбор кандидатом проекта "
"69808262","Aviasales.ru","Team Lead Data Engineer","False","None","None","Более 6 лет","Полный день","[]","Наша команда инфраструктуры аналитики помогает аналитикам делать свою работу. Мы обеспечиваем доставку данных в единое хранилище для аналитики и поддерживаем инструменты для их обработки и презентации. В компании много самодостаточных команд. В каждой команде свои сервисы, свои разработчики и аналитики. Так что у нас достаточно потоков данных самого разнообразного формата и объема. Это данные о букингах, на которые смотрит вся компания, и каждая запись проверяется чуть ли не руками, а еще — данные о билетах объемом более терабайта в сутки. Это требует разных подходов с нашей стороны. Сейчас у нас есть повидавший виды Data Warehouse на базе Impala + HDFS на арендованных серверах. Большая часть данных поступает через Кафку. Мы медленно переезжаем на AWS на Trino + S3 + Iceberg (возможно). Еще рядом Spark для специфических задач. Хочется переезжать быстрее. Для этого надо среди прочего наладить работу с текучкой, чтобы она забирала меньше времени. Что нужно будет делать:    решить часть проблем, связанных с самописным загрузчиком и оркестратором:   местами не успеваем как следует вычитывать толстые топики с данными, так что рано или поздно надо переехать на другое решение (может Flink, может Spark Streaming);     перенести часть задач на Airflow:   сейчас уже понятно, что event-based подход хорошо подходит не для всех задач;   у нас cамописный оркестратор, потому что хотелось event-based систему, а не на кроне, а на момент запуска Airflow и Luigi этого не умели;     привести в порядок агрегированные таблицы, их стало уже много (cейчас переносим из самописного оркестратора в Airflow и думаем насчет dbt);   в Авиасейлс много интересных аналитике действий совершается на клиенте, без общения с серверами:   очень много разных событий идет от клиента с использованием snowplow;   это отдельный большой пласт работ — как с технической стороны, так и с настройкой процессов;     наладить работу текучки, чтобы она забирала меньше времени;   оценивать и декомпозировать задачи совместно с командой, контролировать их выполнение, помогать разбираться с проблемами и узкими местами;   проводить 1-1, давать регулярную обратную связь и участвовать в перформанс ревью команды;   участвовать в найме, проводить собеседования.   Что мы ждем от тебя:    сильный бэкраунд в работе с данными;   опыт менеджмента команды от трех человек, возможно, как “играющего тренера“;   широкий кругозор в инструментах работы с данными;   опыт создания и защиты роудмапов как рутинных задач, так и больших проектов или изменений.   Что мы предлагаем:    сильную команду и возможность влиять как на технологические, так и на продуктовые решения;   возможность выбрать, где работать — удаленно или в одном из наших офисов (Пхукет, Москва или Петербург);   заботу о здоровье: компенсацию индивидуальной психотерапии, медицинскую страховку для тебя и твоей семьи;   поддержку твоих увлечений: компенсацию занятий спортом и изучения иностранных языков.  "
"69355774","билайн","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['Hadoop', 'Hive', 'Linux', 'SQL', 'PostgreSQL', 'Spark', 'SCALA', 'Git', 'Java']","Задачи:  анализ требований к витринам данных (взаимодействие с владельцем продукта, BI-разработчиками, data scientist-ами); поиск и исследование источников данных для последующей интеграции; оценка пригодности, качества исходных данных; разработка ETL процессов на Spark; оркестрация ETL процессов в Airflow; проектирование баз данных; создание конвейеров данных NiFi.  В своей работе DE используют следующий стек технологий:  экосистема Hadoop – HDFS, YARN, Hive, HBase; ETL-процессы – Spark (Scala); потоковая обработка – NiFi, Flink; брокер сообщений – Kafka; оркестрация ETL процессов – Airflow; СУБД – PostgreSQL, Greenplum, Aerospike, Oracle, SQL Server; CI/CD – GitLab.  Требования:  опыт разработки на Spark, Scala от 2 лет; знание стека: Hadoop, Airflow, PostgreSQL, Kafka "
"70581699","Неофлекс","Cloud Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Spark', 'DWH', 'Terraform', 'SQL', 'ETL', 'AWS', 'Azure', 'snowflake']","ЧЕМ ТЫ БУДЕШЬ ЗАНИМАТЬСЯ:  Участвовать в проектах по построению и развитию облачных хранилищ данных.  ТЫ НАШ ИДЕАЛЬНЫЙ КАНДИДАТ, ЕСЛИ У ТЕБЯ:  Более 3 лет практического опыта работы с базой данных Snowflake (включая прием данных, обработку данных, концепцию безопасности); Опыт создания платформ данных (DWH, DataLake и т. д.); Опыт проектирования и внедрения ETL/ELT Frameworks; Знание моделирования данных; Практический опыт работы с одним из «классических» ETL-инструментов, таких как Informatica, Talend, Pentaho; Практический опыт работы с одним из современных инструментов ELT и автоматизации, таких как dbt, Matillion, SqlDBM, Airflow, WhereScape; Практический опыт работы с одним из облачных провайдеров (AWS, Azure, GCP) и сопутствующими сервисами хранения и преобразования данных.  СОВСЕМ КРУТО, ЕСЛИ ТЫ ИМЕЕШЬ:  Практический опыт работы с python (pyspark, pandas и т.д.); Знание PowerBI или любых других инструментов отчетности корпоративного уровня; Практический опыт работы с инструментами CI/CD (Jenkins, Git, Azure DevOps и др.); Опыт IaaC (Terraform, Ansible); Опыт оптимизации производительности SQL-запросов; Опыт работы с MPP-системами; Опыт работы с Hadoop-стеком (среды Spark, Scala). "
"66519211","СБЕР","Data Engineer","False","None","None","От 3 до 6 лет","Полный день","[]","Команда SberData создает централизованное хранилище данных всего Сбера. Это более 350 источников данных и 100+ Пб информации, заказ и получение данных за 15 минут и современный технологический стек работы с данными, включая собственные сборки СУБД на базе Hadoop и Greenplum. Наши решения отмечены международной премией Data Award в 2021г, а лидеры, обладающие уникальными знаниями в разработке кода и современном технологическом стеке С, Scala, Java, Python, Hadoop, Teradata, Oralce и др., являются участниками организации-фонда Apache Foundation. Масштаб задач, объемы данных, сложности финансовых процессов — мы все время на передовой современных технологий, а где-то и создаем их. Обязанности:  разработка дополнительной функциональности для компонентов продукта проработка решений и e2e тестирование ETL интеграций R&amp;D, реализация пилотов по выбору технологий и решений разработка документации для SDP Hadoop поддержка промышленной эксплуатации разработанных решений.  Требования:  опыт разработки на одном из языков не менее 2 лет Java, Scala, Python или Go опыт разработки ETL в экосистеме Hadoop опыт использовании инструментов Continuous Integration или Continuous Deploy или Delivery понимание механизмов работы систем контроля версий, механизмов работы систем управления сборкой и построения модульных тестов опыт и понимание механизмов работы систем функционального и автоматизированного тестирования опыт администрирования Linux предпочтение RHEL, CentOS желание развиваться в направлении Big Data.  ﻿Условия:  амбициозные проекты и задачи профессиональный рост в дружной команде  бесплатный фитнес-зал в БЦ профессиональное обучение, конференции. "
"69662021","Газпром-медиа Развлекательное телевидение (ГПМ РТВ)","Data engineer/Инженер данных","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'SQL', 'PostgreSQL', 'Docker']","О ПРОЕКТЕ Мы — крупнейший в России и восточной Европе медиахолдинг «Газпром-медиа». Собираем команду IT-профи с отличным опытом и классными практиками, чтобы строить видеосервис будущего для пользовательского контента в коротком формате.Тебя ждут хайлоад для многомиллионной аудитории, кластеры и нейронки, компьютерное зрение и микросервисы. И все это на прочном базисе: гибкость аджайла и скорость принятия решений для кратного роста, крутые продуктовые команды, мощная инфраструктура и стабильные условия.Формат работы на твой выбор: можешь работать удаленно или приходить в уютный офис в центре Москвы. КОМАНДА ДЕПАРТАМЕНТА ПРОДУКТА ПЛАТФОРМЫ YAPPY ИЩЕТ DATA ENGINEER Обязанности:  Проектирование и внедрение хранилища аналитических данных Интеграция данные из различных источников в единое хранилище, создание и автоматизация ETL/ELT процедур Разработка и оптимизация витрины данных для аналитики Автоматизация мониторинга качества данных  Требования:  Опыт работы в качестве инженера данных от 1 года Опыт работы с системами аналитики Amplitude, Adjust, AppMetrica и т.п. Продвинутое знание SQL, Python; Глубокое понимание баз данных (PostgreSQL, Citus, ClickHouse) и понимание основ администрирования СУБД (как преимущество) Уверенное владение инструментами оркестрации (Apache Airflow 2.0) Знание Docker, Docker Compose, Swarm  Условия:  Трудоустройство по ТК РФ Формат работы можно рассматривать, как полностью удаленный, гибридный или офис, как будет удобно Гибкое начало рабочего дня ДМС (включая стоматологию) после испытательного срока Поддержка жизненно важных событий сотрудников Скидки на фитнес, кино, курсы английского языка Льготное кредитование "
"68841364","Платформа ОФД","Data Engineer (Big Data)","False","None","None","От 1 года до 3 лет","Гибкий график","['Python', 'Java', 'SCALA', 'SQL', 'Big Data']","«Платформа ОФД» - продуктовая IT-компания, крупнейший в России оператор фискальных данных. Мы создаем полезные и удобные сервисы для предпринимателей. Делаем рыночную аналитику на основе данных из магазинных чеков. Компания резидент Сколково, входит в Экосистему Сбера. Что у нас есть:  Много данных: каждый 3-й чек, пробиваемый в России, находится на наших серверах. Мы принимаем до 50 млн чеков в день, имеем 2 млрд уникальных названий товаров в базе. Развитая инфраструктура: есть несколько кластеров Hadoop, у DS есть несколько мощных машин, GPU делают бр-бр Команда из 4 DE и сильная экспертиза 80% кода мы пишем на Scala Оформление по ТК РФ, белая зп  ДМС с госпитализацией, скорой и стоматологией Офис близко от м. Спортивная/Лужники. Можно работать из дома. Гибкий график начала рабочего дня. Удобная кухня, релакс-зона с тренажером, массажным креслом, приставкой и караоке Пицца-пати раз в месяц и корпоративные праздники  Наш стек: Hadoop, Spark, Hive, PostgreSQL, ClickHouse, Zeppelin/IntelliJ, AirFlow, ElasticSearch, Apache Superset Jupyterhub/PyCharm, python, стандартный python стек (pandas, numpy, sklearn, matplolib), fastext, torch, BERT, HuggingFace, MlFlow GitLab, k8s, Docker, Jira, Confluence Чем предстоит заниматься:  Разрабатывать ETL-процессы с использованием библиотеки Apache Spark на Scala Анализировать, проектировать и создавать витрины данных в соответствии с требованиями конкретного проекта Развивать инфраструктуру для обработки больших данных и кодовой базы Scala Работать с DS для внедрения математических алгоритмов и ML-моделей в промышленные процессы  Откликайся, если ты:  Живешь в Москве или МО Не менее 3-х лет решаешь задачи в области сбора, хранения и анализа данных Используешь при этом один из следующих языков: Java, Scala, Python Отлично знаешь SQL Применяешь в работе инструменты BigData: Airflow, Hadoop, Spark, Hive, Zeppelin Будет плюсом опыт работы с Docker и k8s  Собеседование проходит в 1-2 этапа сразу с руководителем команды разработки"
"68337691","Самокат (ООО Умное пространство)","Data Engineer (Scala)","False","None","None","От 3 до 6 лет","Удаленная работа","['SCALA', 'Java', 'Apache Spark']","Самокат — сервис #1 в России по числу заказов в сфере быстрой доставки Наша миссия — дарить людям время, которое они могут потратить на свои увлечения и близких. Мы создаем новый слой городской инфраструктуры с доставкой продуктов и товаров для дома за 15 минут. Позиция открыта в инфраструктурном продуктовом стриме - Data Platform. Data Platform отвечает за запуск data driven сервисов и продуктов, создание экосистемы и процессов по работе с данными в Самокате, а также за технологическое развитие платформы для решения новых задач компании. Мы в поисках Data Engineer, которому предстоит выполнять следующий блок задач:   реализация сервисов потоковой (near-realtime) обработки данных   разработка компонентов ETL-фреймворка   участие в развитии процессов и фрэймворков команды разработки   Пожелания к кандидатам:   опыт разработки с использованием Apache Spark   опыт и навыки работы с реляционными базами данных   знание базовых алгоритмов и структур данных   опыт разработки на Java или Scala   Мы заботимся о своих сотрудниках, поэтому создаем максимально комфортные условия для реализации профессиональных амбиций:   Официальное трудоустройство   Гибридный формат работы (в офисе или удаленно)   ДМС со стоматологией   Возможность участвовать в профильных конференциях в качестве спикера или участника   Командообразующие мероприятия - дни рождения команд, митапы, презентации, неформальные встречи   Наш фокус это:   Работа в крутом проекте, востребованность которого уже очевидна и будет только увеличиваться   Пул задач, у которых нет тривиальных решений   Профессиональная команда, в которой ты сможешь реализовать свой потенциал   Если ты хочешь развивать продукт, который делает жизнь людей лучше и комфортнее – WELCOME!"
"52071705","Яндекс","Data Engineer в Финтех","False","None","None","От 3 до 6 лет","Полный день","['Python', 'SQL', 'Hadoop', 'Spark', 'Базы данных']","Финтех-сервисы — самое молодое и смелое направление Яндекса. Мы создаём удобные платёжные сервисы и инструменты для управления личными финансами многомиллионной аудитории Яндекса. Придерживаемся data-driven-подхода.Мы работаем с большими объёмами данных, и делать это надо быстро и правильно. Для этого нам нужно создать большую, безопасную и удобную аналитическую платформу. Мы ищем дата-инженеров для разработки DWH. Что нужно делать:  работать с базами данных SQL и NoSQL, масштабировать и шардировать их; обрабатывать массивы данных с учётом eventual consistency и других аспектов ACID; проектировать модели данных; продумывать отказоустойчивость системы: хранилище должно пережить всё, включая падение дата-центра и вторжение инопланетян; вместе с лидом команды проектировать архитектуру хранилища.  Мы ждем, что вы:  пишете код на Python; работали с базами данных SQL и NoSQL; знакомы с брокерами сообщений (Kafka, RabbitMQ или что-то подобное); работали с инструментами Hadoop.  Будет плюсом, если вы:  работали со Spark или Spark SQL; работали с Greenplum; умеете разрабатывать DAG для Airflow. "
"70534590","Работут","Data Engineer","False","None","None","От 1 года до 3 лет","Удаленная работа","['SQL', 'Tableau', 'Английский язык', 'SAP Business Objects', 'Power BI']","Предстоящие задачи:  Разработка и поддержка систем аналитической отчетности; Анализ и оценка бизнес-требований для создания нового функционала. Работа с инцидентами и запросами на изменение; Продвинутая аналитика данных; Создание и оптимизация структуры БД, а также ETL процессов; Взаимодействие с Бизнес-заказчиками, а также с Глобальными и Секторными ИТ подразделениями.    Мы ожидаем от тебя:  Высшее образование (ИТ, Математика или Техническое); Опыт участия в проектах по построению корпоративных хранилищ данных и разработке аналитических систем; Хорошие знание SQL и инструментов визуализации данных (Tableau, Power BI, SAP BusinessObjects). Опыт создания интерактивных дашбордов; Знание английского языка на уровне Intermediate.    Плюсом будет:  Преимущество кандидатам с опытом в Data science.    Как мы работаем  у нас in-house разработка; долгосрочные проекты – игра «в долгую» (не проектная занятость); один на один с трудностями не оставляем, всё обсуждаем и помогаем друг другу.    Мы предлагаем  оплата по рынку и выше; удаленный формат работы; сильная команда специалистов по разным направлениям; Официальное трудоустройство, соблюдение ТК.    Почему у нас хорошо?  Стабильность. Компания экономически устойчива и растёт каждый год. Масштаб. Вы станете участником крупных проектов, полностью меняющих бизнес-процессы и даже бизнесы.   Интерес. Вы попадёте в насыщенную событиями рабочую атмосферу крупной компании. Доверие. Лояльное руководство, отсутствие микроменеджмента. Комфорт. Мы за work-life balance – стараемся жить без переработок и отдыхать в выходные. "
"70536305","Риалвеб, Интернет-агентство","Head of Data Engineering","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'SQL', 'Управление проектами', 'Hadoop', 'Обучение и развитие']","Команда Риалвеб видит потенциал в развитии бизнес и технического консалтинга. Для ускорения развития этого направления мы открыли внутренний стартап - Realweb Consulting (RWC), куда мы ищем руководителя инженерии данных RWC помогает быстрее расти и достигать бизнес-KPI клиентам группы компаний, за счет стратегий развития цифровых продаж, систематизации бизнес-процессов, разработки эффективной системы KPI и системы метрик, внедрения современных решений в сфере работы с данными и продвинутых методов аналитики и моделирования. Люди — наша ценность. Мы уделяем внимание профессиональному и карьерному развитию каждого сотрудника. Даём возможности для роста и реализации своего потенциала. Мы ценим обратную связь и ориентируемся на командную работу и взаимную потребность.   Какие цели и задачи будут перед тобой стоять? Ожидаем, что сотрудник сможет с нуля выстроить центр компетенций по направлению Data Engineering: В направлении по кадрам (начнем с команды в 4-5 человек)  Определит роли и структуру подразделения; Разработает модель компетенций для сотрудников подразделения; Организует найм (внутренний или внешний) в свое подразделение; Выстроит процесс обучения и развития сотрудников в отделе.  В направлении по продукту  Выстроит процессы разработки DWH для задач маркетинга; Наладит интеграции с ключевыми источниками данных; Определит пул технологий, которые будут использоваться, как основные для задач сбора, трансформации, хранения данных, визуализации и моделирования; Запустит регулярный процесс RND новых систем и технологий по направлению цифровых продаж.    Мы уверены, что ты с ними справишься, если:  Тебе знакомы процессы разработки информационно-аналитических систем или цифровых продуктов; Имеешь опыт управления командой; Имеешь опыт работы с ключевыми системами и технологиями работы с данными (Hadoop, Hive, Airflow, Oozie, Kafka, Clickhouse, или аналогами); Знаешь языки обработки данных SQL (любой из диалектов)/Python/R; Знаешь архитектуру и возможности Yandex Cloud / AWS / GCP.  Будет большим плюсом:   Опыт реализации проектов с использованием машинного обучения; Если умеешь в DevOps - поднять и развернуть сервис на виртуалке, настроить окружение, сконфигурировать виртуализацию и тп.    Как мы будем тебе помогать в достижении поставленных целей: Профессионально:  Сложные и интересные задачи; Выделим необходимые ресурсы для достижениях поставленных перед тобой задач; Выделим ментора по направлению ИТ и разработки Культура полной открытости. Ты легко сможешь получить консультацию и провести мозговой штурм с любым сотрудником группы компаний; Богатый опыт разработки собственных технологий и инструментов (Гарпун, AdHands, Мета и др.). В своей работе ты сможешь использовать эти инструменты или наработки команды.  Позаботимся о твоем комфорте:   Работаем удаленно или в уютном офисе (8 мин. от метро «Павелецкая»/«Третьяковская»);   Любим корпоративные мероприятия, квартирники, настолки и весело проводим время вместе;   Работаем в гибком графике;   Живем без бюрократии, дресс-кодов и прочего формализма;  Оформляем и работаем в соответствии с ТК РФ;  Частично компенсируем ДМС.  "
"68693532","Неофлекс","Data Engineer (Отчетность)","False","None","None","От 1 года до 3 лет","Полный день","['Hadoop', 'Hive', 'Java', 'SQL', 'Spark']","ЧЕМ ТЫ БУДЕШЬ ЗАНИМАТЬСЯ:  Организацией процессов выгрузки данных из систем источников и загрузки в хранилище; Анализом корректности и консистентности данных; Доработкой клиентского хранилища данных.  ТЫ НАШ ИДЕАЛЬНЫЙ КАНДИДАТ, ЕСЛИ У ТЕБЯ:  Опыт программирования на Java Core; Опыт работы с экосистемой Hadoop (Spark, HDFS, Hive); Уверенное знание SQL (диалект не принципиален).  СОВСЕМ КРУТО, ЕСЛИ ТЫ ИМЕЕШЬ:  Понимание принципов построения хранилищ данных; Опыт работы в роли разработчика ETL; Опыт работы с Maven, Git, Sonarqube, Jenkins, Jira, Confluence; Знание основ банковского бухгалтерского учета.  У НАС ТЫ СМОЖЕШЬ НАЙТИ:  Прозрачную системы карьерного развития в компании; Персонального наставника с первого дня работы; Возможность развития личной экспертизы и экспертизы компании; Собственную платформу внутренних и внешних образовательных программ; Возможность пройти сертификацию; Возможность участия в обучении, конференциях, митапах; Неповторимую корпоративную культуру компании.  Мы ищем кандидата без привязки к локации.     Офисы компании находятся в городах: МОСКВА, САНКТ-ПЕТЕРБУРГ, НИЖНИЙ НОВГОРОД, САРАТОВ, ВОРОНЕЖ, ПЕНЗА."
"53926816","Газпром-медиа Развлекательное телевидение (ГПМ РТВ)","Data engineer","False","None","None","От 3 до 6 лет","Полный день","['Spark', 'SCALA', 'Java']","ГПМ ДАТА - ДОЧЕРНЯЯ КОМПАНИЯ ГАЗПРОМ-МЕДИА ХОЛДИНГА В рамках Технического департамента открыта вакансия Data engineer О проекте: Задача команды является построение профиля пользователя (сбор и обработка данных из множества источников и &quot;достройка&quot; с помощью ML алгоритмов) и последующее использование в клиентских сервисах для таргетирования рекламы. Вам предстоит заниматься:   Развитие собственного решения по управлению данными в составе команды &quot;Профиля&quot;   Работа с данными, написание сервисов с использованием Spark/Flink   Рефакторинг, доработка и сопровождение написанных сервисов   Написание тестов.   Вы нам подходите, если вы  Уверенное знание scala/java Знание Spark и Hadoop ecosystem Опыт работы с MPP базами (Vertica, Greenplum, Teradata, ClickHouse) Опыт работы с оркестраторами ETL процессов (Ariflow, Oozie) Плюсом будет опыт работы с Apache Flink  Мы предлагаем:  Трудоустройство по ТК РФ Удаленная работа с возможностью выхода в офис по желанию Гибкий график работы ДМС (включая стоматологию) после испытательного срока Поддержка жизненно важных событий сотрудников Скидки на фитнес, кино, курсы английского языка Льготное кредитование "
"70506992","Банк ВТБ (ПАО)","Дата инженер (Data engineer) Управление модельных рисков и валидации","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'SQL']","Работа в составе команды валидации (в связке с DS и методологами), которая занимается оценкой эффективности моделей в анализируемом процессе и оценкой модельных рисков. Анализируются: подходы к разработке модели, стат показатели качества модели, качество данных, используемых на разработке и применении, ИТ системы и процессы использования модели. ОБЯЗАННОСТИ:  автоматизация загрузок с помощью Python; построение витрин данных (SQL, Python); преобразование данных из различных форматов (xml, json) к табличному виду с помощью Python; поиск ошибок и аномалий в данных (SQL, Python, PySpark), автоматизация проверок качества данных; расчёт признаков, подготовка агрегированных витрин данных (SQL, Python, PySpark); подготовка требований к формату и составу файлов для поставщиков данных; выстраивание процессов Data Governance на инфраструктуре Hadoop (контроль качества данных); контроль качества данных в хранилище (Hadoop) – постановка задач, анализ; участие в проектах связанных с витринами данных, банковским хранилищем, MLOps.  ТРЕБОВАНИЯ:  высшее физико-математическое/техническое/экономическое образование или обучение на последнем курсе технического ВУЗа; опыт работы в ИТ компаниях, подразделениях ИТ или рисков банка от 1 года; уверенные знания SQL, Python; знание PySpark как преимущество; опыт работы с BI системами и средствами визуализации как преимущество; знание стандартов по качеству данных.  УСЛОВИЯ:  трудоустройство согласно законодательству; конкурентная заработная плата; профессиональное обучение и развитие; добровольное медицинское страхование, льготные условия кредитования; корпоративная пенсионная программа, материальная помощь; спортивная жизнь и корпоративные мероприятия; возможность построить карьеру в ведущем банке России. "
"70739780","СБЕР","Senior Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Hadoop', 'SQL', 'Spark', 'PostgreSQL', 'Java', 'SCALA', 'Kafka']","Наша команда занимается разработкой витрин данных для продуктов транзакционного бизнеса. От организации тракта получения данных источника в потоковом и пакетном режиме до разработки интерфейсов предоставления доступа к данным. Основа технологического стека : Hadoop, Spark, Sqoop, Kafka, Nifi, PostgreSQL, OpenShift, Java(Spring). Обязанности Вам предстоит проектировать и выстраивать ETL процессы загрузки данных с гибкой логикой обновления внутренней модели. Построение витрин больших объемов (десятки терабайт), в т.ч. с возможностью частичного перестроения. Требования  Уверенное владение Scala в части spark-разработки Понимание принципов распределенных вычислений Знание и понимание DevOps практик Опыт работы с Unix-системами Знание SQL и опыт работы с реляционными СУБД Опыт разработки баз данных от 2 лет  Будет плюсом  Знакомство с итеративной разработкой, подходами CI/CD Опыт работы с in-memory и noSQL БД Знание распространенных паттернов проектирования Опыт проектирования и разработки высоконагруженных систем высокого класса критичности Опыт работы с PostgreSQL  Условия  Профессиональное обучение, семинары, тренинги, конференции ДМС с первого дня, сниженные ставки по кредитованию, ипотеке, программы лояльности для сотрудников Самые инновационные, амбициозные проекты и задачи Работа в комфортном офисе «Sbergile Home» с просторными опенспейсами, лаунж зонами, кафе, рестораном и оборудованными кухнями Бесплатный фитнес-зал Дисконт-программа от множества компаний партнеров "
"70480175","Самокат (ООО Умное пространство)","Data engineer DWH ClickHouse","False","None","None","От 1 года до 3 лет","Удаленная работа","['SQL', 'ETL', 'ClickHouse', 'Airflow']","Самокат — сервис #1 в России по числу заказов в сфере быстрой доставки Наша миссия — дарить людям время, которое они могут потратить на свои увлечения и близких. Мы создаем новый слой городской инфраструктуры с доставкой продуктов и товаров для дома за 15 минут. Позиция открыта в инфраструктурном продуктовом стриме - Data Platform. Data Platform отвечает за запуск data driven сервисов и продуктов, создание экосистемы и процессов по работе с данными в Самокате, а также за технологическое развитие платформы для решения новых задач компании. Мы в поисках Data Engineer, которому предстоит выполнять следующий блок задач:   Разработка хранилища данных на базе ClickHouse   Построение ETL-процессов поставки данных с помощью Airflow   Расчет аналитических показателей для онлайн отчетности   Участие в тестировании потоков с целью обеспечения их стабильной работы   Осуществление поддержки разработанных пайплайнов   Разработка технической документации   Внедрение реализованных решений   Пожелания к кандидатам:   Опыт работы с СУБД ClickHouse (не обязательно, но было бы здорово)   Понимание архитектуры современных аналитических систем и принципов межсистемной интеграции   Понимание архитектуры хранилищ данных   Знание SQL на высоком уровне и опыт оптимизации запросов на какой-либо СУБД   Мы заботимся о своих сотрудниках, поэтому создаем максимально комфортные условия для реализации профессиональных амбиций:   Официальное трудоустройство в аккредитованную IT-компанию   Гибридный формат работы (в офисе или удаленно)   ДМС со стоматологией   Возможность участвовать в профильных конференциях в качестве спикера или участника   Командообразующие мероприятия - дни рождения команд, митапы, презентации, неформальные встречи   Наш фокус это:   Работа в крутом проекте, востребованность которого уже очевидна и будет только увеличиваться   Пул задач, у которых нет тривиальных решений   Профессиональная команда, в которой ты сможешь реализовать свой потенциал   Если ты хочешь развивать продукт, который делает жизнь людей лучше и комфортнее – WELCOME!"
"68929494","Велесстрой","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['СУБД MS SQL', 'MongoDB', 'Hadoop', 'ETL', 'MS PowerBI', 'Python']","Сейчас мы в поиске Data-инженера для реализации масштабного проекта. Вместе нам предстоит построить новое хранилище данных с постепенным охватом всех предметных областей компании. Будем плотно сотрудничать с архитектором данных, бизнес-заказчиком и аналитиками. ЗАДАЧИ:  Проектирование хранилища данных. Автоматизация процессов обработки, «обогащения» и нормализации данных. Управление качеством данных. Поддержка продукта при изменениях в интерфейсах и форматах данных поставщиков, общение с технической поддержкой поставщиков данных.  НАШ ПОРТРЕТ КАНДИДАТА:  Желательно высшее техническое образование. Отсутствие высшего образования не является стопером, если вы отличный инженер или разработчик. Опыт проектирования аналитических систем хранения и обработки больших данных не менее 1 года. Понимание принципов организации DWH. Python, знание стандартных структур данных. Продвинутый уровень владения MS-SQL, MongoDB, Hadoop (Spark), ETL. Продвинутый уровень владения MS PowerBI.  МЫ ПРЕДЛАГАЕМ:  Гибкий график для оптимального баланса работы и личной жизни. Профессиональный рост в команде дружных профессионалов ДМС. Оформление по ТК РФ. Рабочее место в Центральном в офисе Компании: м. Маяковская (2-3 мин. пешком от метро).* Информация о вакансии публикуется для формирования кадрового резерва.   "
"69543162","Ростелеком","Data Engineer","False","None","None","От 3 до 6 лет","Удаленная работа","['Python', 'SQL', 'PostgreSQL', 'ETL', 'СУБД', 'Knime', 'Talend', 'Pentaho', 'Dagster', 'Airflow', 'NoSQL', 'Clickhouse', 'Redis', 'Elasticsearch']","Чем предстоит заниматься:  Проектированием и реализацией моделей данных, исходя из бизнес-требований; Выстраиванием интеграций с различными источниками и ETL процессов; Документировать и поддерживать разработанные процессы (мониторинг/оптимизация); Участвовать в развитии инфраструктурных решений команды.  Пожелания к опыту:  Сильные компетенции в области реляционных СУБД и хранилищ данных. Предпочтение Postgres; Уверенное знание SQL: сложные запросы, процедуры, функции, оптимизация производительности запросов; Опыт разработки ETL-процессов. Будет плюсом знание knime/talend/pentaho; Навыки использования оркестратора Dagster/Airflow; Уверенное владение Python в части обработки данных и ETL; Желание разбираться в бизнес-особенностях данных с которыми предстоит работать.  Как преимущество:  Опыт работы с NoSQL СУБД (Clickhouse,Redis,Elasticsearch); Навыки создания витрин данных с помощью dbt; Опыт разворачивания ETL инфраструктуры.  Условия:  Работу в лидирующей компании на рынке телекоммуникационных услуг (штат сотрудников более 100.000 человек); Официальное трудоустройство по ТК РФ; Полностью «белую» зарплату (фиксированный оклад + 15% квартальная и 14% годовая премии) – стабильные выплаты 2 раза в месяц; График работы: Удалённый формат 5/2, пн-чт с 09:00 до 18:00, пт с 09:00 до 16:45, суббота и воскресенье – выходные; Приобретение уникального профессионального опыта – обучение, опытный наставник и интереснее задачи; ДМС, корпоративные скидки от компаний-партнеров. "
"68042853","Неофлекс","Data Engineer (Миграция)","False","None","None","От 1 года до 3 лет","Полный день","['PostgreSQL', 'Informatica', 'SQL', 'ETL', 'DWH', 'SCALA', 'Spark', 'Базы данных', 'Big Data', 'Bitbucket']","ПРОЕКТ: Перевод текущего функционала DWH на Greenplum. СТЭК: SQL; PL/pgSQL; Python. ЧЕМ ТЫ БУДЕШЬ ЗАНИМАТЬСЯ:  Организовывать процессы выгрузки данных из систем источников и загрузки в хранилище под управлением различных РСУБД (PostgreSQL/Greenplum); Дорабатывать клиентское хранилище данных под управлением различных РСУБД (PostgreSQL/Greenplum); Анализировать корректность и консистентность данных.  ТЫ НАШ ИДЕАЛЬНЫЙ КАНДИДАТ, ЕСЛИ У ТЕБЯ ЕСТЬ:  Уверенное знание SQL и опыт работы с РСУБД (предпочтительно PostgreSQL/Greenplum); Опыт оптимизации приложений на SQL, PL/pgSQL; Понимание принципов построения хранилищ данных.  СОВСЕМ КРУТО, ЕСЛИ:  Умеешь работать с ETL инструментами (Informatica, IBM Data Stage, Talend); Умеешь работать с PXF (Platform Extension Framework); Умеешь работать с Airflow, Scala, Spark, Python; Умеешь работать с Git, Bitbucket.  У НАС ТЫ СМОЖЕШЬ НАЙТИ:  Прозрачную системы карьерного развития в компании; Персонального наставника с первого дня работы; Возможность развития личной экспертизы и экспертизы компании; Собственную платформу внутренних и внешних образовательных программ; Возможность пройти сертификацию; Возможность участия в обучении, конференциях, митапах; Неповторимую корпоративную культуру компании.  Мы ищем кандидата без привязки к локации. Офисы компании находятся в городах: МОСКВА, САНКТ-ПЕТЕРБУРГ, НИЖНИЙ НОВГОРОД, САРАТОВ, ВОРОНЕЖ, ПЕНЗА."
"69573057","Правительство Москвы","Data инженер","False","None","None","От 3 до 6 лет","Полный день","['Python', 'SQL', 'Hadoop']","Работа в Правительстве Москвы — это возможность делать наш город современнее и удобнее. Если ты тоже неравнодушен к Москве, хочешь развивать ее и развиваться сам, присоединяйся к нашей команде! Обязанности:   Организация витрин данных (Стек технологий: SQL, pyspark)   Оптимизация хранилища данных   Разработка и оптимизация ETL-процессов   Разрабатывать надежные pipeline обработки данных   Анализировать новые источники данных, настраивать процесс загрузки данных в Data Lake   Развивать платформу данных, в т.ч. Data Quality   Требования:   Высшее образование   Опыт работы с Hadoop (HDFS, Yarn), понимание структур данных (Parquet).   Опыт разработки высокопроизводительных ETL-процессов на pySpark   Глубокое знание SQL, основных принципов работы реляционных и nosql СУБД, в т.ч. MPP.   Умение писать оптимальные запросы   Уверенное знание Python   Навыки реализации оркестрации в Airflow   Знание основных методологий построения хранилищ данных   Условия:  Работа в динамично развивающейся организации Правительства Москвы Интересные и амбициозные задачи Профессиональный коллектив, возможность развития и карьерного роста Стабильная заработная плата График работы 5/2 "
"67162218","СБЕР","Data Engineer","False","None","None","От 3 до 6 лет","Полный день","[]","В SberData мы создаем централизованное хранилище данных всего Сбера. Это более 350 источников данных и 100+ Пб информации, заказ и получение данных за 15 минут и современный технологический стек работы с данными, включая собственные сборки СУБД на базе Hadoop и Greenplum. Наши решения отмечены международной премией Data Award в 2021 г., а лидеры, обладающие уникальными знаниями в разработке кода и современном технологическом стеке С, Scala, Java, Python, Hadoop, Teradata, Oracle и др., являются участниками организации-фонда Apache Foundation. Масштаб задач, объемы данных, сложности финансовых процессов — мы все время на передовой современных технологий, а где-то и создаем их. Мы создаем платформу данных Экосистемы Сбера, которая позволит компаниям Экосистемы применять передовые «облачные» практики в своих процессах обработки данных и предлагаем вам погрузиться в самые инновационные и амбициозные проекты и задачи. Обязанности:  Развертывание, настройка и администрирование Apache Airflow для пилотов и других проектов; Техническая поддержка пользователей; Создание собственных расширений Apache Airflow (DAGs) по требованиям проектов; Участие в разработке solution архитектур с применением Airflow; Участие в автоматизации развертывания и интеграции Apache Airflow в Облаке; Проведение презентаций и обучения по Apache Airflow, создание обучающих курсов.  Требования:  Знание и опыт работы с Apache Airflow, включая развертывание, настройку и интеграцию в enterprise среде; Опыт промышленной разработки на python; Background в Big Data; Знание экосистемы Apache Hadoop на уровне пользователя; Понимание основ облачных технологий и технологий виртуализации и контейнеризации; Знание SQL; Опыт работы с популярными РСУБД (pgsql, mysql/mariadb, желательно Oracle), включая развертывание и настройку; Знание основ администрирования ОС Linux; Понимание основ TCP/IP и сетевых протоколов.  Плюсом будет:  Практический опыт с автоматизацией развертывания ПО; Знание средств автоматизации развертывания (Ansible, Puppet, etc.); Опыт администрирования РСУБД .  Условия:  Профессиональное обучение, семинары, тренинги, конференции, корпоративная библиотека; ДМС, страхование жизни; Самые инновационные, амбициозные проекты и задачи; Свободный дресс-код; Гибкий график для оптимального баланса работы и личной жизни; Льготные кредиты и корпоративные скидки; Конкурентная компенсация (оклад и премии по результатам деятельности). "
"70417491","СБЕР","Data Engineer в команду «Сеть Продаж»","False","None","None","От 3 до 6 лет","Полный день","['Hadoop', 'Spark', 'SQL', 'Python', 'SCALA']","В поиске Data Engineer`а в команду Data Science Блока «Сеть Продаж». Мы разрабатываем и внедряем end-to-end решения на основе машинного обучения в бизнес-процессы банка, связанные с обслуживанием клиентов в отделениях Сбера. Задачи:  Работа с большими объемами разнотипных данных (данные банка и Экосистемы) Построение витрин данных для разработки и применения ML-моделей Создание и поддержка регулярных процессов подготовки, поставки данных и исполнения моделей  Требования:  Понимание принципов разработки, внедрения и сопровождения ML-решений Опыт работы с SQL, Spark, Python, CTL, ETL  Условия:  Технологии для воплощения идей: лучшие данные Сбера а в Teradata/ Greenplum /Hadoop, Python на мощных серверах с GPU Большое Data Science Сommunity Возможности для саморазвития: оплата курсов популярных платформ, командировки на конференции, карьерный рост Спортзал с занятиями и сауной в офисе (йога, кроссфит, пр.) ДМС, специальные предложения от партнеров ДМС, спортивный зал, персональные предложения от партнеров "
"69978113","билайн","Data engineer (Middle)","False","None","None","От 1 года до 3 лет","Удаленная работа","['SQL', 'SCALA', 'Linux', 'Работа в команде', 'hadoop', 'Spark', 'Kafka', 'NiFi', 'Airflow', 'Python', 'Big Data', 'Cистемы управления базами данных']","  Что и как мы делаем? Мы создаем продвинутое аналитическое хранилище (Data Lake) по принципам Data Mesh. Все данные мы описываем в каталоге, чтобы их можно было легко находить и переиспользовать. Для каждого куска данных мы определяем владельца, который относится к ним, как к продукту. Для этого в каждом подразделении должны быть люди с навыками data engineer, погруженные в предметную область. Под эту задачу мы сейчас активно набираем людей.   Наши преимущества:  Мы развиваем свое сообщество DE. Сейчас в нем состоит 300 человек с разным уровнем навыков от Junior до Senior. Инженеры помогают друг другу бороться с трудностями и развиваться, делятся друг с другом кодом, всякими лайфхаками; Мы проводим митапы по инфраструктурным и софтовым темам. Коллеги делятся опытом, помогают разобраться в востребованных темах; Мы даем все инструменты для обучения и развития (в том числе, платные). Конференции, подписки, книги, курсы – все что помогает расти профессионально; Мы даем возможность выбрать удобный формат работы: удаленный или гибридный.  Типовые задачи DE:  Анализ требований к витринам данных (взаимодействие с владельцем продукта, BI-разработчиками, data scientist-ами); Поиск и исследование источников данных для последующей интеграции; Оценка пригодности, качества исходных данных; Разработка ETL процессов на Spark; Оркестрация ETL процессов в Airflow; Проектирование баз данных; Создание конвейеров данных NiFi.  Мы понимаем, что каждый DE индивидуален. Поэтому даем описание как бы выглядел идеальный кандидат.  DE в Билайн:  Любит работать в команде и умеет это делать; Знает SQL на высоком уровне (в т. ч. DDL, табличные выражения, оконные функции); Работал с Hive, PostgreSQL; Умеет разрабатывать ETL процессы Spark на Scala (потоковая обработка как преимущество); Пользовался AirFlow или другими оркестраторами – Oozie, Luigi, ну или cron; Может что-то написать на Python – в объеме чтобы пользоваться AirFlow или еще круче; Имеет опыт потоковой разработки конвейеров данных в NiFi или Flink; Интересуется Flink, пробовал применять его в проектах; Умеет проектировать базы данных (знает Data Vault 2.0 например); Понимает принципы работы реляционных СУБД и HDFS; Имеет представление о колоночных и NoSQL СУБД; Понимает подходы к работе с качеством данных; Применяет системный подход к работе, думает о конечной бизнес-задаче, мыслит логически, уделяет внимание деталям.  Наши проекты У нас все организовано в виде продуктов, имеющих бесконечный срок жизни. Продуктов очень много – несколько сотен. Если делить их по группам – получится так: Клиентский опыт:  обобщаем все клиентские взаимодействия с компанией в одну историю, под одним универсальным идентификатором; прогнозируем и корректируем общую выручку от клиента на всем периоде жизни с компанией; боремся с фродом на стороне клиентских устройств сотовой связи; предсказываем отток и next-best-action для клиентов.  Управление оборудованием и качеством услуг связи:  собираем и анализируем метрики качества предоставления связи; выполняем интеллектуальное планирование постройки базовых станций; осуществляем предиктивное обслуживание оборудования.  Для продуктовых команд:  создаем для себя фреймворки и утилиты; развиваем сервис мониторинга как единую точку сбора и просмотра метрик.  Стек и технологии В своей работе DE используют следующий стек технологий:  Экосистема Hadoop – HDFS, YARN, Hive, HBase; ETL-процессы – Spark (Scala); Потоковая обработка – NiFi, Flink; Брокер сообщений – Kafka; Оркестрация ETL процессов – Airflow; СУБД – PostgreSQL, Greenplum, Aerospike, Oracle, SQL Server; CI/CD – GitLab.  Присоединяйся к лучшей команде в качестве Data Engineer!  "
"70425910","Марс","Data Engineer","False","None","None","От 3 до 6 лет","Полный день","[]","Job Description: The Data Engineer will be responsible for ingesting, processing and distributing the data used by Mars’s data-driven business systems. And also, Data Engineer will provide software development and product delivery support for data products and product teams. One of the focus areas at this role will be design, develop and support Manufacturing Data Platform &amp; Data Domain as a centralized space for work with Data (incl. streaming) based on cloud technologies across all MARS PN RU factories   What are we looking for?    Extensive experience in providing data solutions for Cloud applications Understanding database operation and DWH development principles; Understanding of data warehousing, data cleaning, data pipelines and other analytical techniques required for data usage Experience with Hadoop-based technologies (HDInsight, Spark, Hive, Scala, etc.); Good knowledge of SQL, Python/Scala or Java; Experience with Nifi, Kafka, Airflow, InfluxDB/Flux Experience with data streaming engineering Understanding KEP/OPC relay servers technology (OPC UA/DA, tags, naming, publishing) as a plus 1+ year experience with Yandex Cloud Platform/ Azure stack; Self-starter who welcomes responsibility, along with the ability to thrive in an evolving organization and an ability to bring structure to unstructured situations English level - Intermediate or higher.    What will be your key responsibilities?  Design and develop data marts; Design, develop and support ETL processes for loading data to / from Data Lake / Factory Data Platform; Integrate new sources to DataLake/DWH/Factory Data Platform; Configuring and supporting filtering, routing, data queues to Factory Data Platform Configuring and supporting data post-enrichment, transformation, and contextualization in the Factory Data Platform Defining and supporting tag/data storage policy Quality control of uploaded data (manual and automated DQ tests); Write documentation; Work with data and analytics experts to strive for greater functionality in our data products    What can you expect from Mars?  Work with over 130,000 like-minded and talented Associates, all guided by The Five Principles. Join a purpose driven company, where we’re striving to build the world we want tomorrow, today. Best-in-class learning and development support from day one, including access to our in-house Mars University. An industry competitive salary and benefits package, including company bonus.        Mars is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. The company is pleased to provide such assistance, and no applicant will be penalized as a result of such a request."
"70535542","ЦУМ, ТД","Data engineer/ BI engineer","False","None","None","От 3 до 6 лет","Полный день","['Git', 'Python', 'SQL', 'PostgreSQL', 'Linux']","Торговый Дом ЦУМ - лидер luxury-сегмента электронной коммерции, находится в поиске Дата инженера.  В ЦУМ у Вас есть возможность построить карьеру и влиять на развитие продукта с мировым именем, а наша большая команда профессионалов поможет Вам расширить свою экспертизу Мы используем технологии, подходящие под задачи, и, при необходимости, привлекаем дополнительные ресурсы для комфортной работы каждого сотрудника Наши команды активно расширяются, а процессы и коммуникации всегда остаются эффективными  Откликайтесь и станьте частью большой команды профессионалов ЦУМ! БЫТЬ ЧАСТЬЮ КОМАНДЫ ЦУМ - ЭТО:  Официальное трудоустройство; График работы: 5/2 с 10:00 до 19:00 (сб., вскр. - выходные); Уровень дохода обсуждается индивидуально по итогам интервью; Компенсации и льготы: корпоративное кафе с вкусным комплексным питанием по специальной цене для сотрудников, ДМС, включающий стоматологию и страховку для выезжающих за рубеж; Работа в динамично развивающейся и уникальной по структуре, масштабам и возможностям компании с культовыми мировыми брендами класса люкс; Неограниченный доступ к офлайн и онлайн курсам Академии ЦУМ, а также эксклюзивным тренингам от представителей брендов; Место работы: БЦ &quot;Романов Двор&quot; (м. Боровицкая, Александровский сад), поддерживаем гибридный формат работы.  Задачи:  Поиск, сбор, подготовка, анализ и визуализация данных в BI; Поддерживать существующие ETL процессы и реализовывать новые;  Участие в инфраструктурных проектах и доработках; Предоставлять экспертную поддержку потребителям данных по вопросам, связанным с их использованием; Спроектировать и разработать витрины данных, подготовить описания для них.  Пожелания к кандидатам:  Опыт работы в роли Data Engineer/ETL Developer/BI Аналитик/Продуктовый аналитик; Уверенно владеть SQL на продвинутом уровне;  Владеть Python как средством разработки ETL-процессов; Опыт работы с любой из BI-систем в качестве разработчика; Обладать навыками проектирования витрин данных; Умение декомпозировать задачи, структурировать и оптимизировать решения.  Будет преимуществом:  Опыт работы с оркестраторами; Опыт работы с BigQuery. "
"70434197","РТ Лабс","Data Engineer","False","None","None","От 3 до 6 лет","Удаленная работа","['Git', 'SQL', 'Apache Kafka', 'NoSQL', 'Kafka', 'Hadoop', 'Greenplum', 'Python']","Наша команда объединяет более 1300 IT специалистов в 13 офисах по всей стране. Мы гордимся нашими проектами:  Портал gosuslugi.ru, число пользователей которого достигает 130 млн Высоконагруженные системы электронного правительства Инновационная платформа биометрической идентификации Передовые отраслевые и бизнес-решения  Мы ищем профессионалов, способных мыслить глобально и создавать тренды. Сейчас нам требуется Data Engineer, способный быстро погрузиться в новые задачи и усилить нашу команду. Что ты уже умеешь:  Знание SQL и оптимизация запросов; Опыт разработки с использованием Python Опыт работы с реляционными СУБД (Oracle / PostgreSQL / MSSQL); Опыт разработки и внедрения алгоритмов работы с данными (матчинг данных и др.) Опыт визуализации данных (BI, Python: Pandas+matplotlib и т.д) СОВСЕМ КРУТО, ЕСЛИ: Знаком с экосистемой Hadoop (HDFS, Hive, Spark, HBase, etc.) Знаком с MPP системами (Greenplum, Teradata, Vertica и т.д.)  Что ты будешь делать:   Стек технологий: Python, Airflow, NiFi, Hadoop, Spark, Kafka, Greenplum   Методология: Agile   Участвовать в разработке Data Lake и хранилища данных в роли Data Engineer;   Участие в разработке архитектуры ETL процессов и инструментов для работы с данными;Участие в тестировании;   Обеспечение контроля качества данных, мониторинг, анализ и устранение проблем с качеством данных.   Что мы предлагаем:  Опыт работы над социально значимыми проектами; Официальное трудоустройство в соответствии с ТК РФ с первого рабочего дня; Возможность работать удаленно; Конкурентную заработную плату; Квартальные и годовые бонусы; Кафетерий льгот; ДМС + стоматология и международная страховка; Интересную программу внутреннего и внешнего обучения. "
"68380037","СБЕР","Data Engineer (Комплаенс)","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'Java', 'Python', 'hadoop', 'QlikView']","Команда занимается развитием платформы данных Комплаенс. В задачи команды входят:  аналитика источников и подготовка технических требования на их использование; разработка витрин данных для построения отчетности, исследований и интеграционных потоков; снабжение данными процессов Комплаенс; внедрение прогнозных моделей; разработка отчетных форм; мониторинг процессов передачи данных;  В задачи сотрудника будет входить:  разработка механизмов и объектов для хранения данных, аналитика данных в источниках (исследование, проектирование и разработка модели данных и хранилищ и витрин данных, механизмов наполнения хранилищ из источников данных, разработка enterprise решений). Так же потребуется написание unit-тестов на разработанный функционал (где применимо).  Функционал:  загрузка источников Комплаенс в ядро Облака данных; подготовка данных для аналитиков; разработка витрин данных; автоматизация расчёта витрин и моделей данных; интеграция с потребителями рассчитанных значений; визуализация данных Комплаенс (аналитическая отчётность).  Ожидаемые знания и умения:  Hadoop, ANSI SQL, PL/SQL, PL/pgSQL, Java, Python, ETL; владение SQL; навыки проектирования баз данных; знания в области мат. статистики; навыки разработки на Java или Python; умение документировать разработку.  Дополнительно:  опыт построения интеграционных решений; знание Scala,брокеров сообщений, ETL-инструментов, в том числе IPC будет плюсом.  Условия:  Конкурентная оплата труда + бонусы по результатам работы; Возможность посещения всероссийских и международных IT-конференций; Профессиональное развитие: тренинги, митапы, мастер-классы, изучение английского языка не выходя из офиса, доступ к различным образовательным платформам; ДМС и страховка от несчастных случаев, льготные условия для близких родственников; Льготные предложения от компаний-партнеров, корпоративные условия от ведущих фитнес-клубов и многое другое; Офис в бизнес центре “Даниловская мануфактура”. Шаговая доступность от МЦК Верхние котлы, станции метро Тульская или офис метро Кутузовская, 7 мин пешком. Корпоративный транспорт, парковка для сотрудников. "
"70519280","WILDBERRIES","Data engineer","False","None","None","От 3 до 6 лет","Полный день","[]","Задачи: • Участие в создании новых и эффективных ETL-процессов• Построение надежной и эффективной архитектуры для работы с данными• Настройка и поддержка системы для обработки больших баз данных• Написание и оптимизация запросов• Взаимодействие с другими подразделениями по вопросам обработки данных• Оптимизиации пайплайнов• Создание продукта, который сделает лучше пользовательский опыт   Мы ожидаем: Основные требования:• Высокий уровень SQL и Python• Релевантный опыт работы, суммарно от 3-х лет• Опыт написания серверов• Иницативность в вопросах оптимизации• Быстрая адаптивность к новым задачам и ориентация на результатДополнительные (необязательные) требования:• Знание Kafka• Опыт работы с базами типа ClickHouse, Greenplum, PostgreSQL• Опыт работы с Redis• Знание Linux на уровне уверенного пользователя• Опыт работы с Kubernetes   Мы предлагаем: • Интересные проекты, которым нет аналогов• Команду, которая стремится внедрять самые эффективные high-технологии• Конкурентный уровень оплаты труда• Полностью удаленный формат работы или гибридный график работы• Комфортабельный офис на ст м. Автозаводская• Гибкое начало и окончание рабочего дня• Возможности для профессионального развития и роста• ДМС после прохождения испытательного срока• Официальное оформление по ТК РФ, либо контракт, как с самозанятым, ИП, ГПХ"
"68619509","Skillbox","Спикер на курс по Data Engineering","False","None","None","От 3 до 6 лет","Гибкий график","['Python', 'Data Science', 'SQL', 'Scikit-learn', 'Numpy', 'Pandas', 'Big Data', 'Matplotlib', 'Keras', 'Git', 'sklearn', 'NiFi', 'Prometheus', 'Airflow', 'ClickHouse', 'Spark', 'Kafka', 'Flask', 'Docker', 'Grafana', 'SCALA', 'Engineering', 'Hive', 'hdfs']","Skillbox – крупнейшая образовательная онлайн-платформа в России, объединяющая ведущих экспертов и практиков рынка. На сегодняшний день мы предлагаем 780 образовательных продуктов: курсов дополнительного профессионального образования, программ бакалавриата, магистратуры и бизнес-образования (совместно с ведущими университетами страны), а также корпоративное обучение для бизнеса. Благодаря онлайн-образованию Skillbox уже изменил жизни сотен тысяч людей к лучшему. И мы верим, что это только начало. Прямо сейчас наша команда в поиске Спикера для совместной работы над созданием курса &quot;Data Engineer&quot;. Курс рассчитан на студентов знакомых со сферой Data Science (прошедших курс “Введение в Data Science”), но не имеющих опыта в Data Engineering. Чем предстоит заниматься:   Подготавливать материалы к съемкам видеоуроков (планы модулей, сценарии, презентации)   Подготавливать дополнительные материалы к модулям (наборы заданий, инструкции проверяющим преподавателям и т.п.)   Участвовать в съемках видеоуроков в качестве преподавателя (спикера)   Взаимодействовать с командой разработки по улучшению и исправлению уже созданных материалов   Мы будем рады видеть тебя в нашей команде, если:   Обладаешь опытом работы на позиции Data Engineer от 2-ух лет   Имеешь техническое образование (желательно)   Обладаешь уверенными знаниезнаниями SQL: сложные запросы, аналитически функции, понимание физической реализации join’ов, оптимизация производительности запросов   Знаешь один или несколько языков программирования: Java, Python, Scala. Python - обязательно   Имеешь опыт разработки промышленных решений по обработке больших объёмов данных   Имеешь навык работы с инструментами из списка ниже, не обязательно владение всеми, но чем больше, тем лучше: Docker, Kafka, NiFi, Prometheus, Push Gateway, Grafana, HDFS, YARN, Spark, Hive, ClickHouse, Airflow, Greenplum, Flask   Есть мотивация на создание качественного образовательного продукта   Умеешь грамотно выражать мысли как устно, так и письменно   Имеешь опыт преподавания (желательно)   Что мы готовы предложить:   Проектная работа, гибкий график по согласованию с продюсером   Занятость в среднем 7-10 часов в неделю   Возможность как совмещать с основной работой, так и работать на полной ставке   Поддержка от методистов, продюсеров и других членов команды   Если у вас созрела необходимость структурировать и поделиться своими знаниями, если вы хотите прокачаться как спикер, повысить уровень медийности, стать для сотен студентов “точкой входа” в профессию, и влиять на формирование кадрового потенциала в ИТ — давайте сотрудничать."
"68702292","ЛОКО-БАНК","Data Engineer (Big data)","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'Linux', 'Hadoop', 'MS SQL', 'PostgreSQL']","Обязанности:  Разработка и поддержка компонентов Big Data платформы сбора, обработки и анализа данных; Разработка ETL - процессов; Построение аналитических витрин данных; Построение автоматизированных процессов обеспечения и контроля качества данных; Проектирование и реализация интеграций данных различных проектов; Выбор и тестирование новых технологий; Участие в проектировании и развитии архитектуры Big Data платформы.    Требования:  Опыт работы от 1 года; Хорошее знания SQL; Хорошее знание OS Linux / bash; Опыт работы с базами данных MS SQL, PostgreSQL; Опыт работы с Apache NiFi \ Spark \ Airflow \ Hive; Опыт работы с экосистемой Hadoop и решениями Big Data; Опыт разработки на Python; Желателен опыт работы с Docker \ Flask; Желателен опыт вывода ML моделей в production.    Условия:  Местоположение офиса: Москва, Ленинградский проспект 39 стр. 80. Возможен удаленный формат работы. Режим работы: с 9.30 до 18.30, в пятницу с 9.30 до 17.30 Финансовые условия обсуждаются индивидуально, в зависимости от профессионального опыта и стажа работы Социальный пакет: компенсация ДМС/фитнес (на выбор сотрудника), льготное кредитование, скидки на туристические и иные услуги от партнеров Банка, внутреннее обучение за счет Банка в Учебном центре.   "
"69822593","Платформа ОФД","Team Lead Data Engineer (Big Data)","False","None","None","От 3 до 6 лет","Гибкий график","['Python', 'Java', 'SCALA', 'SQL', 'Big Data', 'Hadoop', 'Spark', 'ETL', 'Docker']","«Платформа ОФД» - аккредитованная IT-компания, крупнейший в России оператор фискальных данных. Мы создаем полезные и удобные сервисы для предпринимателей. Делаем рыночную аналитику на основе данных из магазинных чеков. Компания резидент Сколково, входит в Экосистему Сбера.Приглашаем на работу Руководителя отдела подготовки и сопровождения баз данных Что у нас есть:  Много данных: каждый 3-й чек, пробиваемый в России, находится на наших серверах. Мы принимаем до 50 млн чеков в день, имеем 2 млрд уникальных названий товаров в базе Развитая инфраструктура: есть несколько кластеров Hadoop, мощные машины, GPU Команда Big Data: продакты, аналитики, ресечеры, DS и DE 80% кода мы пишем на Scala Железо и лицензии от компании Оформление по ТК РФ, белая зп Отсрочка от мобилизации, согласно условиям Минцифры Гибридный график работы (офис + дом). Гибкий график. ДМС (поликлиника, стоматология, госпитализация, скорая) Скидки в фитнес-клубы, мерч, подарки детям к праздникам Офис в 50 метрах от м. Спортивная/ МЦК Лужники. Удобная кухня, релакс-зона с тренажером, массажным креслом, приставкой и караоке Открытая рабочая атмосфера: ежемесячные статус - митинги с топами Пицца-пати за счет компании и корпоративные праздники  Наш стек: Hadoop, Spark, Hive, SCALA, PostgreSQL, ClickHouse, Zeppelin/IntelliJ, AirFlow, ElasticSearch, Apache Superset Jupyterhub/PyCharm, python, стандартный python стек (pandas, numpy, sklearn, matplolib), fastext, torch, BERT, HuggingFace, MlFlow GitLab, k8s, Docker, Jira, Confluence Чем предстоит заниматься:  Управлять командой, состоящей из 4 Data Engineer`s Оптимизировать инфраструктуру и внутренние сервисы по обработке больших данных Участвовать в проектировании архитектуры DWH и витрин данных Разрабатывать ETL-процессы с использованием библиотеки Apache Spark на Scala Анализировать, проектировать и создавать витрины данных Сотрудничать с DS для внедрения математических алгоритмов и ML-моделей в промышленные процессы  Откликайся, если ты:  От 3-х лет решаешь задачи в области сбора, хранения и анализа данных Имеешь опыт работы на позиции Team Lead от года Пишешь на Scala, Python/Java, SQL Используешь инструменты BigData: Airflow, Hadoop, Spark, Hive, Zeppelin Знаешь Docker и k8s   "
"69295100","СИТИЛИНК","Data Engineer/Разработчик DWH+ETL","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'Big Data', 'ETL', 'Python\\AirFlow', 'E-Commerce', 'Big Query', 'MS SQL\\Vertica\\ClickHouse']","Требуемые компетенции:  Высшее образование (Профиль: математический, технический); Опыт внедрения проектов корпоративных хранилищ данных (DWH), построения/развития корпоративной архитектуры, обеспечивающей данными отделы data science, аналитики и отчетности, планирования и контроллинга Опыт разработки/работы c ETL процессов Опыт работы с MS SQL / Vertica или другими MPP (Teradata, Greenplum, ClickHouse) - крайне желательно Уверенный уровень SQL Участие во внедрении практик Data Governance и систем контроля data quality в контур управления данными Опыт разработки на Python от 1 года Опыт работы с Power BI, GA360, Big Query (желательно)  Функции:  Разработка и доработка Big Data Pipeline, участие в построении и эксплуатации конвейеров данных (Python\AirFlow, Spark и пр.) Разработка и доработка витрин данных и скриптов Vertica, ClickHose (развитие логической и физической моделей данных) Ведение глоссария данных и метрик, семантического слоя, описания схем данных (есть отдел DataGov, участие в этих процессах со стороны инженеров) Взаимодействие с аналитическими службами и IT в области компетенции Участие в развитии и оптимизации текущей DATA архитектуры на стеке продуктов Vertica, ClickHouse, GA360, Big Query, Табулярных моделей кубов, Power BI, Anaplan, Loginom и технологий обработки и транспорта данных (Python\AirFlow, Spark, NiFi, Kafka) оптимальность, быстродействие, перспектива, недостающие/лишние звенья  Условия работы:  Официальное оформление, &quot;белый&quot; доход. Оклад и квартальные премии (обсуждается по итогам интервью) Гибкий формат удаленка/офис и гибкое начало рабочего дня Комфортабельный офис располагается по адресу: Бульвар Строителей д.4, строение 1, БЦ Кубик. Шаговая доступность от ст. м. Мякинино, транспортная доступность от станций метро: Строгино, Щукинская, Тушинская, Трикотажная Открытая атмосфера и позитивные коллеги Мы заботимся о сотрудниках: корпоративный соц. пакет, скидки на товары и услуги Ситилинка и компаний-партнеров (парковка у офиса, фитнес, здоровье и красота, недвижимость, изучение языков, отдых и многое другое)  Особенности работы:  Вакансия в отделе &#39;Отделе архитектуры корпоративных данных’, а значит главными заказчиками/партнерами по работе будут коллеги из единого централизованного &#39;управления аналитики и планирования’ в компании: ‘отдел планирования и контроллинга’, &#39;отдел коммерческой аналитики (BI)’, ‘отдел веб-аналитики’, ‘отдел data science’ Управление временем в JIRA, работа по спринтам   "
"68607121","Детский Мир","Big Data Engineer (Spark, Hadoop)","True","None","304500","От 3 до 6 лет","Удаленная работа","['Spark', 'Hadoop']","Нашему будущему коллеге предстоит работать над проектом: OMNI-дашборды - реализовывать систему визуализации отчетности для покрытия потребности в аналитических инструментах для высшего и среднего менеджмента Компании. Работать над развитием Корпоративного Хранилища Данных (Hadoop), которое является основным источником данных для этого проекта. Также, в работе у команды будет проект по переезду с google analytics на snowplow+ custom storage. Через несколько месяцев мы планируем проект по развитию Больших данных клиентской аналитики. Цель проекта – реализовать уникальную систему по работе с клиентскими данными ПАО &quot;Детский мир&quot; на основе используемых компонентов и стэка технологий и программного обеспечения компании. По итогам проекта в компании появится единый источник непротиворечивой и консистентной информации для принятия управленческих решений. Мы ищем эксперта с высокой ролью ответственности на проекте, умеющего самостоятельно и в команде принимать решение по архитектуре и реализации. Наш стек:  последние версии Apache Spark и Apache Airflow Hadoop 3 Docker, Kubernetes GitLab для CI/CD  Что нужно делать?  создавать Spark ETL pipeline для загрузки данных в HDFS и преобразования данных на HDFS обсуждать с аналитиками алгоритмы преобразования данных, переводить SQL от аналитиков в Spark API участвовать в code review проектировать и создавать архитектуру проекта и адаптировать ее под новые требования выстраивать процессы CI/CD, мониторинга заниматься развертыванием REST-сервисов и вспомогательной инфраструктуры (Airflow, Zeppelin, Spark History/Thrift Server) в Kubernetes  Наши ожидания:  опыт создания Spark batch jobs, желательно Scala API (PySpark обсуждаем, но в настоящий момент не используется) опыт работы с Airflow, умение создавать DAG&#39;и, состоящие из Task и Sensor. знания SQL опыт работы с Docker обязателен понимание жизненного цикла разработки ПО, культуры CI/CD  Будет плюсом:  опыт работы с Kubernetes опыт работы с Helm опыт работы с Kafka проектирование сбора/визуализации метрик с использованием Prometheus/Grafana  Мы предлагаем:  Официальное оформление в соответствии с ТК РФ, официальная аккредитация IT компании (выписки по форме от генерального сотрудникам делаем) Разноплановые и нестандартные задачи, уникальный опыт, отсутствие легаси Продуктовая команда, отсутствие бюрократии и длительных согласований, заинтересованный в результате бизнес Высокий уровень оклада + годовая премия График работы 5/2, гибкое начало рабочего времени, возможна частично-удаленная/или удаленная работа на выбор , офис Москва м/мцк/мцд Окружная. Можно работать из другой страны. Расширенный полис ДМС Скидка на продукцию компании Выдаем современные макбуки Льготная ипотека, отсрочка от армии "
"69418341","Иннотех, Группа компаний","Data Engineer направления разработки витрин данных","False","None","None","От 1 года до 3 лет","Полный день","['Hive', 'Hadoop', 'Git', 'Python', 'SCALA']","  Data Engineer Вместе с нами ты будешь:  Заниматься проектированием и разработкой витрин данных для анализа и моделирования Заниматься мониторингом и оптимизацией процессов сборки витрин Заниматься загрузкой и обработкой данных из различных источников Заниматься поддержкой и развитием базы знаний Предоставлять экспертную поддержку внутренним потребителям(data analysts,data scientists)  Какие знания и навыки для нас важны:  Знание SQL Хорошее знание устройства Hadoop,Spark,Hive/Impala Опыт разработки на Python/Scala/Java Понимание основных концепций DWH Понимание базовых команд Git и основных приципов работы  Будет плюсом:  Опыт работы с банковскими данными Опыт работы с различными СУБД в роли разработчика/аналитика витрин данных Опыт работы с Airflow   "
"69610051","Playrix","Lead Data Engineer","False","None","None","От 1 года до 3 лет","Удаленная работа","['Python', 'SQL', 'PostgreSQL', 'Анализ данных', 'Flask', 'AWS', 'TeamCity', 'Базы данных', 'Big Data', 'Spark']","Playrix – является крупнейшей игровой компанией в Европе и входит в топ-3 самых успешных мобильных разработчиков в мире. Мы создали такие хиты, как Gardenscapes, Fishdom, Manor Matters, Homescapes, Wildscapes и Township, которые скачали более 2 миллиарда игроков. В игры Playrix каждый месяц играют более 120 миллионов человек. Наша команда занимается разработкой и поддержкой программных продуктов для извлечения, трансформации, выгрузки и анализа данных. Объем данных в нашем Data Lake больше 2.5 петабайт: маркетинговые метрики, игровые события, параметры операционной деятельности. Мы делаем всё возможное, чтобы не возникало ни малейших сомнений в полноте, актуальности и достоверности информации, которую мы предоставляем. Особое внимание уделяем скорости обработки и качеству данных. Это позволяет нам принимать верные решения относительно развития наших игр! Для нас Tech Lead — это сильный технический специалист, который совместно с Product Owner может обеспечить стабильную работу команды, создающей внутреннюю платформу по автоматизации бизнес-процессов компании. Наши задачи    архитектурная проработка, проведение PoC, разработка и улучшение наших сервисов;   контроль выполнения задач, работа с рисками;   найм, развитие, оценка и мотивация сотрудников;   участие в составление Roadmap продуктов совместно с Product Owner;   внедрение показателей для команды, сбор и анализ статистики, выявление узких мест;   развитие процессов и их прозрачности для команды и внешних стейкхолдеров.   Наш стек Наш основной язык программирования - Python. Для доступа к данным используем SQL. Datalake построен на S3, Parquet и Glue. В качестве DWH используем Redshift/Postgresql. Работаем в облачной инфраструктуре AWS. Используем решения мейнстримовых вендоров (Databricks, MonteCarlo, DBT). Практикуем serverless подход работы с ресурсами и горизонтальное масштабирование, используем предиктивные модели. Уделяем внимание рефакторингу и кода, и архитектуры. CI/CD - TeamCity. . Мы ожидаем   опыт управления командой от 2 лет (6+ человек);   отличное знание Python, опыт разработки на нем, умение читать и анализировать чужой код;   навыки написания и оптимизации SQL запросов, готовность прокачивать свои знания и писать более сложный код на SQL;   широкие представления о средах и инструментариях разработки, технологиях, фреймворках;   опыт разработки архитектуры.   Будет плюсом   опыт работы с большими данными или BI;   знание предметной области маркетинга;   опыт работы в любой облачной инфраструктуре (AWS, Google Cloud, Azure);   опыт работы с системами управления ETL (Luigi, Airflow);   знание Tableau, опыт работы с backend web-приложений (Django/Flask);   аналитические навыки работы с данными.  "
"69049305","Звук","Senior Data Engineer (Streaming)","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'Spark', 'SCALA', 'Java', 'Hadoop', 'Kubernetes', 'Flink', 'Kafka']","Звук - аккредитованная ИТ компания Чем нужно будет заниматься?  Оптимизировать Spark Streaming джобы. Трансформировать старый ETL из родного overnight batching в модный молодёжный realtime streaming на Spark3/Flink. Создавать пайплайны NRT обработки данных для аналитики и рекомендаций. Подключать новые источники событий. Внедрять Data Quality Gates и лучшие практики работы с данными.  Что мы ожидаем от кандидата:  Релевантный опыт работы на scala/ java от 2-х лет. Практический опыт работы с spark, Flink, hadoop, kafka. Опыт работы с stream и batch обработкой данных. Знакомство с kubernetes и опыт его использования в работе. Будет плюсом:  Python: у нас много кода на питоне, но уверенно идем к скала платформе. Опыт разработки инструментов контроля качества данных. Опыт работы с airflow. Опыт взаимодействия с Presto/Trino, clickhouse, cassandra, Scylla, GreenPlum.  Работа должна быть в удовольствие, поэтому:   Мы аккредитованная ИТ-компания, все преимущества распространяются на наших сотрудников в полном объеме  Возможность участвовать в развитии продуктов, которыми пользуются миллионы Стильное рабочее пространство рядом с м. Кутузовская (7 минут от метро и МЦК) Возможность работать удаленно или в гибридном графике, выбирать начало и окончание рабочего дня, брать дополнительные дни отпуска ДМС с первого рабочего дня Заботу о психологическом здоровье и компенсацию затрат на платформе «Ясно» Полезные завтраки и перекусы в офисе каждый день Занятия в офисном спортзале, йогу и фитнес, компенсацию затрат на спорт Оплату участия в конференциях и помощь в подготовке выступлений, скидки на изучение английского, доступ к корпоративным обучающим ресурсам Возможность делиться новостями и экспертизой с коллегами на ежемесячных митапах и внутренних мероприятиях Ламповые вечеринки с пиццей и настолками под гитару, вечеринки артистов в офисе каждый месяц!  А еще, у нас есть своя музыкальная группа)  "
"68335494","Лига Цифровой Экономики","Data Engineer/ Разработчик DWH/ ETL - разработчик","False","None","None","От 1 года до 3 лет","Полный день","[]","Проект в Банке ТОП-5 по России. По созданию информационной платформы Блока инвестиционных технологий. Твои задачи:  Разработка и оптимизация Data Warehouse на Greenplum с использованием техник моделирования Data Vault и Star/Snowflake Scheme; Разработка и оптимизировать ETL/ELT потоки загрузки данных из различных источников данных (Hadoop, Greenplum, flat files, kafka, REST API, etc); Оптимизация/автоматизация существующих процессов разработки; Написание авто-тестов.  Что мы ждем:  Опыт работы в роли разработчика ПО от 2х лет; Понимание архитектуры современных аналитических систем и принципов межсистемной интеграции; Понимание стандартов и методологий разработки ПО, в том числе инженерных практик (VCS, разработка на dev, code review, написание тестов, CI/CD). Понимание основ ООП; Опыт разработки/ понимание хотя бы одного из языков общего назначения (Python или Java или C++ или C#); Опыт работы с хотя бы одной из реляционных СУБД (Teradata или Greenplum или Vertica или Sybase IQ или Oracle или DB2 или MS SQL Server или PostgreSQL); Опыт разработки процессов сбора, хранения и обработки больших массивов данных из различных источников данных (РСУБД, Queues, web-сервисы, файлы, логи); Опыт использования хотя бы одного из ETL-инструментов (Informatica или IBM DataStage или SAS DI или Oracle DI или Talend или Pentaho); Опыт проектирования физической модели данных (Data Vault, 3NF, Ancor, Dimensional Modeling); "
"68874939","СБЕР","Data Engineer/Support Engineer","False","None","None","От 1 года до 3 лет","Полный день","[]","В SberData мы создаем централизованное хранилище данных всего Сбера. Это более 350 источников данных и 100+ Пб информации, заказ и получение данных за 15 минут и современный технологический стек работы с данными, включая собственные сборки СУБД на базе Hadoop и Greenplum Находимся в поисках Data Engineer/Support Engineer в команду &quot;Корректировки данных&quot; управления сопровождения корпоративной аналитической платформы. То, почему именно наша команда:  работа над сопровождением нескольких крупных проектов хранилищ данных на базе Apache Hadoop, Teradata, Oracle Hadoop Ecosystem: Сверхмощный кластер для аналитики, Apache Spark, Hive, HBase, Kafka, NiFi оптимизация: Performance Tuning, Совершенствование процессов мониторинг: качество данных, управление Мастер-данными, поставка данных  Тебе предстоит:  разбор и решение инцидентов по платформам и сервисам, находящимся в зоне ответственности взаимодействие с разработчиками, бизнесом и второй линией сопровождения. Инженер является связующим звеном между бизнесом и данными анализ и классификация причин проблем функционирования прикладного ПО, выработка предиктивных мер по устранению будущих инцидентов участие в приемо-сдаточных испытаниях прикладного ПО, передающегося в промышленную эксплуатацию оказание консультаций по ПО бизнес-пользователям, 2-й линии сопровождения выпуск исправляющих патчей по дефектам сопровождаемого ПО исследование состояния и поведения системы, анализ причин отклонений, локализация дефектов, взаимодействие с разработчиками и аналитиками для описания текущего и целевого поведения. Создание и контроль задач в JIRA.  Что для нас важно:  желательно высшее техническое образование. Отсутствие высшего образования не является стопером, если Вы отличный инженер или разработчик опыт работы функциональным/системным аналитиком, Data инженером или разработчиком от 2х лет. понимание основных принципов функционирования Систем Хранения Данных (OLTP, DWH); уверенное владение SQL (плюсом будет знание PSQL); опыт работы с несколькими ведущими СУБД, предпочтительно Oracle, Teradata, PostgeSQL; понимание устройства и работы клиент-серверных приложений, приложений микросервисной архитектуры; знание принципов создания интеграции систем и компонентов через REST API. английский язык для чтения технической документации (общение внутри коллектива или подрядчиком на английском не требуется); коммуникабельность (обязательное требование из-за масштаба компании), стрессоустойчивость; желателен опыт работы со стеком технологий Hadoop, в частности с Apache Hive, Spark, Kafka. Python является плюсом.  Работа в Сбере - это:  гибкий график для оптимального баланса работы и личной жизни место работы: г. Москва, м.Нагатинская/Тульская/Верхние котлы/Коломенская, бизнес-центр &quot;Даниловский Форт&quot; профессиональный рост в команде дружных профессионалов ДМС, страхование жизни самые инновационные, амбициозные проекты и задачи. свободный дресс-код. льготные кредиты и корпоративные скидки профессиональное обучение, семинары, тренинги, конференции, корпоративная библиотека конкурентная компенсация (оклад и премии по результатам деятельности). "
"68608637","Tele2","Data-engineer (в подразделение digital-analytics)","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'SQL', 'Spark', 'Анализ данных', 'Обучение и развитие']","О проекте: Мы занимаемся сбором и анализом данных для высоконагруженных сервисов компании(web-сайт, app и т.п.) с более чем 3 млн. пользователей ежедневно.Наша задача помочь бизнесу найти смысл в данных и принимать data-driven решения вразвитии сервисов. Мы ищем человека, который поможет нам развивать наше решение по сбору и обработкеданных. Ваши будущие задачи:  Поддержка и развитие платформы clickstream; Развитие DWH аналитики; Автоматизация процессов работы с данными.  Что ожидаем от вас:  Хорошее знание Python; Высокий уровень владения SQL; Понимание архитектуры data-processing сервисов.  Будет плюсом:  Опыт работы с Docker; Знание Airflow; Опыт работы с Kafka; Использование Spark.  Плюсы для вас:  Интересная работа в быстроразвивающейся компании; Уникальная система обучения для каждого сотрудника на основе индивидуальных планов развития; Зеленый свет для новых идей и предложений: мы часто делаем то, на что другие не отваживаются; Возможности профессионального и карьерного роста; Белая заработная плата, годовые бонусы; Полное соответствие ТК РФ; Расширенная медицинская страховка в России и за пределами страны; Компенсация затрат на мобильную связь; Дополнительные материальные выплаты (пособия при рождении ребенка, вступлении в брак и т.п.); Компенсация занятий спортом через год работы. "
"52666026","ОМЕГА","Data инженер","False","None","None","От 1 года до 3 лет","Гибкий график","[]","Крупная компания (РФ и РБ), дистрибьютор запасных частей к европейским грузовым автомобилям приглашает на работу инженера по данным (Data Engineer) Мы развиваем сложные информационные системы, чтобы грузовой транспорт мог ежедневно доставлять товары в разные точки страны.Мы разрабатываем собственное комплексное решение с функциями товародвижения, системы управления складом и двором (WMS,YMS), блоком взаимоотношений с клиентом (CRM) и продажами (POS), с блоком аналитики.Имеем развитую систему филиалов и магазинов, интегрированных в корпоративную сеть и связанных с ЦОДами. Ищем в команду инженера по данным, который готов продолжить своё развитие в высоко технологичной и динамически развивающейся компании Обязанности: - создание корпоративного хранилища данных (DWH); - развитие и переработка существующих OLAP-кубов; - развитие ETL процессов; - подготовка и сопровождение витрин данных для BI визуализации и отчетов; - построение плоской, многомерной отчетности и дашбордов; - технологический стек: Основа - MS SQL Server/Integration/Analysis/Reporting Services/Power BI; Требования: Обязательно: - опыт работы в качестве разработчика баз данных, DWH, ETL или OLAP в течение 2 лет; - знание теории и практический опыт построения баз данный, хранилищ данных; - отличное знание T-SQL (планы выполнения, оптимизация); - MS SQL Server (Database Engine, Integration Services, Analysis Services, Reporting Services, Power BI); - базовые знания MDX, DAX; Желательно (будет плюсом): - базовые знания платформы .NET; - знание одной из торговых учетных систем (1С, SAP, и др.); - экспертное знание принципов работы MS SQL (хранение данных, индексы, статистика, транзакции, уровни изоляции транзакций, блокировки); Условия: - на испытательные срок – 5/2 в офисе;- после успешного прохождения испытательного срока возможна частичная или полная «удалёнка»;- офис расположен по адресу 2-я Мелитопольская ул. метро Бульвар Дмитрия Донского (в шаговой доступности от жд. ст. Бутово- позитивная, созидательная среда и интересные проекты;- работа в команде профессионалов, неравнодушных, активных, ответственных за свой результат. Почему у нас хорошо?Стабильность. Компания экономически устойчива и растёт каждый год.Честность. Мы работаем с полным соблюдением законодательства.Масштаб. Вы станете участником крупных проектов, полностью меняющих бизнес-процессы.Интерес. Вы попадёте в насыщенную событиями рабочую атмосферу крупной команды ИТ.Доверие. Лояльное руководство, отсутствие «чайка»-менеджмента.Комфорт. Мы за work-life balance – стараемся жить без переработок и отдыхать в выходные."
"67962758","ЕвроХим, Минерально-Химическая Компания","Data Engineer/Ведущий эксперт","False","None","None","От 3 до 6 лет","Удаленная работа","['ETL', 'SQL', 'Python']","Обязанности:  Создавать и улучшать процессы обработки данных; Разрабатывать схемы подключения новых источников в DataLake; Принимать участие в архитектурных решениях; Разрабатывать процедуры формирования детального слоя данных и слоя витрин DataLake/DataWareHouse; Документировать свою работу в Confluence  Требования:  Знания принципов работы БД, построения ХД; Опыты разработки ETL процессов (AirFlow); Опыт участия в разработке для систем Apache; Знание Kafka, AirFow, Spark; Опыт работы с высоконагруженными распределенными системами хранения и обработки данных; Отличное знание SQL, опыт оптимизации запросов; Опыт разработки на Python; Опыт работы с Jira, Confluence; Понимание задач по загрузке и трансформации данных (ETL, API); Стремление развиваться и изучать новые технологии; Коммуникабельность, умение работать в команде; Знание английского языка.  Будет плюсом:  Опыт взаимодействия Apache Superset, опыт работыS3 совместимыми хранилищами Знакомство с подходами Git OPS  Условия:  Крупные проекты в компании–лидере отрасли Возможность модернизировать процессы реального производственного бизнеса в команде сильнейших экспертов Оформление по ТК РФ в штат компании (бессрочный трудовой договор) Конкурентный уровень заработной платы Удобный график работы, возможность удаленного формата Программа заботы о здоровье сотрудников (ДМС) "
"69775868","Shortest Track","AWS Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Python', 'SQL', 'SCALA', 'Java', 'AWS', 'Glue', 'Athena', 'S3']","Job Summary  This role will provide our team with an experienced, creative, and proactive engineer. The ideal candidate will have extensive experience working with data science and analytics teams and will feel confident accomplishing data tasks within AWS (Amazon Web Services). This role requires extensive experience with S3, Athena, Glue, Step Functions, Lambda, and Redshift among other Amazon services.  Responsibilities    Participate as lead engineer on projects and assist in communication/collaboration sessions with US clients (CST)   Identify, build, and implement highly scalable Data pipeline with automation   Build out ETL jobs per architectural guidelines which integrates within the data pipeline   Work with stakeholders including executive and data teams to troubleshoot and identify issues within the Data Platform   Suggest and implement optimization patterns for an existing or new Data Platform   Identify and implement BI tools per architectural guidelines   Assist project team with oversight of junior team members: task delegation; technical assistance; oversight and QA   Ability to proactively identify performance gaps in a solution and provide improvements   Ability to work independently with minimal supervision   Ability to articulate a problem and find solutions in a timely manner   Required Qualifications:    Bachelor’s degree in computer science, mathematics, statistics, or a similar quantitative field   Excellent Python or Java programming skills   Solid SQL skills   Core understanding of Big Data principles and architectural patterns   Have proven experience in building out Data Lakes using AWS services and other CSPs   Deep understanding of AWS Data &amp; Analytics services (Kinesis, S3, EMR, Athena, Redshift, etc.)   Proven experience building or administering BI tools such as Tableau, Domo, Sisense, etc.   Be able to build data pipelines and platforms using proven development tools and languages such as AWS Glue, Eclipse, IntelliJ, Python, Scala, Spark, etc.   Experience implementing Machine Learning solutions and services on AWS or GCP   Strong customer focus, ownership, urgency and drive   Excellent communication skills. Russian native language, English C1+   Effective analytical, troubleshooting, and problem-solving skills   What do we offer  Remote High income Friendly team where we support each other Interesting and challenging tasks for large US companies "
"67507955","СБЕР","Data Engineer (Ранжирование)","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'Spark', 'SQL', 'SCALA', 'Hadoop']","Мы - новая быстрорастущая команда, которая разрабатывает платформу для создания и управления рекламными коммуникациями на поверхностях Банка и Экосистемы. Это платформа, которая объединит в себе последние технологические решения AdTech и возможности больших данных экосистемы. Наша цель - создать конкурентоспособный сервис на рынке рекламных технологий, который позволит показывать эффективную и полезную рекламу. Нам нужны талантливые и амбициозные специалисты в сфере ИТ, которые хотят создавать лучшие продукты. Команда ранжирования занимается созданием и развитием сервисов поиска наиболее подходящих под конкретного пользователя рекламных объявлений. Основными задачами являются оптимизация показов и формированием ставки, которая будет участвовать в рекламном аукционе. Мы работаем не только над предиктивными моделями и алгоритмами, но и над инфраструктурой работы с данными и применения наших алгоритмов. Мы ищем middle/senior дата инженеров, которые выведут нашу инфраструктуру на новый уровень по масштабу. Чем предстоит заниматься:  разрабатывать и оптимизировать пайплайны обработки данных на python от логов nginx до записи в ClickHouse создавать и развивать ETL слоя, на современном стеке технологий по работе с большими данными: Airflow, Kafka, Hadoop, Hive, Spark, Clickhouse внедрять и поддерживать MLOps технологий.  Мы ожидаем:  понимания как работают сложные системы по обработке данных или сильное желание в этом разобраться опыт промышленной разработки python/java/scala от 2-х лет хорошее знание python знание базовых алгоритмов и структур данных уверенное знание SQL и опыт работы с различными БД.  Несомненно будет плюсом:   опыт работы с BigData и/или работа по созданию ETL слоя опыт проектирования сложных систем по обработке данных понимание специфики ML задач привычка писать тесты.  Мы предлагаем:  интересные задачи по продуктам, влияющим одновременно на всю Экосистему: команда специалистов из топовых ИТ компаний регулярное обучение и профильные конференции, современное оборудование для работы уровень дохода, который готовы обсуждать и отталкиваться от ваших пожеланий, плюс премии комфортный офис и гибкий график множество плюшек от Сбера.   "
"70368102","Марс","Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Python', 'SCALA', 'SQL', 'Spark', 'Java', 'Data Platform', 'Yandex Cloud']","We are looking for experienced Data Engineer to join our team What will be your key responsibilities?  Design and develop data marts; Design, develop and support ETL processes for loading data to / from Data Lake / Factory Data Platform; Integrate new sources to DataLake/DWH/Factory Data Platform; Configuring and supporting filtering, routing, data queues to Factory Data Platform Configuring and supporting data post-enrichment, transformation, and contextualization in the Factory Data Platform Defining and supporting tag/data storage policy Quality control of uploaded data (manual and automated DQ tests);  What are we looking for?   Understanding database operation and DWH development principles;   Experience with Hadoop-based technologies (HDInsight, Spark, Hive, Scala, etc.);   Good knowledge of SQL, Python/Scala or Java;   1+ year experience with Yandex Cloud Platform/ Azure stack;   Experience with Greenplum as a plus;   Experience with Nifi, Kafka, Airflow as a plus;   English level - Intermediate or higher.   What can you expect from Mars?  Work with over 130,000 like-minded and talented Associates, all guided by The Five Principles. Join a purpose driven company, where we’re striving to build the world we want tomorrow, today. Best-in-class learning and development support from day one, including access to our in-house Mars University. An industry competitive salary and benefits package, including company bonus. "
"66537908","СБЕР","Data Engineer (Apache Airflow)","False","None","None","От 3 до 6 лет","Гибкий график","[]","В SberData мы создаем централизованное хранилище данных всего Сбера. Это более 350 источников данных и 100+ Пб информации, заказ и получение данных за 15 минут и современный технологический стек работы с данными, включая собственные сборки СУБД на базе Hadoop и Greenplum. Наши решения отмечены международной премией Data Award в 2021 г., а лидеры, обладающие уникальными знаниями в разработке кода и современном технологическом стеке С, Scala, Java, Python, Hadoop, Teradata, Oracle и др., являются участниками организации-фонда Apache Foundation. Масштаб задач, объемы данных, сложности финансовых процессов — мы все время на передовой современных технологий, а где-то и создаем их. Мы создаем платформу данных Экосистемы Сбера, которая позволит компаниям Экосистемы применять передовые «облачные» практики в своих процессах обработки данных и предлагаем вам погрузиться в самые инновационные и амбициозные проекты и задачи. Обязанности:  Развертывание, настройка и сопровождение Apache Airflow; Разработка и сопровождение ETL-процессов (DAG); Адаптация программных модулей Apache Airflow для требований заказчика; Участие в разработке архитектур с использованием Apache Airflow; Опыт работы с контейнерами и контейнеризированными приложениями; Формирование базы знаний по продукту.  Требования:  Знание и опыт работы с Apache Airflow, понимание принципов работы; Опыт разработки на Python; Понимание принципов работы с большими данными; Понимание работы компонентов Apache Hadoop; Знание SQL; Опыт работы с популярными РСУБД; Знание основ администрирования Linux.  Плюсом будет:  Знание средств автоматизации развертывания (Ansible, Puppet, etc.); Опыт работы с другими известными ETL-инструментами; Опыт разработки на других языка программирования; Опыт администрирования РСУБД.  Условия:  Профессиональное обучение, семинары, тренинги, конференции, корпоративная библиотека; ДМС, страхование жизни; Самые инновационные, амбициозные проекты и задачи; Свободный дресс-код; Гибкий график для оптимального баланса работы и личной жизни; Льготные кредиты и корпоративные скидки; Конкурентная компенсация (оклад и премии по результатам деятельности). "
"70346269","СБЕР","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","[]","В команду отдела расчета результативности и премий ищем Data engineer. Мы занимаемся учетом продаж и контролем премий, включая Ad Hoc аналитику для руководства банка. Мы предлагаем работу в одном из крупнейших в банке проектов в области построения хранилищ данных и автоматизации отчетности. Вы будете участвовать в развитии Big-Data хранилища с последующей миграцией на обновленный техстек. В нашей команде царит отличная атмосфера, дружелюбные руководители, приветствуется обучение, развитие и обмен опытом. Обязанности  Участие в развитии одного из крупнейших в банке хранилищ данных; Разработка на СУБД Oracle (80%), Teradata (20%) Разработка пакетов интеграции данных (ETL): Informatica Big Data/Informatica PowerCenter Разработка нового, еще более крупного хранилища данных на связке Hadoop (Hive/Spark) + Greenplum + Informatica Big Data + Kafka; Перевод процесса разработки на DevOps-конвейер Участие в проведении аналитических исследований и встречах с бизнес - подразделениями Банка по вопросам развития отчетности;  Требования  Разработка пакетов интеграции данных (ETL): Informatica Big Data/Informatica PowerCenter/SSIS/NiFi Разработка корпоративного хранилища данных (КХД) на связке Hadoop (Hive/Spark) + Greenplum Знание основ моделирования КХД Опыт работы с любой из реляционных СУБД: Oracle, Teradata, SQL Server, MySQL, Postgres Владение SQL (DML/DDL), навыки оптимизации SQL-кода  Условия · Работа в крупнейшем банке России; · Трудоустройство согласно ТК РФ; · Регулярное корпоративное обучение; · ДМС, страхование от несчастных случаев и тяжелых заболеваний; · Материальная помощь и социальная поддержка, корпоративная пенсионная программа; · Льготные условия кредитования."
"66187574","Май","Data engineer","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'MS SQL', 'Аналитическое мышление', 'Базы данных', 'Python']","Функциональные задачи    Организация автоматизированного сбора данных из различных источников в единое централизованное хранилище Настройка, интеграция и создание витрин данных для работы аналитиков и исследователей Контроль и повышение качества данных Цифровые компетенции     Понимание принципов работы СУБД, хорошее знание SQL (аналитические функции, процедуры и триггеры, планы запросов). Сейчас используем MS SQL, в перспективе ClickHouse Хорошее (middle+) знание python (взаимодействие с API, обработка данных) Будет плюсом:  Знание C# (junior+) Знание языка запросов Dax Опыт работы с MS Tabular Опыт работы с Apache Airflow Базовые знания ML    Требования к должности 1. Образование/профиль: высшее техническое 2. Специализация/квалификация математика, ИТ 3.Уровень владения компьютером: профессиональный 4.Уровень владения иностранным языком: не требуется 5. Дополнительное образование: не требуется 6. Наличие сертификатов: не требуется 7. Практический опыт: от 2-х лет работы на аналогичной должности   Офис находится м. Академическая Уровень ДВ высокий Оформление на работу по ТК РФ"
"67508908","СБЕР","Senior data engineer","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'PostgreSQL', 'Python', 'Java', 'MySQL']","Senior data engineer (ТОП-100) Мы - новая быстрорастущая команда, которая разрабатывает платформу для создания и управления рекламными коммуникациями на поверхностях Банка и Экосистемы. Это платформа, которая объединит в себе последние технологические решения AdTech и возможности больших данных экосистемы. Наша цель - создать конкурентоспособный сервис на рынке рекламных технологий, который позволит показывать эффективную и полезную рекламу. Нам нужны талантливые и амбициозные специалисты в сфере ИТ, которые хотят создавать лучшие продукты. Команда аналитической платформы Топ-100 отвечает за создание и развитие набора сервисов продуктовой аналитики для web и mobile ресурсов. Мы активно развиваем продуктовое направление, а также технологическую платформу сервиса. Каждый день в сервис поступает больше 1 млрд событий, которые обрабатываются с помощью spark streaming на кластере Hadoop и записываются в ClickHouse. В рамках данной позиции предстоит:  разрабатывать и поддерживать микросервисы для работы с большими данными в экосистеме Hadoop (Java/Scala, Spark, Hive, Kafka), а также создавать витрины данных в ClickHouse.  Мы ожидаем:  опыт разработки на стеке Java/Scala/Python от 2х лет поиск, обработка и построение витрин данных на инфраструктуре Hadoop опыт разработки приложений с использованием инструментов Spark Streaming, Hbase, Spark, Kafka, Hive, Impala, Hue и т.д. хорошие знания SQL, опыт работы с одной из реляционной БД - PostgreSQL/mySQL опыт разработки под ClickHouse (оптимизация запросов, понимание движка и способов шардирования и тд) понимание подходов к организации автоматизации разработки (CI/CD, DevOps, shift-right тестирование).  Мы предлагаем:  интересные задачи по продуктам, влияющим одновременно на всю Экосистему команда специалистов из топовых ИТ компаний регулярное обучение и профильные конференции, современное оборудование для работы уровень дохода, который готовы обсуждать и отталкиваться от ваших пожеланий, плюс премии комфортный офис и гибкий график множество плюшек от Сбера. "
"69310084","МАГНИТ, Розничная сеть","Senior Data Engineer (проект CVM)","False","None","None","От 3 до 6 лет","Удаленная работа","['Python', 'SQL', 'Spark', 'Hadoop']","Именно CVM позволяет укрепить имидж бренда и стимулирует повторные покупки.А как это сделать ещё лучше? Конечно погрузившись в данные с помощью PySpark и строить модели, двигаясь от простого sklearn к интересным моделям возвращения клиентов LSTM/RNN на TensorFlow. Используя персонализацию, наша команда превращает случайного покупателя в постоянного. В этом нам помогают алгоритмы поиска закономерностей, модели машинного обучения и нейронные сети. Наши задачи:  Работа в продуктовой команде по автоматизации и улучшению ETL процессов для моделей (Feature Engineering и т.д.); Работа в Yandex.Cloud над процессами Machine Learning; Развитие Feature Store для проектов Customer Success/Customer Value Management; Оркестрация и автоматизация в Airflow; Участие в проектах машинного обучения (разработка процессов тестирования, проверка Data Quality, стандартизация разработки); Развитие MLOps практик в команде; Построение и развитие REST API  Наши ожидания:   Опыт построения и автоматизации ETL/ELT процессов;   Понимание механизмов CI/CD;   Опыт работы с облачными платформами (YC, AWS, Azure) будет плюсом;   Python, SQL, PySpark, Shell;   Informatica, Docker;   Опыт работы с проектами автоматизации пайплайнов из Apache Foundation.   Стек: PySpark, Python, SQL Мы предлагаем:  График работы 5/2 с гибким началом; ДМС; Возможность работать в офисе в Москве и удаленно; Развитие – мы оплачиваем обучение на курсах/конференциях. "
"70528745","Северсталь","Data Engineer (Северсталь Диджитал)","False","None","None","От 1 года до 3 лет","Полный день","['Spark', 'Hadoop', 'Python', 'PostgreSQL', 'Hive']","Мы ищем дата инженера с желанием развиваться в работе с Hadoop для реализации поставки и предобработки данных в проектах по созданию моделей машинного обучения на производстве. Уже сейчас наши продукты управляют агрегатами размером с футбольное поле. Нам нужен человек, который на начальных этапах проектов готов разобраться в данных, описать их, наладить поток поступления данных, создать витрины в Hive с необходимыми агрегатами, а также подготовить пайплайны предобработки данных для дальнейшего переобучения и валидации моделей. Возможности: • Работа с проектами Data Science: сбор и предобработка больших массивов данных, создание пайплайнов, которые будут работать быстро и без ошибок • Работа с экосистемой Hadoop (Spark, HDFS, Hive, etc.) • Работа в небольшой команде, где ваши идеи будут быстро находить реализацию • Взаимодействие с Data Scientists для определения необходимых для построения модели данных и их производных Наш стек: • NiFi, MiNiFi • Apache Kafka, Avro Schema Registry • Apache Airflow • Hadoop, Hive, Spark • PostgreSQL • Redis Пожелания к знаниям: • Хорошее знание SQL, знакомство с реляционными базами Oracle, Postgres, MS SQL • Знакомство с экосистемой Hadoop (HDFS, Hive, Spark) • Опытный пользователь Linux • Желание изучать и внедрять новые фреймворки (Nifi, Airflow) Плюсом будет: • Знание Python/Scala, опыт работы с PySpark • Опыт работы с NiFi • Опыт проектирования ETL-пайплайнов • NoSQL базы данных Условия: • Трудоустройство по ТК РФ; • Льготы для сотрудников, которые распространяются для IT компаний (по законодательству); • Годовой бонус; ежегодная индексация з/п; • График работы 5/2 (40 часов в неделю), гибкое начало дня; гибрид/удаленка; • Корпоративная мобильная связь, ДМС и страхование жизни (c первого рабочего дня); • Классный новый офис с уютными зонами отдыха; • Оплата дистанционных курсов; • Офис м. Войковская, МЦК Балтийская и МЦД Красный Балтиец."
"69946807","Неофлекс","Data Engineer (Greenplum / Arenadata DB)","False","None","None","От 1 года до 3 лет","Полный день","['Greenplum/ArenadataDB', 'Informatica PowerCenter', 'SCALA', 'Spark']","ЧЕМ ТЫ БУДЕШЬ ЗАНИМАТЬСЯ:  Организацией процессов выгрузки данных из систем источников загрузки в хранилище под управлением Greenplum / ArenadataDB с использованием PXF; Анализом корректности и консистентности данных; Проводить доработку клиентского хранилища данных.  ТЫ НАШ ИДЕАЛЬНЫЙ КАНДИДАТ, ЕСЛИ У ТЕБЯ:  Уверенное знание SQL и опыт работы с Greenplum / ArenadataDB; Опыт оптимизации приложений на SQL, PL/pgSQL; Понимание принципов построения хранилищ данных. Опыт работы с Informatica PowerCenter; Опыт работы с python, scala, spark, аirflow.  СОВСЕМ КРУТО, ЕСЛИ ТЫ ИМЕЕШЬ:  Опыт работы с Prometheus, Grafana; Опыт работы с TeamCity, Git; Знание основ банковского бухгалтерского учета. "
"69084352","КИБЕР-РОМ","Senior data engineer","False","None","None","От 3 до 6 лет","Гибкий график","['Python', 'PostgreSQL', 'Big Data', 'RabbitMQ', 'Elasticsearch']","КИБЕР-РОМ — это неформальная обстановка, крутые ИТ-проекты в области медиаиндустрии и продукты, конкурирующие с лидерами рынка! У нас есть команда, которая создает ML решения, аналогов которых нет на рынке! Спроси себя, хочешь ли ты:  обрабатывать миллионы видео со стриминговых сервисов предоставлять пользователям релевантный контент искать структуры в неструктурированных данных бороться за каждую миллисекунду скорости делать продукт, который сделает лучше пользовательский опыт.  Наша команда растет и мы ищем Senior data engineer, который сможет организовать работу ML проектах в океане больших данных.В твоих силах сделать продукты, используя Hi-End решения, которыми воспользуются миллионы пользователей!BigData + Machine Learning + HighLoad &gt; Это про нас) Чем предстоит заниматься:  Построение высоконагруженных систем хранения больших данных; Выстраивание ETL процессов; Проектирование витрин данных; Выстраивание эффективных Pipeline-ов параллельной обработки данных с многотипными форматами данных.  Что ожидаем от тебя:  Опыт построения ETL в коммерческих продуктах от 4х лет (минимум 2 успешных продукта на рынке); Промышленный опыт работы с реляционными базами данных (PostgreSQL / MySQL) от 4х лет; Промышленный опыт работы c NoSQL Базами данных (ClickHouse, ElasticSearch, Redis) от 2х лет; Опыт работы с очередями (Kafka / RabbitMQ); Опыт промышленной эксплуатации AirFlow / Luigi; Применение Pyhton и PySpark в продакшн системах с использованием распределенных систем хранения и обработки данных; Опыт работы с Docker и K8s.  Будет плюсом:  Опыт подготовки данных для ML задач; Умение развернуть инфраструктуру обработки данных и выстроить ее поддержку; Опыт работы с Real-Time и NRT обработкой больших данных.  Наш стек:  Lang: Python/Go/C++ ; DB: Postgre, GreenPlum, Mongo, ClickHouse, Elastic, Redis; Queue: Rabbit, Kafka; Храним артефакты в Nexus; Orchestration: K8s (Rancher); Services: AirFlow, SuperSet, ML-Flow, Git-Lab; Log: ELFK; Monitoring: Grafana, Prometheus.  Бенефиты:  Стильный просторный лофт на территории Трехгорной мануфактуры. Тебе предстоит легкая 10 минутная пешая прогулка через парки от метро 1905 года и Краснопресненская; Сhill Lounge с пятой плойкой и капсулой для сна с массажем; Современное топовое оборудование, мощные ноутбуки; Комфортная кухня с вкусным кофе, чаем, какао, орешками, фруктами, снэками и прочими ништяками; Холодильник с колой и энергетиками; По пятницам в офисе пицца, роллы или грузинская кухня; Оформление по ТК РФ, конкурентная заработная плата; График работы 5\2 Гибкое начало дня с 8-11 до 17-20 в офисе (возможен гибридный график работы); ДМС со стоматологией и госпитализацией в классных клиниках; Профессиональное обучение и конференции по запросу сотрудника; Годовая подписка на топовый онлайн кинотеатр.  Минусы:  Очень много интересных и сложных задач))) Будут спрашивать результаты работы; Нет возможности работы из-за границы.  Готов создавать ML сервисы будущего? Готов брать на себя ответственность за сложные вызовы? Если да, ждем твое резюме, рады будем видеть тебя в команде!  "
"66548949","Яндекс","Инженер данных / Data Engineer | Яндекс.Толока","False","None","None","От 3 до 6 лет","Гибкий график","['Python', 'Spark', 'SCALA', 'AirfFlow', 'Luigi', 'Prefect', 'ETL']","Яндекс.Толока — это краудсорсинговая платформа для сбора данных и экспертных оценок. Сервис позволяет интернет-пользователям за плату выполнять несложные задания: ходить и что-то фотографировать, записывать голос, оценивать читаемость сайта на мобильных телефонах и др. Яндекс (и не только) использует результаты выполнения заданий, чтобы настраивать и проверять алгоритмы машинного обучения, а также поддерживать актуальность баз данных. Сейчас тысячи человек из разных стран выполняют десятки тысяч заданий в день, пользуясь мобильным приложением. А планов и идей у нас еще много! О команде вкратце: Наша команда развивает и поддерживает инфраструктуру для продуктовой аналитики в Toloka AI. Мы отвечаем за контуры обработки логов (ETL), организацию хранения данных (DWH), средства визуализации (BI) и A/B-тестирования. Для работы с международными клиентами мы переносим часть нашей инфраструктуры на внешние платформы. Ищем дата-инженера, который поможет нам в этом. Что нужно делать:  проектировать DWH и создавать ETL-процессы; внедрять и интегрировать Data Discovery, мониторинг, CI и другие системы для создания удобной и надёжной аналитической платформы; организовывать поставку данных; предоставлять экспертную поддержку команде аналитиков и исследователей.  Мы ждем, что вы:  уверенно владеете Python; владеете SQL, понимаете принципы оптимизации запросов и организации хранения данных; разрабатывали ETL-процессы; работали с системами контроля версий.  Будет плюсом, если вы:  работали с облачными платформами AWS, Azure или GCP; пользовались оркестраторами AirFlow, Luigi, Prefect; работали со Spark; владеете Scala.  Условия:  сильная и динамичная команда, в которой есть, у кого учиться и куда расти; высокая автономность команды: берём всё лучшее от коллег из большого Яндекса, но сохраняем дух стартапа, свободу в принятии бизнес-решений и минимизируем любую бюрократию; возможность влиять на разрабатываемый продукт; развитая система компенсаций и льгот; премии каждые полгода; компенсация оплаты питания; расширенная программа ДМС, оплата 80% стоимости ДМС для супругов и детей; помощь с переездом для кандидатов из других городов; развитая корпоративная культура: спорт (тренажёрный зал, йога, бассейн, лёгкая атлетика, футбол, баскетбол), английский язык, внешние тренинги и семинары по профессиональным тематикам, отраслевые конференции, программа развития управленческих навыков, очные мастер-классы, платформы онлайн-образования. "
"68323803","РУСАЛ, Центр подбора персонала","Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Spark', 'SQL', 'Python', 'Hive', 'ETL', 'DWH']","ОК РУСАЛ приглашает кандидатов на вакансию Data engineer. Обязанности:  Проектировать и разрабатывать ETL/ELT процессы; Создавать объекты хранилища и витрины данных на базе Hadoop, Greenplum, Clickhouse, ELK, Postgres; Участвовать в создании Data lake в компании; Участвовать в пилотировании решений; Участвовать в выборе технологий для построения КХД; Иногда включаться в adhoc задачи.  Требования:  Опыт промышленной разработки баз данных не менее 3-х лет; Понимание, чем отличается OLTP и OLAP в части схемы данных и обработки; Разработка на любом из диалектов SQL(T-SQL/PLSQL/pgPLSQL); Работа с любой MPP СУБД (Greenplum/Teradata/Vertica), знание особенностей проектирования БД для MPP СУБД; Практическое применение навыков программирования (python/java/scala), в том числе разработка ETL под Airflow.  Будет плюсом:  Опыт работы с Яндекс облаком; Опыт работы со стеком Hadoop, Spark, Hive, dbt.  Условия:  Работа в крупной стабильной компании, лидере мировой алюминиевой отрасли; Мы одна из самых быстроразвивающихся производственных компаний в России и за рубежом; Оформление согласно ТК РФ в Компанию с аккредитацией Минцифры РФ; Корпоративное обучение; Перспектива карьерного роста; Комфортабельный офис. "
"69093472","КОРУС Консалтинг","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['Power BI', 'SQL', 'Azure', 'Spark', 'Python', 'Business Intelligence Systems', 'BI', 'PySpark']","Мы КОРУС Консалтинг – российская ИТ-компания, признанный лидер в автоматизации ритейла, производства, логистики, финансов и нефтегаза. За это время мы сделали более 1200 масштабных проектов для ведущих компаний России: Bonava, DPD, Tele2, Азбука Вкуса, Газпром нефть, Дикси, Инвитро, Леруа Мерлен, Магнит, Мегафон, О’КЕЙ, Петрович, Росатом, Яндекс.Деньги и это только начало списка. Нас уже больше 1000 человек, а за плечами более 20 лет опыта. Наш Департамент аналитических решений (ДАР) погружен во все современные направления в области работы с данными: внедрение BI-систем и систем аналитической отчетности; проектирование хранилищ и витрин данных; разработка в области продвинутой аналитики и больших данных; применение прикладных продуктов с использованием Machine Learning; внедрение решений в области управлениям данными (Data Governance, Data Quality); разработка методологии и стратегии работы с данными. О проекте: мы разрабатываем облачные хранилища данных и аналитические решения на облачной платформе Microsoft Azure, помогаем нашему заказчику (иностранная ритейл компания) изменять бизнес-процессы путем использования аналитики. Собираем ключевые показатели деятельности и предоставляем эти показатели в виде красивых дашбордов высшему менеджменту заказчика. Ищем к себе в команду Data Engineer&#39;a с желанием развиваться в эту сторону, планируем погружать новых коллег в PySpark) Что нужно делать:  Разрабатывать витрины/хранилища данных для последующей визуализации средствами Power BI. Разрабатывать и оптимизировать ETL процессы.  В работе поможет:  Опыт написания запросов для анализа и преобразования данных (на любой реляционной СУБД) от 1 года (коммерческий опыт).  Опыт разработки ETL-процессов от 1 года (коммерческий опыт).  Умение моделировать данные. Опыт в подготовке витрин данных\кубов для аналитических отчетов от 1 года. Опыт работы на языке запросов DAX.  Совсем отлично, если есть:  Опыт разработки на Python от 1 года (коммерческий опыт).  Опыт создания собственных визуализаций для PowerBI.   Что можем предложить:  Заработная плата: состоит из оклада и бонуса за выполнение проекта. Знаем рынок, готовы обсуждать оклад индивидуально по итогам тех. собеседования. Оформляем в штат и все выплаты производим официально. Гибкое начало рабочего дня. Удаленный формат работы. Очные встречи командами или работа в офисе (Москва, Санкт-Петербург, Ярославль) по желанию. Командировки для сотрудников из других городов по желанию. Индивидуальный план развития: подбираем задачи для роста экспертизы, выделяем бюджет на обучение, предоставляем курсы и программы обучения (в т.ч. английскому языку), предоставляем обратную связь, регулярно обсуждаем развитие. Полис ДМС, корпоративные тарифы на фитнес и др. Открытая корпоративная культура: каждый может предложить идею и получить возможности для ее реализации, мы помогаем друг другу, открыто обсуждаем сложные моменты, шутим и поддерживаем друг друга, часто и весело проводим корпоративы. Минимум бюрократии и отчётов, а совещания только по необходимости и с чёткой программой.  Приходи к нам в команду, будем вместе создавать крутые и полезные вещи!"
"69147148","АМТЕХ","Data инженер","False","None","None","От 3 до 6 лет","Полный день","[]","АМТЕХ гарантирует:  Высокий уровень стабильной, белой заработной платы и официальное оформление в рамках ТК РФ; Современный, дизайнерский офис с 8-ю атриумами, зелеными зонами отдыха, теннисным кортом и экспозицией современного искусства; Возможность роста в молодой, динамично развивающейся компании;   Роскошный соцпакет после испытательного срока; Гибкий/гибридный режим рабочего времени. Возможен удаленный формат работы; Корпоративная скидка на изучение английского языка на платформе Skyeng.    Свяжитесь с нами сегодня, направив отклик на hh.ru О компании: Разработчик передовых решений для управления городской средой современного мегаполиса. Наши проекты объединяют в себе решение задач безопасности граждан, улучшения их информированности, доступности городских сервисов для населения, а также комфорта городской среды. Чем предстоит заниматься:  Проектированием и разработкой программных модулей для автоматизации ETL на базе Apache NIFI, Apache Airflow; Проектированием и разработкой архитектуры мультиарендной платформы данных; Участие в разработке CI/CD процессов NIFI &amp; Airflow; Разработкой технического дизайна платформы данных; Помощь в документировании проектных решений;  Какие навыки и знания требуются нашим кандидатам:  Отличное знание Linux; Опыт разработки на Python от 3-х лет; Опыт проектирования и реализации пайплайнов с данными в NIFI &amp; Airflow от 2-х лет; Отличное знание SQL; Знание архитектуры NIFI &amp; Airflow; Опыт оптимизации производительности NIFI &amp; Airflow; Опыт работы с Gitlab; Опыт работы с инструментами CI/CD;  До встречи в АМTEX!"
"67995874","Спортмастер","Senior Data engineer","False","None","None","От 3 до 6 лет","Полный день","['Data Engineer', 'Python', 'SQL', 'Hadoop', 'ORACLE']","Мы находимся в поиске Ведущего Data engineer&#39;a, который будет заниматься развитием направлений :Цифровая аналитика &quot;Антифрод&quot;, &quot;Инфраструктура DataLake&quot;. Ваши задачи:   Реализация ETL в Hadoop (с помощью Airflow).   Работа с различными источниками данных: Oracle MS SQL API личных кабинетов, микросервисы.   Батч и стримы с помощью PySpark и Kafka.   Подготовка витрин для анализа (Hive + Spark+ SQL)   Наш стек: Cloudera hadoop; Kafka, Spark, Airflow; Jira; Confluence; GitLab Мы ждем от будущих коллег:  Уверенное владение Python. Опыт использования эко-системы Hadoop: HDFS, Apache AirFlow, Hive, Kafka,Spark. Знание SQL Опыт работы с реляционными базами данных (Oracle). "
"70528216","СБЕР","Разработчик ETL-процессов / Data Engineer (Hadoop)","False","None","None","От 1 года до 3 лет","Полный день","['Hadoop', 'ETL']","Тебе предстоит:  выполнение функциональных обязанностей в рамках проекта по Миграции Платформ данных; обеспечение загрузки необходимых данных для Команды из КАП (стек Hadoop); проектирование и разработка, тестирование и внедрение ETL-процессов (framework, конвейер данных); оптимизация существующего кода на другой СУБД под технологии Hadoop; разработка хранимых процедур; сбор, систематизация требований по исходным данным для Команды.  Мы ждем от тебя:  высшее образование (Информационные технологии, Физика, Математика); опыт работы в роли разработчика на стеке Hadoop не менее 2 лет; опыт разработки приложений с использованием инструментария экосистемы Hadoop (Spark, Spark SQL, Kafka, Hive, Impala, Hue и тд); опыт разработки на Java / Python; опыт разработки ETL / Self service ETL процессов; отличное знание sql-запросов, опыт работы с СУБД SAS, Teradata; знание и опыт работы с инструментами DevOPS.  Плюсы быть в команде Сбера:  работа в крупнейшем банке России; регулярное корпоративное обучение; карьерный рост и профессиональное развитие; насыщенная корпоративная жизнь и развитая корпоративная культура спортзал; ДМС, страхование от несчастных случаев и тяжелых заболеваний; материальная помощь и социальная поддержка, корпоративная пенсионная программа; льготные условия кредитования. "
"70475854","Hype Auditor","Data Engineer (Golang)","True","None","230294","От 3 до 6 лет","Полный день","['Golang', 'SQL', 'PHP', 'MongoDB', 'Big Data']","Привет! Мы – команда разработчиков HypeAuditor, мы строим продукт, который помогает тысячам компаний по всему миру запускать эффективные рекламные кампании у блогеров. В основе нашего продукта лежит мощная аналитическая система, которая с помощью ML-алгоритмов оценивает аккаунты по сотням параметров, позволяет отсекать фрод и мошенников, и дает рекламодателям глубокое понимание аудитории блогеров. Наш продукт используют известные бренды - Dior, Amazon, L’Oreal, IKEA, Unilever - и крупнейшие мировые агентства - GroupM, Havas, Ogilvy, OMD, а такие издания, как Forbes, The Washington Post, Business Insider, Entrepreneur, используют наши данные для своих отчетов. Сейчас в нашей технической команде 40 человек - это разработчики и инженеры, product менеджеры и аналитики, а всего в HypeAuditor нас почти 100. Мы - международная команда, и наши сотрудники работают из России, США, Бразилии, Аргентины, Испании, Кипра и других стран. Мы в списке лучших стартапов-работодателей США по версии Forbes. В прошлом году мы выросли в 2 раза, в этом - растем еще быстрее. У нас открываются новые географии и появляются новые подпродукты. В связи с расширением, мы ищем Data Engineer. Наша система обрабатывает миллионы событий в минуту, хранит терабайты сырых данных и делает сотни тысяч rpm к базам. В данный момент мы рассматриваем только кандидатов с перспективой релокации, Есть возможность полной удаленки (за пределами РФ), есть офис в Армении, Сербии, и в скорем времени откроется возможность релокации на Кипр, готовы помогать с релокацией.   Задачи:   Разработка сервиса сбора, хранения и доступа к статистике и данным;   Проектирование, деплой и поддержка инфраструктуры проекта;   Какими знаниями нужно обладать:   Отличное владение языком программирования (PHP/Go), умение писать чистый поддерживаемый код;   Опыт работы с MongoDB и MySQL;   Уверенное знание UNIX Shell;   Git;   Навыки разработки многопоточных приложений;   Опыт проектирования высоконагруженных сервисов.   Будет плюсом:   знание SQL и принципов работы реляционных БД;   Опыт работы с Elasticsearch;   опыт работы с очередями сообщений;   опыт написания и поддержки кода, работающего под высокой нагрузкой в режиме 24x7;   опыт применения алгоритмов машинного обучения (Python/R);   опыт профилирования производительности приложений, написанных на PHP, Python.   Какие личные качества мы хотим видеть в кандидате:   Внимательность и организованность;   Самостоятельность (чтоб ставить задачи себе и добиваться их качественного решения в срок);   Умение эффективно работать в команде;   Продуктовое мышление.Наш стек:  GO/PHP; БД: MongoDB, Mysql, Clickhouse; ML: Python с библиотеками Keras и CUDA; Шина: RabbitMQ; Мониторинг и логи: Prometheus, Grafana, ELK; GitlabCI; Docker, k8s, helm и т.п.  Почему у нас круто? Мы находимся на стадии масштабирования компании и нашей платформы. Команды компактные и отвечают за свои части продукта, поэтому вклад каждого сотрудника - заметен и важен. Это редкая возможность для разработчиков и инженеров создавать фичи, отвечать за их конечную реализацию, что напрямую влияет на продукт и, как следствие, на весь рынок Influencer маркетинга, потому что нас знают по всему миру и ориентируются на то, что мы создаем. Мы поощряем продуктовый подход в работе, регулярно делимся обратной связью от пользователей, так, команды сами определяют логику изменений в продукте, и у каждого разработчика и инженера есть возможность влиять на выбор технологического стека. Работая короткими итерациями, мы обладаем присущей стартапам гибкостью в коммуникации и процессах, что позволяет нам быстро двигаться, при этом у нас экспертная команда, налаженные процессы, и мы по-настоящему гордимся культурой и атмосферой в компании. А еще у нас:  Интересный продукт с живой иностранной аудиторией; Нет тестовых заданий на этапе отбора; Много возможностей для роста, обучения, прокачивания скиллов, внутренние тренинги и воркшопы; Возможность выбрать формат работы (офис/удаленка/гибрид); Классные офисы на Кипре, в Ереване и Белграде, помощь в релокации; Всевозможные закуски, фрукты и напитки без ограничений; Крутые выезды и корпоративы; Все по локальным ТК + ДМС, бесплатный корпоративный английский, компенсация питания и гибкость без лишней бюрократии.  Будем рады познакомиться и подробно рассказать о продуктовой стратегии, задачах и наших планах.Не рассматриваем кандидатов, которые не готовы переехать"
"70108469","СБЕР","Data engineer","False","None","None","От 1 года до 3 лет","Полный день","[]","Команда Чат-Бота работает над автоматизацией и ускорением обслуживания розничных клиентов в текстовых каналах банка. Мы ищем опытного инженера данных для автоматизации потоков данных. Обязанности:  построение ETL для внутренних процессов команды (напр. разметка данных, отчетность); подготовка прототипов витрин данных для BI-систем; оптимизация распределенной обработки данных на базе Hadoop (CDH); ad-hoc выгрузка данных для DA/DS команды.  Требования:  владение SQL, Python для обработки данных; знание распределенных систем обработки данных – Hadoop/Spark; умение работать с большими массивами данных.  Будет плюсом:  опыт работы с БД Greenplum/PostgreSQL; опыт работы с ETL системами/фреймворками – Airflow/Informatica; опыт работы с QlikView; опыт работы с Колл Центрами и автоматизацией обслуживания обращений​​​​​. "
"70358686","Деловые Решения и Технологии (ДРТ)","Инженер данных (Data Engineer)","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'ETL', 'Управление проектами', 'Анализ данных', 'Аналитическое мышление', 'Spark']","Что ты будешь делать в этой роли:  Проектировать корпоративные хранилища данных (Data Warehouse). Руководить командой из 5–10 человек для разработки ETL-конвейеров (NiFi, Spark, SQL) и дашбордов. Обеспечивать актуальность данных и высокую доступность КХД.  О подразделении: Команда занимается решением задач клиентов в различных областях, в том числе внедрением ИТ-систем и визуализацией данных. Среди типичных проектов команды: разработка дашбордов для руководства, разработка КХД, управление качеством данных. Обязанности:  Анализ бизнес-требований по получению новых данных, оценка наличия данных, сложности и рисков. Анализ и оценка качества данных в новых источниках (внешних и внутренних). Проектирование и разработка аналитических витрин данных. Разработка и внедрение эффективных методик контроля качества данных. Мониторинг и оптимизация процессов загрузки, преобразования данных и сборки витрин. Разработка автоматизированных инструментов процессов обработки данных. Предоставление экспертной поддержки внутренним потребителям по вопросам, связанным с использованием данных.  Требования к кандидатам:  Опыт работы в качестве инженера баз данных/ETL-разработчика не менее двух лет. Опыт применения Apache Spark. Знание SQL на продвинутом уровне (аналитические функции, подзапросы, хранимые процедуры, оптимизация производительности). Знание основных понятий и концепций из области Data Warehousing, опыт разработки витрин данных будет являться преимуществом. Навык реализации ETL-процессов (в том числе умение работать с REST API). Опыт работы в PowerBI будет являться преимуществом.  Что мы предлагаем:  Достойное вознаграждение и прозрачную систему карьерного и профессионального развития. Комплекcную программу страхования. Обучение и развитие, поддержку в получении сертификатов профессиональной квалификации. Программы поощрения сотрудников. Возможность работать по гибкому графику. Комплексную программу корпоративных скидок. Компенсацию расходов на мобильную связь. "
"70080031","ВСК, САО","Дата-инженер / Data engineer","True","130500","None","От 1 года до 3 лет","Полный день","['SQL', 'MS SQL', 'Анализ данных', 'Python', 'BPMN']","Страховой Дом ВСК приглашает в свою команду Дата инженера (Data engineer). В Блоке урегулирования убытков строится локальное хранилище (DWH) накопившее достаточное количество исторической аналитики, все данные нужно описать, понять что есть, чем можно пользоваться, отрисовать описательную часть в любой нотации. Это позволит наладить процессы отслеживания потребления ресурсов и планирования заказов, сделать сквозной data lineage от источников до моделей с возможностью поиска и удаления неиспользуемых таблиц, построить систему проверки качества данных, создать рабочий инструмент для аналитиков бизнеса в ежедневных рабочих процессах. Обязанности:  Поддержка БД MS SQL Обработка и структурирование данных, консолидирование и анализ данных из различных источников Расчет сложных аналитических показателей в витринах данных Участие в тестировании потоков с целью обеспечения их стабильной работы Разработка технической документации  Создание витрин данных  Генерация новых признаков/полей на основе исходных данных для команды аналитиков Работа с аналитиками для оптимизации существующих витрин Проектирование и сбор витрин данных по разработанному ТЗ  Качество данных  Выявление проблем качества данных Интеграция новых источников данных в Data Lake Консультирование владельцев данных по выявленным ошибкам  Пожелания к кандидатам   Образование среднее специальное техническое или высшее Знание SQL на высоком уровне и опыт оптимизации запросов на какой-либо СУБД Аккуратность при работе с информацией и данными Опыт написанию документаций к данным, скриптам и бизнес-процессам  Ключевые компетенции  Навыки работы с базами данных SQL Опыт в проектах по построению систем BI Навыки работы с OLAP (DAX, MDX) Опыт разработки проектной документации (Confluence) Навыки работы с инструментами визуализации программной документации (draw.io, BPMN)  Условия:  Работа в крупной и стабильной компании Официальное трудоустройство по ТК РФ Корпоративное добровольное медицинское страхование (ДМС) График работы обсуждается: гибридный формат (офис+удалёнка) или 100% удалённый формат работы Заработная плата оклад + квартальная премия (совокупный доход обсуждается в зависимости от уровня квалификации на личном собеседовании) Офис расположен: п. Заречье м. «Говорово» , м. «Озёрная», м. «Сколково» Корпоративный транспорт от м. «Парк Победы» "
"68576647","СБЕР","Data engineer","False","None","None","От 3 до 6 лет","Полный день","[]","Оперативное обеспечение Data Science специалистов блока Риски качественными выборками из различных информационных систем Банка, необходимыми для построения новых моделей, а также для оценки качества и корректировки существующих моделей. Результатом работы дата-инженера и дата-сайнтиста становится прототип математической модели исследуемого бизнес-процесса,который становится основой для внедрения модели в промышленные системы. Для успешного внедрения прототип должен быть подробно задокументирован,как с точки зрения алгоритма получения выборки (зона ответственности дата-инженера),так и с точки зрения заложенной в модель математики (зона ответственности дата-сайнтиста). Технологический стек: · Bigdata: Hadoop, Hive, Impala, Spark, Scala, Python · СУБД: Oracle, Teradata, Greenplum · ETL: Informatica, SAS Enterprise Guide · управление требованиями: Jira, Confluence, BitBucket Задачи  участие в проектировании и разработки витрин и хранилищ данных участие в проектировании и реализации инструментов автоматизации разработки  Мы ожидаем , что у тебя есть  высшее, техническое либо финансовое образование опыт работы от 3х лет с одной или несколькими СУБД: Oracle, MS SQL, Teradata либо СУБД стэка Bigdata опыт работы с хранилищами данных от 1го года опыт работы в роли аналитика с функцией подготовки выгрузки данных для заказчика уверенное знание SQL: сложные запросы, аналитически функции, понимание физической реализации join’ов, оптимизация производительности запросов знание одного или нескольких языков программирования: PL/SQL, T-SQL, Java, Python, Scala на уровне переменных, процедур, функций, циклов, условных операторов знание одного или нескольких ETL-инструментов: Informatica, MS SSIS, SAS, ODI понимание принципов организации хранилищ данных, подходов к проектированию логической и физической моделей, понимание основной проблематики хранилищ и подходов к решению  Мы предлагаем  офис рядом со станцией метро Кутузовская бесплатный спортзал; места для отдыха - настольный теннис, несколько playstation, кикер, бильярд возможность работать с современным стеком технологий социальный̆ пакет (ДМС) огромный каталог образовательных программ, возможность обучения и сертификации за счет компании программа льготного кредитования в Сбербанке дисконт-программы от множества компаний партнеров возможность принять участие в других крупных и уникальных проектах Банка "
"68897555","СБЕР","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['MS Access', 'Работа с банками', 'Управление знаниями', 'Продвинутый пользователь ПК', 'Решение проблем', 'Hadoop', 'Хранилища данных']","Сбер – высокотехнологическая компания и самый крупный банк России, Центральной и Восточной Европе. Мы создаем экосистему удобных онлайн-сервисов в самых разных сферах. Сейчас в нее входит более 40 компаний. Наша команда занимаемся развитием продуктов для Программы лояльности Сбербанка “Спасибо от Сбербанка”, которой пользуются уже более 38 млн. человек. В нашу большую команду ищем дата инженера. Основные функции:  проработка архитектуры и реализация интеграций с внешними системами проектирование и разработка витрин данных разработка и внедрение фреймворков и автоматизированных инструментов обработки данных мониторинг и оптимизация процессов загрузки, преобразования данных и сборки витрин настройка и мониторинг компонент кластера Hadoop анализ и устранение проблем производительности компонент кластера Hadoop проведение code review предоставление экспертной поддержки внутренним потребителям по вопросам, связанным с использованием данных описание фичей, подготовка требований для смежных команд, разработка спецификаций и технической документации.  Требования к кандидату:  не менее 3 лет работы в качестве Data Engineer/ETL Developer отличное знание SQL и современных промышленных СУБД (Oracle/Teradata/GreenPlum) опыт разработки хранилищ данных знание экосистемы Hadoop (HDFS, Hive, Yarn, Spark, Impala) знание языков программирования Python/Scala/Java опыт работы с Devops-инструментами и системами контроля версий.  Приветствуется:  опыт работы по Agile (SCRUM, Kanban, и т.д.) опыт администрирования и инсталляции продуктов экосистемы Hadoop опыт администрирования ОС Linux     "
"70023364","Marfatech","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'SQL']","Привет! Мы международная IT компания Marfatech. На рынке мы выросли от стартапа по привлечению трафика до компании с международными проектами. Мы занимаемся разработкой в сфере гейм-приложений и системами BI-аналитики. Нам нравится создавать и внедрять новые технологии максимально быстро, с минимумом бюрократии, чтобы менять IT-мир к лучшему! У нас нет старого легаси кода и большая свобода от рутинных задач, не нужно с нуля разрабатывать базовые вещи. Что нужно делать:  Систематизировать и автоматизировать процесс сбора данных из различных внутренних и внешних источников; Разрабатывать и поддерживать витрины данных; Выстраивать ETL процессы; Интегрировать новые источники данных; Следить за качеством данных.   Мы ожидаем:  Отличное знание SQL, Python; Опыт работы с Airflow; Опыт работы и знание особенностей Clickhouse; Понимание логики построения аналитического хранилища данных, OPLAP.Будет плюсом: Знание основ PHP   Мы предлагаем:  Достойный уровень заработной платы; Сложные, интересные и разнообразные задачи; Возможность карьерного роста; Возможность профессионального развития (обучение, конференции, литература, сильная команда высокого класса с большим опытом); Открытая и свободная friendly среда - у нас ты сможешь сам задавать тренды, а не следовать им; Трудоустройство в аккредитованной белой IT-компании (предоставление всех IT-льгот); ДМС Удаленный формат работы или современный комфортный офис рядом со ст.м. Тульская; Кухня с завтраками, разнообразием фруктов, снеков и газировки; Разнообразные корпоративные и спортивные мероприятия; 50 % корпоративная скидка на изучение английского языка "
"67384069","СБЕР","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","[]","Мы - новая быстрорастущая команда, создающая сервис, который будет управлять цифровыми рекламными поверхностями и платформой по размещению рекламы на поверхностях Банка и Экосистемы. Используя данные Сбера и технологические решения его экосистемы, мы сможем реализовать уникальный сервис на рынке digital marketing. Он позволит нам показывать пользователям рекламу, которая будет нравиться и которая будет делать рекламные продукты эффективными и измеримыми для рекламодателей. Если вы талантливый и амбициозный специалист в сфере ИТ, если вы хотите быть причастными к созданию лучших продуктов для лучших клиентов и уверенно отвечать за результаты, то мы рады принять вас в команду. Чем предстоит заниматься:  разрабатывать и поддерживать микросервисы для работы с данными в экосистеме Hadoop, Cloudera, Hortonworks, Apache BigTop и др. разрабатывать и оптимизировать пайплайны обработки данных на python / scala - от логов nginx до записи в ClickHouse решать сложные технические задачи в ETL слое - развивать и сопровождать DWH на базе Hadoop/ Greenplum/ ClickHouse быть одним из драйверов развития архитектуры и инфраструктуры проекта.  Мы ожидаем:  поиск, обработка и построение витрин данных на инфраструктуре Hadoop опыт промышленной разработки на стеке Java/Scala/Python опыт реализации REST, SOAP. Понимание принципов работы SSO, Kerberos опыт разработки приложений с использованием инструментов Spark Streaming, Hbase, Spark, Kafka, Hive, Impala, Hue и т.д. хорошие знания SQL, опыт работы с одной из реляционной БД - Oracle/PostgreSQL/mySQL/MS SQL Server понимание принципов модели распределенных вычислений, принципов организации Data Lake/DWH понимание подходов к организации разработки (CI/CD, DevOps), практический опыт работы с инструментами Jenkins, SonarQube, TeamCity, Anisble, Git (BitBucket/GitLab).  Мы предлагаем:  интересные задачи по продуктам, влияющим одновременно на всю Экосистему: мы пишем продукты с 0, а значит никакого legacy и свобода творчества команда специалистов из топовых ИТ компаний регулярное обучение и профильные конференции, современное оборудование для работы; Уровень дохода, который готовы обсуждать и отталкиваться от ваших пожеланий, плюс премии комфортный офис и гибкий график множество плюшек от Сбера. "
"70221657","ЕвроХим, Минерально-Химическая Компания","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'SQL', 'DWH', 'MS PowerPoint', 'Анализ данных']","В направлении R&amp;D компании АО «МХК «ЕвроХим» открыта вакансия &quot;Data Engineer&quot;. Основная задача данной позиции - Участие в разработке платформы обработки и анализа больших данных. Глобальная цель направления – реализация полномасштабной программы по внедрению на предприятиях группы компаний лучших научных, технических и цифровых решений и их взаимная интеграция. Обязанности:  Конфигурирование доменных и продуктовых потоков данных; Разработка моделей данных; Проектирование витрин данных; Настройка проверок качества данных; Анализ и ведение доменов данных.  Требования:  Хорошее знание SQL, python; Участие в проектах по работе с DWH или DataLake системами. Понимание с agile и gitops практиками; Опыт работы с ELT/ETL инструментами, инструментами bi, k8s; Будет плюсом опыт работы с airflow, greenplum, dbt, superset.  Условия:  Оформление в соответствии с нормами ТК РФ, социальные гарантии; Крупные проекты в компании–лидере отрасли; Возможность модернизировать процессы реального производственного бизнеса в команде сильнейших экспертов; Полис ДМС (включая стоматологию); Удаленная работа; Доступ к корпоративной OnLine библиотеке; Обучение в программах Корпоративного университета; Корпоративный спорт, конференции, культурные мероприятия. "
"70067888","СБЕР","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","[]","Обязанности: • проектировать логическую модель и логику преобразования данных • анализировать требования бизнеса по получению новых данных, оценивать наличие и качество данных • разрабатывать End2End процессы формирования витрин данных в Hadoop на фреймворке Spark • разрабатывать ETL-алгоритмы и витрины данных (Apache Spark, Hadoop) • проводить системное, функциональное и интеграционное тестирование • разрабатывать и внедрять эффективные методики контроля качества данных • оптимизация загрузок. Требования: • опыт разработки на Spark (RDD, Dataframe, Spark SQL) • опыт промышленной разработки на Spark • базовые знания SQL желание развиваться в направлении BigData • проектирование и разработка потоков данных, алгоритмов загрузки и обработки данных в Hadoop; • выстраивание и оптимизация производительности ETL-потоков; • наличие развитых аналитических способностей и системное мышление • знакомство с инструментами экосистемы Hadoop: Hive, Spark • наличие опыта разработки на Python / разработки на Spark (желательно pySpark) • общий стаж работы не менее 5 лет • непреодолимая тяга к обучению и саморазвитию • высшее техническое образование. Преимуществом будет: • знание и понимание DevOps практики • опыт разработки на Spark on Scala • знание SAS Base и инструментов для работы с платформой SAS (SAS Enterprise Guide). Что мы предлагаем: • современный IT-офис вблизи Москва-Сити в пяти минутах от метро &quot;Кутузовская&quot;, с фитнес залом и бесплатным подземным паркингом • стабильную белую заработную плату и годовую премию • заряженную команду профессионалов и адекватное руководство • важные и амбициозные задачи • бесплатное обучение в лучшем корпоративном университете • льготные условия по ипотеке • скидки и бонусы от партнеров Сбера • ДМС с первого рабочего дня с возможностью прикрепления родственников к программе • сплоченный коллектив, работающий над общими задачами и умеющий хорошо отдыхать."
"55441562","СИНЕРГИЯ","Data engineer","True","200000","None","Нет опыта","Полный день","['Python', 'Bash', 'Docker', 'Data Analysis', 'ML']","Прямо сейчас мы ищем Data Engineer`a. Аналитический центр - это сердце корпорации, ведь именно с его помощью мы принимаем решения по развитию бизнеса.   Вот чем нужно будет заниматься:  Организация новых и оптимизация существующих ETL и Data Quality пайплайнов (SQL,Python); Помощь с внедрением в продакшн и мониторингом моделей машинного обучения (*nix, docker, airflow, mlflow); Настройка ML\Data инфраструктуры; Оптимизация тяжелых запросов для сбора единых витрин (MS SQL Server); Настройка MLflow и Airflow, перенос ETL matomo в Airflow;  Опыт и навыки, которые необходимы для выполнения задач:  Хорошее знание Python, умение писать поддерживаемый код; Flash\Fast API; Git, Bash, Airflow, Docker; SQL на экспертном уровне (умение оптимизировать запросы ).  Желательно:  ML flow, опыт организации DWH; Опыт организации мониторинга ML-моделей; Опыт работы с ClickStream; Опыт работы с BI решениями.  Что мы готовы предложить:  Работа над уникальными проектами с лидерами сегмента; Возможности профессионального роста и развития в динамично развивающейся компании; Возможность бесплатного посещения бизнес-мероприятий Корпорации; Корпоративные скидки на любые образовательные программы Синергии до 100%; Собственная столовая с вкусной и здоровой пищей, корпоративное кафе; Офис в шаговой доступности от м. Сокол; График работы: 5/2 с 9.00 до 18.00, или с 10.00. до 19.00. "
"55374209","СберМаркет","Data engineer в СберМаркет","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'Python', 'ETL', 'DWH', 'Databases', 'PostgreSQL']","Привет! Мы в СберМаркете активно ищем data engineer в нашу дружную команду. СберМаркет — это высоконагруженный маркетплейс. В компании работают более 600 мобильных и веб-разработчиков, DevOps, SecOps, QA-специалистов, аналитиков, продакт-менеджеров. С нами ты будешь влиять на продукт, пробовать новое и задавать тренды. Масштабируй знания, профессиональные навыки и вместе с этим масштабируй компанию. А мы обеспечим тебе лучшие условия (сильное Tech-комьюнити, отличную зарплату, гибкий соцпакет, well-being программы). Отдел Data Engineering: Мы создаем хранилище данных и инструменты для их анализа. Наш отдел развивается вместе с бизнесом, этот юнит формируется с нуля, а значит, что у тебя будут все возможности проявить себя и на 100% участвовать в построении процессов по направлению данных. Чем предстоит заниматься:  Работой с DWH, проектированием схем для хранения данных; Загрузкой данных из различных источников (как внутренние БД, так и внешние аналитические/ маркетинговые платформы и т.д.) в DWH; Автоматизацией data-pipelines, ETL процессов; Поиском ошибок и аномалий в данных, автоматизацией проверок качества данных; Подготовкой витрин данных; Подготовкой и поддержанием в актуальном состоянии каталога метаданных; Организацией CI/CD и мониторингом процессов обработки данных.  Мы ожидаем от кандидата:  Уверенное знание SQL, умение строить сложные запросы и оптимизировать производительность; Опыт разработки на Python (мы используем Airflow).  Будем плюсом:  Опыт работы с ClickHouse/PostgreSQL/Greenplum. Знание Scala (в тяжелых задачах мы используем Spark); Понимание принципов организации хранилищ данных, принципов работы колоночных БД;    Что интересного у нас есть: • можешь выбрать свой соцпакет — ДМС, спорт, промокоды на заказ продуктов или билеты в отпуск; • работаем удаленно. В Москве есть большой и красивый офис, поэтому если ты из этого города или будешь проездом — welcome; • предоставляем технику для работы на твой выбор; • обеспечим интеллектуальное и физическое развитие (электронная библиотека, книжный клуб, футбол); • мы уделяем большое внимание обучению сотрудников, поэтому в нашей knowledge base ты найдёшь много интересных курсов, книг и записей конференций. А еще: • мы сами участвуем в конференциях, как спикеры; • проводим внутренние митапы и дискуссионные клубы; • не боимся экспериментировать с новыми решениями и технологиями; • заботимся о сотрудниках, а не выжимаем их: в компании есть специалист по здоровью на случай, если заболел ты или кто-то из родных. Психолог для разговоров по душам и команда счастья для неформальных мероприятий и внерабочих активностей :)  "
"70022988","Яндекс","Data engineer","False","None","None","От 3 до 6 лет","Полный день","['Python', 'Hadoop', 'Hive', 'SQL', 'Pandas']","В отделе качества рекламы разрабатываются модели для решения множества задач машинного обучения, и этим моделям нужны регулярные данные из разных систем. Данных очень много — нужно обрабатывать несколько петабайт в день, при этом необходимо постоянно добавлять новые источники, разрезы и датасеты с минимальными затратами железа и времени разработчиков. Мы хотим построить масштабируемый DWH для ML, который будет справляться с большой нагрузкой и, кроме того, позволит наладить процессы отслеживания потребления ресурсов и планирования заказов, сделать сквозной data lineage от источников до моделей с возможностью поиска и удаления неиспользуемых таблиц, построить систему проверки качества данных. Мы ждем, что вы:  имеете опыт работы с MapReduce, например с Hadoop или Hive; имеете опыт работы с пайплайнами данных — Airflow, Azkaban, Luigi, Oozie и др.; хорошо знаете Python.  Будет плюсом, если вы:  разбираетесь в машинном обучении; знаете любой диалект SQL, работали с базами данных; знаете C++ на прикладном уровне.  Условия:  сильная команда, с которой можно расти; возможность влиять на процесс и результат; зарплата на уровне рынка и выше; премии каждые полгода для всех, кто успешно прошёл ревью; расширенная программа ДМС, оплата 80% стоимости ДМС для супругов и детей; гибкий график работы. "
"69957654","МСП Банк","Data Analyst / Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'Python', 'VBA', 'MS PowerPoint']","По требованиям: · опыт работы в банках, желательно в рисках; · знание SQL, опыт работы с Oracle, SQL server, MS Access. Написание сложных запросов, процедур, оконных функций; · отличные знания Excel, построение моделей, прогнозов, отчетов, желательно знание VBA; · желателен опыт работы с BI средствами (например Power BI); · знание языков программирования будет плюсом, особенно Python; · понимание банковских процессов; · Коммуникабельность , желание развиваться. По функционалу: · поддержка всех видов рисков (кредитный, ликвидности и т.д.) с точки зрения данных. · подготовка витрин данных и отчетов на oracle, sql server. · построение отчетности с помощью excel, powerpoint, bi средств. · построение системы мониторинга качества данных. · разработка дашбордов по всем видам риска. МЫ ПРЕДЛАГАЕМ  оформление согласно ТК РФ; комфортный офис на ст. м. Павелецкая; конкурентная &quot;белая&quot; заработная плата, квартальный и годовой бонус; социальный пакет (ДМС после испытательного срока, страхование жизни); обучение за счет Банка, возможность роста развития; возможность стать частью сильной команды, реализовать свой потенциал "
"66103323","СБЕР","Data engineer","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'Big Data', 'SQL', 'MongoDB', 'Анализ данных', 'hadoop', 'Spark']","SberDevices - новое направление компании, которое занимается созданием девайсов для массового пользователя и продуктов на основе речевых и голосовых технологий и многими другими интересными проектами.Чем предстоит заниматься:  Участие в создании инфраструктуры управления корпоративными данными. Разработка процедур потоковой и пакетной загрузки данных из различных типов внешних источников:   Сырые данные по телеметрии устройств; Сырые данные по активности пользователей; Структурированные источники данных;   Разработка процедур обработки и анализа данных. Разработка процедур доставки обработанных данных в внешние системы. Разработка процедур контроля качества данных.  Профессиональные навыки:  Практический опыт работы со стеком технологий Big Data (Hadoop, Spark/Hive/Hue). Практический опыт работы с технологиями баз данных (Postgres, MongoDB, Cassandra). Практический опыт работы с облачными платформами управления данными (Yandex.Cloud, AWS, Mail.ru Cloud Solutions). Практический опыт участия в проектах по созданию DWH, Data lake, Data management platforms. Практический опыт разработки и оптимизации запросов к данным на базе SQL, Python. Практический опыт построения и развития высоконагруженных систем.  Что предлагаем:  Работа с крупнейшими массивами данных на рынке России. Амбициозный и дружный коллектив. Профессиональное обучение, семинары, тренинги, конференции, корпоративная библиотека. ДМС, страхование жизни. Самые инновационные, амбициозные проекты и задачи. Свободный дресс-код. Гибкий график для оптимального баланса работы и личной жизни. Льготные кредиты и корпоративные скидки.     "
"69909885","М2М","Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Git', 'Python', 'PostgreSQL', 'MongoDB', 'SQL', 'Английский язык', 'ClickHouse', 'Databases', 'Data Analysis']","M2M — продуктовая IT-компания. Мы разрабатываем программное обеспечение для музыкальной индустрии. Наши продукты помогают миллионам пользователей во всем мире заниматься музыкальным творчеством. Наши инновационные решения в сфере музыки разработаны совместно с ведущими мировыми экспертами. М2М участвует в разработке продуктов, которые лидируют в отрасли. На данный момент мы хотим обеспечить качественный сбор и анализ данных на всех этапах разработки, для этого мы создаем команду аналитиков и ищем Data Engineer. Data Engineer возьмет на себя контроль и управление процессами загрузки данных из всех источников в аналитическое хранилище, что позволит решать задачи по транспортировке данных, необходимые для аналитики при разработке. Чем предстоит заниматься:  Интеграция новой CRM системы Разрабатывать новые решения для обработки данных (ETL, конвейеры, службы интеграции, пакетная и потоковая обработка, службы обогащения и т. д.) Поддержка текущих решений для данных при создании новой платформы данных. Обеспечение плавного перехода от текущей архитектуры к целевой Построение инфраструктуры, необходимой для оптимального хранения данных, обработки данных и предоставления данных клиентам, возможность работы с данными (обработка, преобразование, объединение, выполнение ad-hocs, расчет) для команд Подготовка наборов данных из различных источников по запросу коллег Настройка мониторинга, определите SLA, анализ метрик доставки и обработки данных Разработка встроенного решение для сверки данных Сборка больших и сложных наборов данных, отвечающие функциональным/нефункциональным бизнес-требованиям (бизнес-кейсы). Повышать надежность, эффективность и качество данных  Для этого нужно:  Опыт проектирования баз данных с помощью устройств массовой параллельной обработки: Druid, BigQuery, ClickHouse, PostgreSQL и т. п. Более 2-х лет практического опыта развертывания производственного кода качества Профессиональный опыт использования Python, Java или Scala для обработки данных (предпочтительно Python) Знание и опыт работы с пакетами Python, связанными с данными Практический опыт внедрения лучших практик ETL (или ELT) в масштабе Опыт работы с потоковыми технологиями, такими как Kafka, и технологиями платформ данных, такими как ClickHouse, MongoDB. Практический опыт работы с инструментами конвейера данных (Airflow, Luigi, Azkaban, dbt) Экспертное знание SQL и оптимизация запросов Git DVCS    Soft Skills:  Ориентация на детали и забота о качестве данных Умение работать со строгими сроками Вы умеете справляться с конструктивной критикой и умеете развивать отношения с командой для достижения общих целей Вы умеете внимательно слушать, и вы умеете убеждать и аргументировать.    Работая с нами у тебя будет:  Развитие: оплачиваем профильное обучение, конференции, курсы английского.  Свобода: удаленная работа, гибкий график, можно работать из дома или из коворкинга, затраты на который мы компенсируем.   Международный опыт: работа с коллегами из Европы, США и других стран, прокачка английского прямо на работе, по-настоящему международные продукты.  Забота: расширенный ДМС с первого дня работы, скидка на страхование членов семьи, корпоративный психолог, помощь в сложных жизненных обстоятельствах. "
"67998194","СБЕР","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","[]","Мы - новая быстрорастущая команда, создающая сервис, который будет управлять цифровыми рекламными поверхностями и платформой по размещению рекламы на поверхностях Банка и Экосистемы. Используя данные СБЕРа и технологические решения его экосистемы, мы сможем реализовать уникальный сервис на рынке digital marketing. Он позволит нам показывать пользователям рекламу, которая будет нравиться и которая будет делать рекламные продукты эффективными и измеримыми для рекламодателей. Если вы талантливый и амбициозный специалист в сфере ИТ, если вы хотите быть причастными к созданию лучших продуктов для лучших клиентов и уверенно отвечать за результаты, то мы рады принять вас в команду. В рамках данной позиции предстоит:  разрабатывать и поддерживать микросервисы для работы с данными в экосистеме Hadoop развивать и сопровождать DWH на базе Hadoop.  Наши ожидания от вас:  практический опыт разработки и хорошее знание Python опыт промышленной разработки на стеке Java/Scala/Python практический опыт разработки приложений с использованием инструментов Spark Streaming, Hbase, Spark, Kafka, Hive, Impala, Hue и т.д. хорошие знания SQL, опыт работы с одной из реляционной БД - Oracle/PostgreSQL/mySQL/MS SQL Server понимание подходов к организации разработки (CI/CD, DevOps), практический опыт работы с инструментами Jenkins, SonarQube, TeamCity, Anisble, Git (BitBucket/GitLab).  Мы предлагаем:  интересные задачи по продуктам, влияющим одновременно на всю Экосистему: мы пишем продукты с 0, а значит никакого legacy и свобода творчества команда специалистов из топовых ИТ компаний регулярное обучение и профильные конференции, современное оборудование для работы официальное трудоустройство согласно ТК РФ белая заработная плата ДМС оздоровительные программы для детей сотрудников возможность обучения за счет компании выплаты материальной помощи в особых/чрезвычайных случаях дисконт-программы от компаний партнеров льготное кредитование комфортный офис и гибкий график. "
"69953081","VK","Data Engineer (Data Office)","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'ETL', 'DWH', 'Hadoop', 'Airflow', 'SQL', 'Big Data']","Сейчас мы ищем разработчика на задачи разработки DWH-платформы и автоматизации ETL-процессов. Команда занимается разработкой платформы для хранения и обработки данных ключевых сервисов VK (Почта, Медиапроекты, Облако, Звонки, Маруся и др.). А также разработкой сервисов для решения различных бизнес-задач с этими данными. В настоящий момент наш вычислительный кластер хранит десятки петабайт данных. Для нас важен большой опыт Python-разработки. У нас много разноплановой разработки, часть из которой связана с железом. На проекте используются реляционные базы данных, Hadoop, Spark, Airflow, bash. Часть кода написана на Java/Scala, поэтому их знание будет для нас большим плюсом. Задачи:  интеграция разнородных систем-источников; проектирование и сопровождение потоков данных; разработка инструментов качества данных; доработка и поддержка CI/CD.  Требования:  навыки разработки на Python; опыт работы с реляционными СУБД, отличное знание SQL, понимание, как работают индексы, опыт оптимизации запросов; умение писать код, решать алгоритмические задачи.  Будет плюсом:  имеется представление о работе аналитических СУБД и MPP-систем (Vertica, ClickHouse или др.); знание методологий проектирования хранилищ данных; опыт разработки потоков данных на Airflow; опыт работы со стеком Hadoop (Spark/Scala). "
"54996943","МТС","Data Scientist / ML-инженер (Big Data)","False","None","None","От 3 до 6 лет","Полный день","['Python', 'Machine Learning', 'Data Science', 'Математическая статистика', 'Git']","Big Data МТС – место, где телеком данные превращаются в реально работающие IT-продукты. Мы создали и протестировали несколько десятков сервисов. Самые успешные из них уже стали частью экосистемы МТС. Например, МТС Маркетолог, рекомендации в KION (МТС ТВ), услуга “Кто звонит?” или Спам blacklist. Кого мы ищем? Сейчас мы ищем Data Scientist’ов и ML-инженеров в следующие продукты:  Скоринг (Middle)  Продукт направлен на выявление потенциальных дефолтников в банке и мошенников (фродеров) на интернет-площадках с помощью ML-моделей. Мы создаем рисковые и антифрод ML-модели, а также создаем Auto-ML.  Customer Profile (Middle/Senior)  Группа Customer Profile занимается стратегически важной задачей, а именно задачей формирования профиля клиента. Команда занимается выделением персон по пользованию и персонализацией (обогащение персоны профилем).  Сustom Recsys (Middle/Senior/Lead)  Мы в BigData помогаем развивать экосистему МТС, поэтому активно занимаемся рекомендательными система как в продуктах, так и в виде SaaS/платформы. В команде Сustom Recsys - мы строим рекомендательные системы для различных продуктов компании: МТС Библиотека, Ticketland, МТС Live, Shop MTS, МТС Банк. Обязательно:  опыт работы от 2 лет в области анализа данных и машинного обучения вы знаете, как работают ML-алгоритмы и не будете тратить время на эксперименты с заведомо плохими решениями понимаете, когда нужно остановиться и использовать вместо ML более простые и быстрые подходы у вас продвинутые знания Python, в т.ч. основных ml-библиотек умеете делать препроцессинг данных на SQL или PySpark умеете работать с git есть базовые навыки работы в Linux/Unix  Желательно:  знаете минимум один из классических языков C, Java, Scala, C/C++/C# и есть опыт программирования в прошлом есть опыт вывода ml-решений в продакшн  Что предстоит делать?  выгружать и готовить/обрабатывать данные (находить аномалии и инсайты) перебирать гиперпараметры ml-моделей, пока кросс-валидация не даст нормальный результат :) дорабатывать ml-модели из стандартных библиотек проверять бизнес гипотезы в offline и готовить дизайн A/B тестов доводить модель до прода совместно с разработчиками  Что вы найдете в команде Big Data? Стек технологий:  работаем с данными на классическом hadoop-стеке (Spark, Hive) разрабатываем на python3: R&amp;D делаем в Jupyter, продуктивизируем в PyCharm обучаем модели на отдельных мощных машинах с видеокартами Tesla V100 используем собственные разработки для скоринга больших данных и MLFlow для экспериментов храним код в gitlab, CI/CD в Jenkins, процессы запускаем в Airflow  Команда: в команде Data Science сейчас 30 человек (во всей Big Data МТС более 300 человек). Все DS поделены на группы со своими лидами - есть группа рекомендательных систем, скоринга и другие. Каждую неделю мы обмениваемся опытом на совместных синках. DS работают в продуктах со своей автономной командой, в которой есть все роли: аналитики, DE, DS, разработчики, девопсы, менеджеры продукта. Условия: каждый месяц - аванс и зарплата, дважды в год - премия. ДМС + стоматология, корпоративная связь, специальные предложения от партнеров и друзей МТС, отпуск 31 день в год. Выдаем 16 MacBook Pro или Dell на выбор. Есть ли обучение?  Локальные конференции, митапы Корпоративный университет МТС и масштабная виртуальная библиотека А ещё мы регулярно обмениваемся опытом на совместных синках с лидами экспертизы  Какой график? Гибкое начало рабочего дня в промежутке с 8 до 11. Есть возможность работать несколько дней вне офиса по договоренности с командой. Сколько этапов при отборе? Не более трех:  HR + первое тех. интервью с лидом направления Тестовое задание/второе интервью - по необходимости Собеседование с PO и командой, выбор кандидатом проекта "
"70096933","Лига Цифровой Экономики","Data Engineer (middle)","False","None","None","От 1 года до 3 лет","Удаленная работа","['Hadoop', 'Spark', 'SCALA', 'SQL', 'Java', 'DWH', 'Python', 'ETL', 'Databases']","Крупный проект DWH, в одном из ТОП Банков России.  Твои задачи:  Заниматься проектированием и разработкой витрин данных для анализа и моделирования; Заниматься мониторингом и оптимизацией процессов сборки витрин; Заниматься загрузкой и обработкой данных из различных источников; Заниматься поддержкой и развитием базы знаний; Предоставлять экспертную поддержку внутренним потребителям(data analysts,data scientists).  Что мы ждем от тебя:  Знание SQL; Хорошее знание устройства Hadoop,Spark,Hive/Impala; Опыт разработки на Python/Scala/Java; Понимание основных концепций DWH; Понимание базовых команд Git и основных принципов работы; Будет плюсом: знание или опыт в Airflow. "
"70012988","РТ Лабс","Data Engineer (Портал gosuslugi.ru)","False","None","None","От 3 до 6 лет","Полный день","[]","Наша команда объединяет более 1600 IT специалистов в 13 офисах по всей стране. Мы гордимся нашими проектами:  Портал gosuslugi.ru, число пользователей которого достигает 130 млн Высоконагруженные системы электронного правительства Инновационная платформа биометрической идентификации Передовые отраслевые и бизнес-решения  Мы ищем профессионалов, способных мыслить глобально и создавать тренды. Сейчас нам требуется Data Engineer на проект АНАЛИТИЧЕСКАЯ ПЛАТФОРМА. Стек технологий: Python, Airflow, NiFi, Hadoop, Spark, Kafka, GreenplumЧЕМ ТЫ БУДЕШЬ ЗАНИМАТЬСЯ:   Участвовать в разработке Data Lake и хранилища данных в роли Data Engineer; Участие в разработке архитектуры ETL процессов и инструментов для работы с данными; Участие в тестировании; Обеспечение контроля качества данных, мониторинг, анализ и устранение проблем с качеством данных.  ТЫ НАШ ИДЕАЛЬНЫЙ КАНДИДАТ, ЕСЛИ У ТЕБЯ ЕСТЬ:  Знание SQL и оптимизация запросов; Опыт разработки с использованием Python; Опыт работы с реляционными СУБД (Oracle / PostgreSQL / MSSQL); Опыт разработки и внедрения алгоритмов работы с данными (матчинг данных и др.); Опыт визуализации данных (BI, Python: Pandas+matplotlib и т.д)   СОВСЕМ КРУТО, ЕСЛИ:  Знаком с экосистемой Hadoop (HDFS, Hive, Spark, HBase, etc.) Знаком с MPP системами (Greenplum, Teradata, Vertica и т.д.)    КОМПАНИЯ ПРЕДОСТАВЛЯЕТ:  Социально значимые проекты; Конкурентная зарплата; Квартальные и годовые премии; Расширенный полис ДМС со стоматологией и международной страховкой; Компенсация 16 видов расходов на выбор: КАСКО, ОСАГО, ДМС для родственников, билеты на концерты, оплаты дет. сада, авиа и жд билеты; Классная программа корпоративного обучения и оплата любых проф. курсов; Гибкое начало рабочего дня: с 8 до 11. Выбор комфортного формата работы: удаленный или гибридный (2-3 дня в офисе); Увеличенное количество дней отпуска. "
"70034071","Cotton Club (Коттон Клаб)","Data Engineer (Специалист MS SQL)","True","None","180000","Нет опыта","Полный день","['SQL', 'SSAS', 'Business Intelligence Systems', 'MS SQL Server', 'Olap (online analytical processing)', 'Cистемы управления базами данных']","CottonClub – современная, динамично развивающаяся компания, которая имеет высокие темпы роста, команду профессионалов и устойчивые отношения с бизнес - партнерами. Уже несколько лет мы являемся Лидерами на рынке ватно-бумажной продукции и средств гигиены с собственными производственными площадками на территории ЕС и РФ, сертифицированные по всем видам выпускаемой продукции. Компания развивает направления: &quot;КРАСОТА&quot;, &quot;ДЕТСТВО&quot;, &quot;ЗДОРОВЬЕ&quot;, &quot;HOUSEHOLD&quot;, &quot;Косметическая продукция&quot;. В портфель Компании входят такие известные бренды как: &quot;Я Самая&quot;, &quot;AURA&quot;, &quot;Солнце и Луна&quot;, &quot;Просто Чисто&quot;, &quot;CLEAN RACE&quot;, &quot;MADITOL&quot;, &quot;QUALITA и др. Мы помогаем людям заботиться о себе, улучшая качество жизни взрослых и детей В связи с развитием компании открыта новая вакансия &quot;Data Engineer&quot; Обязанности:  Поддержка и разработка хранилища данных на MS SQL Server; Создание и поддержка аналитических витрин и OLAP-кубов; Контроль качества данных в витринах и OLAP-кубах.  Требования:   Высшее/ незаконченное высшее образование;  Продвинутый уровень: MS SQL Server, SSAS, SSIS; Средний уровень: XML, Excel; Высшее техническое образование; Опыт работы более 3 лет в качестве Data Engineer; Опыт разработки и поддержки витрин данных, OLAP и BI-систем на основе MS SQL.  Условия:  Официальное оформление по ТК РФ, оплата отпуска, больничных листов и пр.; Уровень заработной платы обсуждается по результатам собеседования; График работы: понедельник-пятница, 8.00/09.00 - 17.00/18.00 ч.; Возможность работать в офисе в г. Железнодорожный или офисе г. Москва, бульвар Энтузиастов, дом 2 (метро Римская, Площадь Ильича). "
"69905007","СБЕР","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","[]","В настоящий момент мы развиваем собственную аналитическую платформу на базе ClickHouse и GreenPlum, в будущем доступную в облаке. Задачи: ∙ Проектирование и настройка процессов трансформации данных используя внутренние и внешние источники данных; ∙ Обеспечение полноты и доступности данных для решения задач в области статистического анализа и машинного обучения; ∙ Проектирование, разработка и поддержка инфраструктуры для хранения и обработки больших данных; ∙ Проектирование и настройка систем отчетности для разовых и периодических выгрузок данных для внутренних и внешних заказчиков.   Требования: ∙ Высшее образование; ∙ Уверенные знания SQL; ∙ Опыт работы с хранилищами данных и MPP-системами; ∙ Уверенные знания принципов работы баз данных и распределенных систем хранения и обработки данных; ∙ Владение Python будет плюсом; ∙ Английский на уровне чтения документации.   Мы предлагаем: ∙ трудоустройство согласно ТК РФ; ∙ высокий доход (оклад + годовая премия); ∙ возможность работать по смешанному графику; ∙ регулярное корпоративное обучение; ∙ социальный пакет (ДМС, страхование от несчастных случаев и тяжелых заболеваний; тренажерный зал; йога); ∙ возможность работать с последними версиями программных продуктов; ∙ льготные условия кредитования."
"67512061","СБЕР","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'ML', 'Airflow', 'SQL']","О проекте: Наша команда создает информационную платформу для решения аналитических и исследовательских задач в области создания продуктов и услуг на мировых финансовых рынках: торговля валютой, ценными бумагами, производными финансовыми инструментами. Этот бизнес потребляет и порождает огромное количество данных, которые должны обрабатываться c целью: ∙ Разработки витрин данных и моделей машинного обучения ∙ Поддержки регулярных бизнес-операций ∙ Проведения Ad hoc анализа ∙ Подготовки регулярной аналитической и управленческой отчетности Разрабатываемая нами информационная платформа – высоконагруженное решение, призванное обеспечить достижение всех этих целей.   Обязанности:  Проектирование и разработка корпоративной аналитической платформы (ПКАП) Системная разработка, поддержка и оптимизация ETL-процессов и ML-моделей (MLOps) на платформах Greenplum и Hadoop R&amp;D, реализация пилотов по выбору технологий и решений Поддержка промышленной эксплуатации разработанных решений Проведение презентаций и обучения процессам в ПКАП членов продуктовых команд Поддержание технической документации в актуальном состоянии    Требования:  Опыт промышленной разработки на python (ETL-процессы Airflow) Опыт с экосистемой Hadoop (HDFS, Hive, Impala, Spark, Oozie, ...) Опыт работы с CI/CD решениями на базе Jenkins и Bitbucket/Git Опыт работы с популярными РСУБД (Greenplum, Teradata, Oracle, MSSQL) Знание SQL, PL/SQL Знание основ администрирования ОС Linux Понимание построения хранилищ данных (DWH)    Будет плюсом:  Опыт программирования на Java/Scala Создание моделей машинного обучения, прохождение курсов по машинному обучению Понимание основ облачных технологий и технологий виртуализации и контейнеризации Практический опыт с автоматизацией развертывания ПО Опыт администрирования РСУБД "
"55212183","Тинькофф","Data Engineer","False","None","None","От 3 до 6 лет","Удаленная работа","['Python', 'SQL', 'Java', 'ООП', 'Pandas', 'DWH', 'Базы данных', 'ETL']","Мы создаем собственную платформу данных, которая включает в себя инструменты по получению, обработке, загрузке и визуализации данных. С помощью этих инструментов мы каждый день загружаем терабайты информации в единое хранилище данных (DWH), приводим ее к удобному виду, предоставляем нашим пользователям и даем возможность на основании их строить важные отчеты, графики, проводить анализ. Часть из этих инструментов мы разрабатываем с нуля, часть - разрабатываем на основе opensource решений, таких как Apache Airflow, NiFi, Zepellin, Flink и других. В нашу команду нужны талантливые дата-инженеры, которые будут развивать продукты Data Platform. Чем предстоит заниматься  Участие в разработке ядра продуктов Data Platform Разработка и оптимизация процессов выгрузки данных  Что Вам необходимо  Знание одного из языков программирования: Python, Java, Go Знание основ ООП, теории алгоритмов, структур данных Знание SQL Опыт написания процессов загрузки данных (ETL) Будет плюсом знакомство с современными системами хранения данных: MPP, NoSQL Будет плюсом знакомство с: Pandas, Numpy, Groovy Исследовательский склад ума, любознательность, широкий кругозор, желание изучать новые технологии  Мы предлагаем  Работу в офисе у метро «Водный стадион». График работы — гибридный  Платформу обучения и развития Тинькофф Апгрейд. Курсы, тренинги, вебинары и базы знаний. Поддержка менторов и наставников, помощь в поиске точек роста и карьерном развитии   Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким  Бесплатный фитнес-зал Tinkoff Sport. Тренируйтесь, посещайте групповые программы, грейтесь в сауне и участвуйте в спортивных турнирах  Бесплатные обеды в Tinkoff Cafe. А если захотите перекусить, на каждом этаже есть кухня с чаем, кофе и фруктами Достойную зарплату — обсудим ее на собеседовании "
"70103342","СБЕР","Data Engineer (Кампании продаж)","False","None","None","От 1 года до 3 лет","Полный день","[]","Ищем кандидата на вакансию Data Engineer (Аналитик-разработчик) в команду, занимающуюся разработкой витрин данных для клиентской аналитики и кампаний продаж. Что нужно делать: • анализировать потребности бизнеса в данных и исследовать источники данных • разрабатывать и сопровождать витрины данных • работать в Agile-командах и взаимодействовать с другими командами Ты нам подходишь, если есть: • опыт разработки витрин данных • опыт работы с ETL-инструментами • опыт работы с SAS/Oracle/Greenplum/Hadoop • знание процесса анализа требований и разработки программного обеспечения Будет плюсом: • участие в сложных интеграционных проектах и работа с большими объемами данных • знание Kafka, Spark, SAS DIS, Informatica, Oozie, Hive, Impala, Java, Python, SAS Base, BitBucket, Jenkins • понимание процессов DevOps • знание и умение работы с wiki-системой Confluence, трекинговыми системами (Jira, Redmine и т.д.). • опыт оптимизации запросов/кода и ревью кода других разработчиков Наши условия: · регулярное корпоративное обучение; · ДМС, страхование от несчастных случаев и тяжелых заболеваний; · материальная помощь и социальная поддержка, корпоративная пенсионная программа; · льготные условия кредитования; · современный офис в центре Москвы (Кутузовский пр-кт) · трудоустройство согласно ТК РФ."
"69859250","Altenar","Data Engineer","False","None","None","От 3 до 6 лет","Удаленная работа","['Git', 'Kubernetes', 'Spark', 'Business intelligence', 'Hadoop', 'HIVE', 'HDFS', 'Data engineer', 'DWH', 'ETL']","На стартовавший проект Altenar Intelligence Platform мы приглашаем талантливого инженера с опытом администрирования Hadoop, готового принять участие в построении и развитии DWH на основе данных с других продуктов компании. Если вы готовы к работе в тесном сотрудничестве с разработчиками, DevOps, аналитиками при построении и масштабировании хранилища, мы будем рады видеть вас в нашей многонациональной и мультикультурной команде! Просьба направлять резюме на английском. Важно свободное владение английским языком (команда международная, общение внутри команды на английском языке) Чем предстоит заниматься:  Непрерывная разработка и развитие хранилища и процессов сборки данных Участие в разработке систем визуализации данных, нацеленных на помощь бизнесу в принятии решений - DDDM (Data-driven decision-making) Взаимодействие с командами инфраструктурных инженеров, инженеров по работе с данными и аналитиков. Поддержка и управление кластеров и хранилища HDFS (+HIVE) Развитие и поддержка инструментов для анализа данных Взаимодействие с инженерами по администрированию серверов Обеспечение доступности платформы путем мониторинга, поиска и устранения узких мест, резервного копирования и восстановления Планирование роста и увеличение мощностей для поддержки платформы Участие в обсуждении архитектурных решений, в том числе и других проектов компании, источников данных Обеспечение безопасности данных внутри хранилища  Профессиональные знания и навыки:  Опыт работы с ETL/ELT системами Опыт в развертывании систем обработки данных как потоковом режиме так и пакетном Опыт построения или участия в построении хранилищ данных, выбора технологий/инструментов и последующей поддержки Знание и опыт работы с одном из языков Python(Pyspark),Scala или Java. Наличие сертификатов будет плюсом. Навык работы с очередями, такими как Kafka и RabbitMQ Опыт работы с Grafana, Prometheus приветствуется Практический опыт с Git Глубокое понимание механизмов работы IO подсистем, сетевых протоколов, систем виртуализации Опыт работы с Docker, Kubernetes как плюс. Навыки работы с Ansible приветствуются Свободное владение английским языком (команда международная, общение внутри команды на английском языке)  Мы предлагаем:  Работу над интересными проектами, на которые вы сможете активно влиять Оформление по ТК РФ и достойную белую заработную плату Peformance review 2 раза в год ДМС, стоматологию и страхование жизни за счет работодателя с первого дня работы Удаленную работу или работу в офисе по вашему выбору (Москва, Санкт-Петербург, Владимир) Корпоративную парковку Бесплатное корпоративное обучение английскому Участие в профильных конференциях, митапах и корпоративных активностях по желанию Возможность сертификации и обучения за счет компании "
"68293641","Ренессанс cтрахование, Группа","Data engineer","False","None","None","От 1 года до 3 лет","Полный день","['SQL']","С 1997 года команда «Ренессанс страхование» помогает нашим клиентам — таким же людям, как и мы сами — в сложных ситуациях. Мы прошли путь от классической страховой компании до компании InsurTech, которая цифровизирует процессы на всем пути клиента и создает новые онлайн-сервисы и digital-продукты. Мы первыми на рынке страхования прошли полноценное IPO в октябре 2021 года! И впереди у нас большие планы, которые мы реализуем вместе с тобой. Сейчас мы идем к тому, чтобы стать одной из передовых insurtech компаний Европы: создаем digital продукты, идем на новые рынки, работаем по agile и скрам, автоматизируем процессы. В команду отдела развития анализа данных IT-департамента ищем Дата-инженера. Что нужно будет делать:  Работать с анализом и нормализацией внешних и внутренних данныx; Заниматься генерацией новых признаков/полей на основе исходных данных для команды data science; Подготавливать витрины данных для команды data science;  Наш идеальный кандидат:  Уверенно владеет SQL(в нашем случае MS SQL); Имеет аналитический склад ума и системное мышление; Аккуратен, склонен к кропотливой и монотонной работе с данными; Склонен к написанию ёмких документаций к данным, скриптам и бизнес-процессам; Владеет представлением о том как работает машинное обучение; Имеет интерес к R&amp;D формату работы по поиску новых эффективных признаков для моделей машинного обучения вместе с командой data science;  Будет плюсом:  Навыки работы на языке Python и с git;  Наши преимущества:Мы собираем команду, которая хочет менять устоявшиеся подходы и создавать новые тренды, тем самым трансформировать классическое страхование в InsureTech. Наши продукты помогают сделать мир безопаснее, и мы получаем живой отклик от наших клиентов, постоянно улучшая качество предоставляемого сервиса. «Ренессанс страхование» заботится о своих сотрудниках, ведь мы понимаем, что наши люди — ключевая движущая сила бизнеса. Мы будем рады, если ты присоединишься к нам. У нас здорово работать, потому что:  Мы развиваем InsurTech. Диджитализируем сервисы, чтобы упростить жизнь клиентам и изменить рынок. Делаем продукт, которым пользуются миллионы людей и мы сами. У нас сильная команда. Можно делиться знаниями и получать новые. Поддерживаем быстрый темп работы, не стоим месяцами на месте в ожидании согласования, внедряем Agile во все сферы бизнеса, не только в IT. Продвигаем инициативы без бюрократии: если ты найдешь неэффективный процесс, то всегда можешь залидировать его изменение. Закрепляем за каждым новичком бадди, который поддерживает и помогает быстрее адаптироваться. Предлагаем гибридный формат работы: приезжай в офис при необходимости. А еще мы можем оформить тебя удаленно. Подключаем ДМС и страховку от несчастного случая с 4 дня работы и предлагаем выбрать опции в кафетерии льгот на твой вкус. У нас есть система внутреннего обучения сотрудников, где ты можешь стать тренером-экспертом и помочь коллегам прокачать свои навыки. Постоянно обновляем корпоративную электронную библиотеку, у нас более 2000 книг для профессионального развития как хард, так и софт-скиллов. "
"69802072","MTRENDO","Data Engineer","True","78300","130500","От 3 до 6 лет","Полный день","['Анализ данных', 'Аналитическое мышление', 'SQL', 'Python', 'Django Framework', 'Google Cloud Platform', 'Google Big Query', 'Яндекс.Метрика', 'Yandex Data Lens', 'JavaScript', 'DevOps', 'Google Analytics']","Полный рабочий день // Можно удалённоНаша команда работает с большими клиентскими данными, помогая бизнесу с оборотом в десятки млрд рублей принимать взвешенные решения. Мы ищем человека, который будет вместе с нами подключать и разрабатывать конвейеры обработки данных, строить дашборды и отчеты, настраивать аналитику на сайтах и в приложениях. Вам с нами будет комфортно работать, если вы:   владеете Python (если еще и Javascript, то это плюс), SQL, Excel,   понимаете как формируются отчеты в Google Analytics и Яндекс.Метрике;   используете API, ETL или самописные скрипты для автоматизации своей работы и отчетов;   Какие задачи придётся решать:   разбираться в бизнес-задачах клиентов и декомпозировать их на аналитические задачи;   выполнять аудиты по аналитике в уже настроенных ранее системах Google Analytics, AppsFlyer, Adjust, AppMetrica для проверки качества и полноты данных;   разрабатывать системы трекинга событий для Google Universal Analytics, Google Analytics App+Web, Яндекс.Метрика, Matomo на сайтах и в приложениях;   разрабатывать и внедрять системы метрик EEC для оценки эффективности электронной торговли на сайте;   составлять технические задания по внедрению систем сбора данных для клиентов;   разрабатывать архитектуру данных для клиентских проектов;   анализировать причины и разрабатывать решения по минимизации потерь клиентских данных;   писать SQL запросы как для клиентских отчетов, так и для внутреннего анализа;   автоматизировать обработку данных с помощью Apps Script, Python, dbt;   создавать отчеты и дашборды в Google Sheet, Google Data Studio, DataLens, PowerBI;   формировать и проверять гипотезы, позволяющие увеличить ключевые показатели клиентских проектов;   Для понимания уровня ваших знаний, просим пройти тестовое задание: Напишите запрос, который найдет три машины, получившие наибольшее количество чаевых в апреле 2022 года, и выведите, как изменялась эта сумма в процентах в последующие месяцы по сравнению с предыдущим в таблице со столбцами taxi_id, year_month, tips_sum, tips_change. В качестве источника данных используйте bigquery-public-data.chicago_taxi_trips. Пришлите нам код запроса в сопроводительном письме и мы дадим обратную связь на ваше решение в течение 10 дней. Мы оставляем за собой право не отвечать на отклики соискателей без сопроводительного письма или с резюме, которое не соответствует описанию вакансии. Просим с пониманием отнестись к этому."
"69820271","ECOMMPAY IT","Lead/Senior Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Python', 'SQL', 'ETL', 'Анализ данных', 'DWH', 'BI']","Сейчас мы в поиске Senior data engineer/ Data Analyst в команду BI Чем предстоит заниматься:  участвовать в процессе проектирования, разработки и оптимизации хранилища данных (DWH); написанием и оптимизацией сложных SQL-запросов; разработкой, доработкой и сопровождением процессов загрузки и трансформации данных (ETL); анализом качества данных, подготовкой требований и скриптов контроля качества данных; анализом и визуализацией данных по требованиям бизнес заказчиков.  Наши ожидания от кандидата:  высшее техническое образование; опыт в проектировании и разработке информационно-аналитических систем, ETL и BI решений от 2 лет; знание теории баз данных, понимание принципов и практики построения DWH; опыт работы с большими объемами данных (десятки терабайт); понимание методов оптимального хранения и извлечения данных для их быстрого и релевантного анализа; уверенное владение SQL, умение и опыт написания сложных аналитических и оптимальных SQL-запросов.  Будет плюсом:  практический опыт построения DWH; знание и умение программировать на Python; опыт работы с BigQuery, ClickHouse.    Мы предлагаем:  официальное трудоустройство; формат работы офис/из дома с гибким началом рабочего дня с 08:00 до 11:00; конкурентную зарплату (в соответствии с вашими пожеланиями и профессиональными компетенциями); возможность влиять на конечный результат и понимать бизнес-логику продуктов: следуем принципам Agile.   "
"69733931","Цай Галина Вячеславовна","Data Engineer","True","205882","323529","От 1 года до 3 лет","Полный день","['Python', 'SQL', 'Английский язык', 'SCALA', 'AWS']","Responsibilities  · You will design and develop data applications using Python, Scala, SQL · Develop, customize and manage data integration tools, databases, warehouses, and analytical systems · Design, build and maintain scalable data models · Work closely with product teams and other stakeholders to desing and build data marts providing reliable and accurate data · Handle data pipelines and contribute towards data strategy and its execution  Requirements  · 2+ years of experience as a Data Engineer building data pipelines and analyticial data models · At least 2 years of hands-on experience with Python, SQL. Knowledge of Scala will be your great asset · Strong competencies in algorithms and software architecture · Strong experience in real-time data processing and data ingestion · At least 2 years of hands-on experience with Big Data systems l · Experience building data infrastructure using at least one major cloud provider, preferably AWS · Knowledge of Terraform will be your great asset · Advanced English and Russian  What`s in it for you  1. Relocation to Baku if you feel like 1.One way ticket on us if you choose relocation We pay your first month payment for the flat We help with WP и TRP If you want we can give you pre-payment for the 1st month  Aren`t you the one we have been looking for...?"
"49971464","Тинькофф","Business Data Engineer","False","None","None","От 3 до 6 лет","Удаленная работа","['SQL', 'Hadoop', 'Работа с базами данных', 'ETL', 'Базы данных', 'DWH', 'СУБД']","У нас в DWH очень много данных: 6000 объектов, 200 тб в Greenplum и 2 пт в Hadoop. С данными активно работают более 3000+ аналитиков в бизнес командах. На стороне команды DWH созданы инструменты, которые позволяют аналитику не зависеть от etl разработчика и самостоятельно собирать данные для своих задач. Сейчас мы ищем экспертов sql, готовых помогать аналитикам оптимально работать с данными. Наша инфраструктура • Greenplum / Hadoop / Clickhouse в качестве ядра обработки данных • Собственные решения на базе Apache Zeppelin, Apache Airflow и Apache Flink в качестве инструментов трансформации данных для аналитиков (selfservice etl) Мы не требуем знания этих инструментов от кандидатов. Наша задача сделать инструменты работы с данными максимально удобными и доступными. Но нам важно, чтобы кандидат на эту позицию имел большой опыт работы с данными. Обязанности  Быть экспертом по данным: помогать аналитикам в решении вопросов с источниками данных, моделью данных DWH Разрабатывать витрины в помощь аналитикам Выступать заказчиком для разработки витрин в смежных командах Оптимизировать существующие запросы Внедрять и развивать культуру написания оптимальных запросов  Требования  Высшее техническое образование Опыт работы с базами данных в качестве разработчика от 1 года Свободное владение SQL Опыт проектирования объектов БД на основании бизнес требований Понимание теории СУБД и ETL-процессов Знакомство с ETL-инструментами будет плюсом  Мы предлагаем  Работу в офисе у метро «Водный стадион». График работы — гибридный  Платформу обучения и развития Тинькофф Апгрейд. Курсы, тренинги, вебинары и базы знаний. Поддержка менторов и наставников, помощь в поиске точек роста и карьерном развитии   Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким  Бесплатный фитнес-зал Tinkoff Sport. Тренируйтесь, посещайте групповые программы, грейтесь в сауне и участвуйте в спортивных турнирах  Бесплатные обеды в Tinkoff Cafe. А если захотите перекусить, на каждом этаже есть кухня с чаем, кофе и фруктами Достойную зарплату — обсудим ее на собеседовании "
"66550537","Спортмастер","Senior Data / ML Engineer (Персонализация)","False","None","None","От 3 до 6 лет","Удаленная работа","['Python', 'SQL', 'ORACLE', 'Oracle Pl/SQL']","В Департамент управления данными, в связи с расширением инженерной команды, ищем senior специалиста на позицию Data / ML Engineer. Наш департамент проектирует и создает data сервисы, которые помогают оптимизировать взаимодействие с клиентом или внутренние процессы компании. Примеры проектов: рекомендательная система, NLU модуль диалоговой системы, прогнозирование продаж, поисковый движок и пр.Чем предстоит заниматься:  Разрабатывать, оптимизировать и поддерживать пайплайны обработки данных и машинного обучения; Реализовывать высокопроизводительные процедуры расчета витрин признаков; Сопровождать data science специалистов в продуктовых командах.  Наш стек:  Для разработки: Python, Oracle PL/SQL, Spark, Spark Streaming, Impala/Hive, Kafka, Airflow. Для организации работы: Jira, Confluence, BitBucket.  Мы ждём от будущих коллег:  Высшее техническое образование; Навыки написания сложных SQL запросов и их оптимизации; Опыт разработки и автоматизации пайплайнов обработки данных; Уверенное владение Python (многопоточность, структуры данных, пакеты для обработки данных); Умение работать с системами контроля версий (BitBucket/ Github/ Gitlab); Продвинутый пользователь Linux; Способность работать самостоятельно, доводить начатое до конца; Развитые problem solving skills, soft skills; "
"69893073","СБЕР","Data Engineer (Рекомендательная система)","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'Git', 'SQL', 'Spark', 'PostgreSQL']","Дорогой кандидат, мы - динамично растущая команда экспертов в области построения рекомендательных систем. Наша главная цель - построить современную, масштабируемую платформу, которая будет постоянно предвосхищать и превосходить ожидания пользователей, предоставляя им персонализированный и релевантный контент на всем клиентском пути в экосистеме Сбер. Наша платформа будет обслуживать широкий круг потребителей и строить персональные рекомендации во всех сферах бизнеса, таких как музыка, фильмы, онлайн торговля, медицина, логистика и многих других, которые присутствуют в быстро растущей экосистеме. Если ты мечтаешь поучаствовать в создании такой рекомендательной системы, то тебе к нам! Наша платформа будет обслуживать широкий круг потребителей и строить персональные рекомендации во всех сферах бизнеса, таких как музыка (Звук), фильмы (ОККО), онлайн торговля (СберМаркет, СберМегаМаркет), медицина (еАптека) и многих других, которые присутствуют в быстро растущей экосистеме. Если ты мечтаешь поучаствовать в создании такой рекомендательной системы, то тебе к нам! Интеллектуальное ядро такой системы - это алгоритмы машинного обучения, которые анализируют по-настоящему большие данные, и в реальном времени рассчитывают предпочтения миллионов конечных пользователей. Работая в нашей команде, ты будешь участвовать в исследовании, разработке, тестировании и внедрении самых передовых алгоритмов классического и глубокого обучения в части рекомендаций. Ты получишь опыт внедрения таких алгоритмов в реальной индустриальной экосистеме, начиненной большими данными и работающей с высокими нагрузками при их обработке. Мы ищем: Middle/Senior Data Engineer в команду единой рекомендательной платформы для компаний экосистемы Сбер. Что предстоит делать:  Разработка продакшен-пайплайнов обработки данных; Продуктизация прототипов команды Data Science; Performance оптимизации кода по обработке больших массивов данных или онлайн сервисов рекомендаций с высокой нагрузкой.  Стек технологий:  Для разработки используем: Python, PySpark, Pandas, Redis, PostgreSQL, AirFLow, MLFlow и др. Для организации работы: Jira, Confluence, Git.  Что для нас важно:  Мотивация учиться и развиваться в области рекомендательных систем; Экспертное знание Python; Уверенное знание Spark (и желательно Pandas); Опыт написания промышленных пайплайнов обработки данных, содержащих множество шагов, зависимостей и сложную логику; Опыт использования Airflow (или другого industry-standard оркестраторов пайплайнов, т.к. Luigi, Dagster и т.д.); Хорошее понимание баз данных SQL / NoSQL.  Будет плюсом:  Опыт оптимизации пайплайнов препроцессинга данных под highload; Знание Scala/Java; Опыт работы с рекомендательными системами; Опыт работы с облачными платформами; Опыт работы с Kafka, Flink, Hadoop; Хороший уровень понимания Computer Science алгоритмов.  Мы предлагаем:  Официальное трудоустройство согласно ТК РФ; Белая заработная плата (оклад + годовая премия); Страхование (от несчастных случаев, ДМС); Оздоровительные программы для детей сотрудников; Возможность обучения за счет компании; Выплаты материальной помощи в особых/чрезвычайных случаях; Дисконт-программы от компаний партнеров (фитнес, страхование, туризм); Льготное кредитование; Столовая на территории офиса. "
"69908642","АНО ЦИСМ","Data Engineer(Python)","False","None","None","От 1 года до 3 лет","Удаленная работа","['Data Analysis', 'SQL', 'Python', 'Git', 'AMQP', 'ETL на Apache Airflow', 'Minio', 'ELK', 'ClickHouse', 'PostgreSQL', 'GreenPlum', 'Docker', 'DWH']","Центр изучения и сетевого мониторинга молодёжной среды – аккредитованная IT-компания, учреждённая по поручению Президента России в октябре 2018 года.  Деятельность организации ориентирована на создание IT-решений, направленных на формирования комплексной системы по защите детей и подростков от воздействия негативной информации в сети. Будем рады, если Вы:  разрабатываете на Python; умеете строить ETL на Apache Airflow; знаете SQL и активно его применяете в деле, имеете опыт работы с различными БД (rowstore, columnstore), не боитесь работы с терабайтами данных; понимаете принципы построения DWH;  Будет замечательно, если Вы:  умеете работать с AMQP; готовы обогащать данными Datalake на Minio; имеете опыт работы с ELK стеком; готовы к активному взаимодействию и командной работе, готовы понять, что нужно DS и DA, и сделать лучше; не забываете про мониторинг всего написанного, цените Data Quality и тесты; умеете отлаживать и валидировать свой или чужой код; работаете с Git.  Будет плюсом:  опыт работы с ClickHouse, ELK, K8s; желание развиваться и не останавливаться на достигнутом; на данный момент готовы рассматривать кандидатов разных уровней, нам есть, чем с Вами поделиться в области знаний/опыта, а также будем рады узнать Ваше видение.  Условия:  Оформление в соответствии с ТК РФ; Заработная плата по результатам собеседования; График работы: 5/2, с 10:00 до 19:00; Формат работы – в офисе в Москве или удаленно. "
"68168818","РАБЛЗ","Data Engineer (middle+/senior)","True","None","348000","От 3 до 6 лет","Полный день","['Python', 'Linux', 'Git', 'Kafka', 'ETL', 'Spark', 'Базы данных', 'PostgreSQL', 'ClickHouse', 'Redis']","Мы в Rubbles занимаемся созданием Data Science-продуктов и разработкой аналитических решений для различных индустрий: системы предсказания спроса на товары для оффлайн-ритейлов, рекомендательные системы в банках, поисковые системы по товарам для онлайн-ритейлеров и многое другое. Среди наших клиентов: Сбербанк, Пятёрочка, KFC, Перекрёсток, Альфа-Банк, МВидео и др. Мы находимся в поиске Data Engineer уровня middle+ или senior для усиления нашего продуктового направления по разработке платформы прогнозирования спроса и товарооборота в ритейле. Сейчас в команде 5 технических специалистов (data scientists, system analysts, разработчиков), и мы планируем увеличить её вдвое. Мы ищем человека, который взял бы на себя задачи по организации потоков данных в рамках разрабатываемой платформы, оптимизацию систем хранения и обработки с использованием современных программных продуктов и подходов, а также привнес свою экспертизу в общее дело и помогал коллегам в повышении культуры работы с данными в целом. Обязанности:  Разработка и поддержка промышленных пайплайнов обработки данных и машинного обучения на Python и Spark с использованием популярных фреймворков (Airflow, Prefect, Airbyte и т.д.), а также собственных разработок; Разработка коннекторов к системам-источникам данных и системам-потребителям; Организация потоков данных в рамках микросервисной архитектуры платформы; Разработка схемы данных компонентов платформы в едином информационном поле; Работа над оптимизацией систем хранения (Clickhouse, PostgreSQL, Redis); Настройка и поддержка интеграционной шины данных на базе Kafka; Поддержка data scientist’ов и аналитиков данных (также работают на Python) в задачах разработки; Код ревью коллег; Обучение проектных команд в работе с компонентами разрабатываемой платформы.  Минимальные требования:  Опыт промышленной разработки на Python от 3х лет; Опыт работы с популярными ETL фреймфорками (Airflow, Prefect, Airbyte и т.д.) от полугода, практический опыт в создании сложных пайплайнов; Опыт работы с MPP – системами и с колоночными хранилищами данных; Опыт разработки схемы данных, знание основных методологий построения хранилищ данных; Участие в строительстве DWH в роли разработчика; Опыт разворачивания, настройки мониторинга и передача на поддержку разработанных решений; Умение работать с командной строкой Linux/MacOS; Умение вести проекты в Git.  На что ещё смотрим:  Знакомство со Spark; Знания Java\Kotlin; Опыт Devops (Docker, Kubernetes, Gitlab-CI, настройка окружения на серверах и др.); Опыт разработки сервисов (Flask, Django, Asyncio и др.); Опыт работы со Streamsets/NiFi.  У нас:  Участие в интересных проектах на перспективном AI рынке; Удаленная работа при желании; Необходимая техника для работы при желании; Возможность участия в различных интересных проектах в рамках компании, если вдруг заскучал; Поддержка кандидата в профессиональном и карьерном росте; Совместная работа с опытными разработчиками, аналитиками данных, менеджерами, продуктологами; Гибкий график работы; Оформление полностью белое по ТК РФ; ДМС (включая стоматологию) после прохождения испытательного срока; Офис в центре Москвы (2 минуты от м. Сухаревская) со всем необходимым для комфортной работы. "
"69856866","Медиалогия","Data Scientist / ML-инженер","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'Adobe Photoshop', 'Аналитические исследования', 'Аналитическое мышление', 'Статистический анализ', 'Machine Learning', 'Neural Networks', 'Deep Learning', 'Data Science', 'Data Analysis', 'Data Mining', 'Big Data', 'Анализ данных']","Медиалогия разрабатывает высоконагруженные системы, которые в режиме реального времени сканируют весь текстовый сегмент Интернета (100+ млн. сообщений в сутки, 1.7 млрд. метрик) и, используя уникальные технологии лингвистического анализа и компьютерного зрения, позволяют осуществлять мгновенный анализ упоминаний наших клиентов в соц.сетях, блогах, форумах и управлять их репутацией. Задачи:  Построение и тестирование гипотез; Создание моделей для задач NLP и CV; Доработка и улучшение существующих моделей; Анализ данных из социальных сетей: классификация, кластеризация, выявление аномалий; Проведение экспериментов на &quot;больших данных&quot; и обработка результатов;  Требования:  Уверенное знание Python и библиотек для работы с данными (numpy, pandas, sklearn, catboost и др.); Умение писать чистый хорошо структурированный код; Опыт создания моделей машинного обучения на TensorFlow, PyTorch и др.; Умение работать с неструктурированными контентом, правильно оценивать качество моделей; Умение работать в команде, хорошие коммуникативные навыки, Желание активно развиваться и браться за самые сложные задачи. Желателен опыт работы с большими данными (Spark, Hadoop);    Условия:  Белая ЗП, официальное оформление по ТК РФ; ДМС с хорошим выбором клиник и международной страховкой; Современный офис в 10 мин от м. Дмитровская (БЦ «Савеловский Сити» с массажистом, кикером, турником и PlayStation) или полная удаленка; Гибкое начало рабочего дня; Возможность посещения профильных мероприятий и карьерное развитие; Рядом дизайн-завод Флакон, кафе и магазины. "
"67907433","24Н Софт","Middle/Senior Data Engineer","True","200000","None","От 3 до 6 лет","Гибкий график","['SQL', 'ETL', 'MS SQL', 'Apache AirFlow', 'Python', 'Pandas', 'Numpy', 'Bash', 'GitHub', 'Git']","Мы специализируемся на разработке программного обеспечения больших, высоконагруженных и производительных систем, созданием сервисов и клиентских приложений для букмекерских компаний.В настойщий момент в команду Data Science ищем опытного Data Engineer для решения вопросов с данными для построения моделей оттока, антифрода, рекомендаций, разработки дашбородов для мониторинга ключевых клиентских метрик. Вас ждут миллионы клиентов и миллиарды транзакций, возможность раскрыть свой потенциал и получить удовлетворение от того, как результаты вашего труда дают эффект в реальном бизнесе. Чем предстоит заниматься:   Извлечение, преобразование, загрузка данных и их обработка   Построение датасетов для data science моделей и аналитики   Построение надежных и оптимальных пайплайнов обработки данных   Интеграции с новыми источниками данных   Оптимизация вычислений и затрат на хранение данных   Что мы ожидаем от кандидата:   Уверенные знания SQL и опыт работы с базами данных;   Опыт написания процессов загрузки данных ETL   Уверенное знание Python (Pandas, Numpy), опыт разработки, желание писать аккуратный и красивый код   Желателен опыт работы с командной строкой Linux (Bash) и GitHub   Опыт работы с Hadoop, Pyspark будет преимуществом   Желательно понимание терминов Data Sciencе при работе с данными (обучающая и валидационная выборки, Data Leak)   Что мы предлагаем:   Оформление в штат компании, полное соблюдение ТК РФ (отпуск, больничный), официальная заработная плата   Персональный гибкий график   После испытательного срока мы подключаем ДМС и еженедельное посещение спорт зала   А еще:   Лучшая техника (Apple) и софт   Интересная предметная область и сложные технические задачи, возможность развития и роста   Просторный и современный офис в 10 минутах пешком от м. Домодедовская, в котором комфортно работать и приятно общаться с коллегами   Бизнес-ланчи в нескольких ресторанах рядом с офисом Компании   Уютные кухни-столовые с чаем, кофе и конфетамиДля того, чтобы мы быстрее связались с Вами, просим ответить на простой вопрос при отклике.  "
"69772409","NobleHire","Data Engineer (Senior) Cloud Data Warehouse - to Bulgaria","True","134850","224677","Более 6 лет","Полный день","['Python', 'SQL', 'SCALA', 'Data Analysis', 'Azure', 'Snowflake', 'Azure Databricks', 'Microsoft Azure', 'cloud data warehouses', 'Data acquisition', 'ETL/ELT Processes', 'data warehouse architecture', 'big data integration', 'Azure DevOps', 'Data Factory']","Уважаемые соискатели, меня зовут Елена и я помогаю компании с головным офисом в Германии и офисом в Болгарии в поиске кандидатов, готовых к релокейту в Болгарию. Пожалуйста, прочитайте объявление полностью - я постаралась максимально подробно предоставить Вам всю необходимую информацию относительно позиции. Компания предлагает релокационный пакет для кандидата (и его семьи, если имеется), состоящий из:  покрытие всех административных расходов (адвокат, переводы, легализация, визы и и т.п.) покрытие расходов, связанных с переездом (билеты, багаж, перевоз имущества) обязательная страховка иностранцев покрытие расходов, связанных с арендой квартиры - комиссия агенту по недвижимости и первые 3 месяца аренды недвижимости  Плюс к этому, каждый сотрудник получает: - дополнительное медицинское страхование - фитнес-абонемент - оплата парковки или оплата проезда на общественном транспорте Процесс оформления разрешения на работу (по состоянию на сентябрь 2022) - занимает от 2 до 4/5 месяцев (крайний случай). DESCRIPTION: As a Data Engineer, together with a competent team, you will collect business and technical requirements and convert them into solutions for preparation and integration of decision-relevant information. You will work in the field of cloud data warehousing with a focus on Microsoft Azure Data Platform and will be able to work and develop with the newest technologies. REQUIREMENTS:  At least 5 years of experience in Microsoft Azure, Azure Databricks and Snowflake Azure DevOps cloud experience and knowledge of related services like Data Factory will be considered as an advantage Good knowledge in development with SQL, Python or SCALA Know-how regarding data warehouse architecture and big data integration Familiar with ETL/ELT Processes  TECH REQUIREMENTS: Primary Language - Python Primary Platform or Framework - Azure Secondary Language - SQL Main Database - Snowflake RESPONSIBILITIES:  Build, test and support data pipelines Analysis as well as re-architecture of cloud data warehouses Adaptation of the architecture to the business requirements of the customer Identify opportunities in the context of improving the reliability, efficiency and quality of the data Data acquisition  TOOLS WE USE: Python, Scala, SQL, Microsoft Azure, Databricks, Data Factory HIRING PROCESS: 1. Preliminary interview - get to know the company and the candidate&#39;s background 2.Technical interview 3. Promptly feedback CULTURE AND PERKS: Flexible working hours Early riser? Late riser? Say goodbye to a 9 to 5 job. Personal development Every employee has the freedom to promote projects, know-how or soft skills. Qualifications can only grow through continuous learning. Learn together Our employees benefit from a wide range of software training, courses and certifications. This makes our employees experts in modern BI solutions who like to use their knowledge for our customers. Social activities We look forward to bringing back our full social calendar of celebrations, team builds. Together we are strong Fun is a must. There are regular events that promote team, cohesion and exchange. We take part in business runs, organize family days, a Christmas parties and have often traveled together. Free drinks Enjoy free drinks with your colleagues. Также есть вакансия и для Middle Data Warhouse Developer - 2.000-3.500 Eur gross    "
"67991690","Hoff","BI/Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['MS SQL Server', 'Python', 'Transact-SQL', 'MS SQL', 'ETL', 'power bi']","Компания Hoff - специализированный маркетплейс, мы активно развиваемся в ИТ, планируем использовать Data driven и Data Quality подходы.Сейчас мы решаем глобальную задачу рефакторинга DWH – есть возможность реализовать идеи новой структуры архитектуры хранения данных, новых витрин, и принять участие в построении real-time отчетности. Стек: MSSQL (OLAP, T-SQL), Power BI, Airflow, Python, BigQuery, YandexCloud. Задачи:  Участие в развитии DWH (новая модель структуры, участие в миграции); Формирование пользовательских витрин данных, отчетности на уровне базы и кубов; Написание Python скриптов / DAG-ов для интеграции с внешними источниками; Интеграция данных в OLAP (табулярные модели, многомерные кубы).  Для сотрудничества с нами вам понадобятся:  Опыт разработки на ANSI-SQL (опыт разработки именно в MS SQL Server будет плюсом) Практический опыт разработки ETL с помощью Airflow (Python) или T-SQL/ ANSI-SQL.  Плюсом будет:  Опыт работы с Power BI / Talend open studio. Опыт работы с OLAP кубами SSAS; Опыт работы с BigQuery, ClickHouse; Опыт работы с высоконагруженными системами DWH, DataLake.  Мы предлагаем:  Возможность развития в крупной компании; Оплата: оклад + квартальный бонус; Удаленно или офис – как удобно; График работы 5/2 c 09-18, 10-19.00; ДМС со стоматологией, корпоративные скидки для сотрудников (до 20% с первого дня работы).  ​​​​​​​При отклике просьба указать уровень желаемой оплаты (net.), если не указан в резюме!"
"69821819","Aero","Data Engineer (Middle)","False","None","None","От 1 года до 3 лет","Гибкий график","['SQL', 'СУБД', 'Python', 'Airflow', 'dbt', 'Базы данных', 'ClickHouse', 'PostgreSQL', 'GreenPlum']","Привет!) Мы Aero — e-commerce разработчик № 1 в России. Делаем онлайн-шопинг простым и приятным, а магазины – надежными и безопасными.В команду к молодым и дружным разыскивается молодой и дружный коллега Что предстоит:  Участие в построении хранилища данных: проектирование, определение сущностей, формирование витрин данных Разработка и оптимизация процессов выгрузки данных из различных источников Разработка процессов обработки данных Оркестрация ETL/ELT процессов в Airflow Поддержка аналитической инфраструктуры  Чего ждем: — Знание MSSQL + OLAP; — Опыт работы с SSAS. Не обязательно, но будет очень круто:  Работа с Airflow и/или dbt; Работа с одной или несколькими базами: ClickHouse, GreenPlum, Postgres; Опыт разработки ETL/ELT процессов.  С чем мы работаем:   ClickHouse, GreenPlum, Postgres; Yandex.Cloud, Google Cloud Platform (в меньшей степени); Python, SQL; Airflow, dbt; Git; (здесь может быть прикольная технология, которую ты предложишь внедрить).  Что предлагаем: Мы готовы гибко обсуждать любые условия. Будем исходить из ожиданий успешного кандидата. Базовый пакет включает:  Продуктовая команда, открытая твоим идеям, руководитель и коллеги с сильной экспертизой; Оформление ТК РФ, белая ЗП, прозрачная система роста; Гибридный формат на выбор (офис м. Достоевская), свободное начало дня; ДМС, английский со скидкой 50%, крутые корпоративы, винишко у тебя в бокале. "
"70176602","BestDoctor","Data Engineer (remote)","True","200000","None","От 3 до 6 лет","Полный день","['PostgreSQL', 'Python', 'SQL', 'AirFlow', 'GreenPlum']","Кто мы: BestDoctor — экосистема медицинских и страховых сервисов, созданная экспертами для удобного управления здоровьем. Мы меняем рынок медицинского страхования и отношение людей к своему здоровью с помощью качественного сервиса и принципиально нового клиентского опыта. Благодаря глубокой экспертизе в создании медицинских решений и любви к своему делу, нам удалось создать ту самую экосистему полезных, эффективных и, главное, простых медицинских сервисов для управления здоровьем компаний и людей. Чего мы достигли: ⚡️ За семь лет мы вышли на второе место по числу клиентов среди insurtech компаний в Европе. Сегодня с нами уже 100 000+ застрахованных пользователей (40 000 из них мы подключили в 2021 году), а в списке клиентов — Мегафон, Aviasales, МТС Банк, Нетология, InDriver, Эвотор, Ivi.ru, Ostrovok.ru, VC.ru и еще 140 компаний. ⚡️ Создали экосистему медицинских сервисов, которая включает: сервис онлайн-поддержки 24/7, сервис онлайн записи в оффлайн клинику, своя виртуальная клиника, сервис второго мнения специалиста, чекапы, сервис поддержки психологов и др. Наша следующая цель — сделать все продукты, входящие в экосистему BestDoctor, мультисервисными, разработать подбор индивидуальных программ, создать возможность управлять бюджетом, улучшить HR-кабинет и умную маршрутизацию каждого сотрудника для B2B сегмента. И для этого ищем лучших экспертов, чтобы вместе захватить рынок. Подробнее о сервисе BestDoctor: https://bestdoctor.ru/ и https://hh.ru/article/29681 О проекте: За время существования компании мы накопили много данных и разных инструментов аналитики. На этих данных мы строим предложения для новых клиентов и продлеваем старых, проводим переговоры с клиниками и непосредственно помогаем нашим пациентам. Главная задача - весь этот информационный поток перенести в чётко организованную систему сбора, обработки и анализа данных любого объёма. Сейчас мы планируем вести сборку data lake house на базе GreenPlum, куда будут сливаться данные всех источников, таких как, PostgreSQL, Yandex,Google drive, сторонние API и др.). Мы мигрируем туда с Postgres+Astroniomer. Также у нас будет большой проект с фичастором и MLFLOW. В твоих задачах будет много архитектуры и хорошего продакшн кода, перенос, рефакторинг старого и написание очень динамического и автоматизированого нового, а также опыт с очень крутым датасайнсом, аналитикой и продуктом. В целом, тебе предстоит:  Мигрировать даги с Airflow c Астрономера на Кубер; Развивать и оптимизировать GreenPlum(PXF) даги; Пилить Data Managment нового поколения; Интегрировать сторонние API.  Что для нас важно:  Опыт программирования на Python 3; Опыт работы с Airflow; Отличные знания и опыт работы с SQL. Опыт работы с GreenPlum и PostgreSQL;  Дополнительным плюсом будет:  Опыт работы с Apache Kafka; Навыки в DevOps / опыт работы с Docker; Опыт работы с Apache Spark; Опыт работы с хранилищами (DataVault Anchor LakeHouse FeatureStore)  Как мы нанимаем: Мы готовы оперативно выходить с оффером, если понимаем, что подходим друг другу.  1 этап - телефонное интервью с HR (15-20 минут); 2 этап - техническое интервью с лидом аналитики; 3 этап - финальное интервью с CDO и HR.  Почему с нами круто:  Мы меняем рынок медицинского страхования, и у нас это отлично получается; В нас поверили и проинвестировали топовые венчурные фонды в России: российский Winter Capital, шведский VNV Global и австрийская страховая компания Uniqa; Удаленный формат работы (будем рады тебя видеть у нас в офисе (м.Савёловская); У нас гибкий график работы, который подойдет как жаворонку, так и сове; Тебя будет окружать команда талантливых и мотивированных людей; Мы предоставим тебе ноутбук и всю необходимую технику, которая позволит эффективно и комфортно работать; Медицинское обслуживание через систему BestDoctor.  ❤️ Ты будешь частью большого социально значимого дела. Мы реализуем амбициозную задачу — меняем рынок здравоохранения, действительно помогаем людям, и у нас это отлично получается. И у тебя получится!"
"69168183","КОРУС Консалтинг","Senior Data Engineer (Greenplum Developer)","False","None","None","Нет опыта","Полный день","['PostgreSQL', 'SQL', 'ORACLE', 'ETL', 'Greenplum', 'DWH', 'AirFlow', 'Sas']","Мы КОРУС Консалтинг – российская ИТ-компания, признанный лидер в автоматизации ритейла, производства, логистики, финансов и нефтегаза. За это время мы сделали более 1200 масштабных проектов для ведущих компаний России: Bonava, DPD, Tele2, Азбука Вкуса, Газпром нефть, Дикси, Инвитро, Леруа Мерлен, Магнит, Мегафон, О’КЕЙ, Петрович, Росатом, Яндекс.Деньги и это только начало списка. Нас уже больше 1000 человек, а за плечами более 20 лет опыта. Наш Департамент аналитических решений (ДАР) погружен во все современные направления в области работы с данными: внедрение BI-систем и систем аналитической отчетности; проектирование хранилищ и витрин данных; разработка в области продвинутой аналитики и больших данных; применение прикладных продуктов с использованием Machine Learning; внедрение решений в области управлениям данными (DG / DQ); разработка методологии и стратегии работы с данными. О проекте: вам предстоит участвовать в масштабной миграции хранилища данных нашего заказчика (крупнейшей транспортной компании) на open-source стек. Хранилище строилось на технологии SAS, а теперь переводим его на актуальный Greenplum. Стартуем проект в ближайшие сроки. Т.к. задач становится все больше, мы усиливаем команду и находимся в поиске Greenplum Разработчика. Что делать:  Проектировать и реализовывать ETL-слои для информационных потоков. Проектировать и реализовывать слои детальных данных. Реализовывать расчетные показатели. Проектировать и реализовывать модели данных. Создавать витрины для передачи данных в другие системы. Писать новый и поддерживать имеющийся код процедур и структуры баз данных. Реализовывать оптимальные решения по загрузке, преобразованию и хранению данных.  Чего мы ждем:  Опыт работы с СУБД PostgreSQL (от 3-х лет). Опыт работы с GreenPlum (от 1 года).  Будет здорово, если есть:  Умение проектировать и реализовывать процедуры и функции взаимодействия с Airflow.  Что можем предложить:  Заработная плата обсуждается по итогам тех. собеседования. Знаем рынок, готовы обсуждать индивидуально. Оформляем официально в штат компании.  Гибкое начало рабочего дня.  Удаленный формат работы. Очные встречи командами или работа в офисе (Москва, Санкт-Петербург, Ярославль) по желанию. Командировки для сотрудников из других городов по желанию. Индивидуальный план развития: подбираем задачи для роста экспертизы, выделяем бюджет на обучение, предоставляем курсы и программы обучения (в т.ч. английскому языку), предоставляем обратную связь, регулярно обсуждаем развитие.  Постоянный профессиональный и личный рост, самостоятельность в работе. Как одно из направлений развития мы предлагаем рост от разработчика до ведущего разработчика, далее тим-лид /архитектор;   Открытая корпоративная культура, минимум бюрократии и отчётов, а совещания только по необходимости и с чёткой программой.   Корпоративный университет, наставничество и развитие, полис ДМС, корпоративные тарифы на фитнес.   Приходи к нам в команду, будем вместе создавать крутые и полезные вещи!"
"68672453","Строительный Двор","Data Engineer (middle)","True","100000","220000","От 1 года до 3 лет","Полный день","['Python', 'SQL', 'Sas', 'Linux', 'PostgreSQL', 'ETL', 'DWH']","Ищем специалиста в области Data Engineering для решения задач в области DIY ритейла, е-commerce для крупной компании с развитой сетью филиалов. Среди задач – участие в построении архитектуры и развертывании новой аналитической платформы, разработка и оптимизация ETL процессов, построение витрин данных, подготовка данных для моделей машинного обучения во взаимодействии с командой Data Science, создание отчетов и дашбордов для бизнес-пользователей и др. Приветствуются умение работать в команде, стрессоустойчивость, умение объяснить полученные результаты, желание развиваться в различных аспектах Data Engineering: от администрирования сервисов, разработке ETL пайплайнов до построения витрин данных и отчетов. Обязанности:  Анализ исходных данных в различных системах для решения бизнес-задач (оценка структуры, качества, полноты и применимости данных) Разработка комплексных ETL процессов для разнородных систем и СУБД Мониторинг, оптимизация и поддержка ETL процессов Обеспечение и контроль качества данных Построение и развитие витрин данных Участие в разработке аналитической отчетности Презентация результатов работы бизнесу  Требования:  Высшее техническое образование Владение языками программирования Python или SAS Base Уверенное знание SQL (Join`ы, агрегаты, группировки, вложенные запросы, аналитические функции); Знание основ дискретной математики и теории реляционных СУБД Понимание принципов аналитической обработки данных в хранилищах данных Аналитический склад ума, системное мышление, ответственность, хорошие коммуникационные навыки Желание быстро овладевать новыми навыками и знаниями  Будет плюсом:  Наличие выполненных проектов в области Big Data, BI Опыт работы с СУБД; Понимание процессов ETL, ELT; Понимание общей архитектуры хранилищ данных (DWH) наличие e-learning сертификатов (Coursera, Udemy, Edx) о прохождении обучения по технологиям data engineering  Стек используемых технологий:  SAS Viya, SAS Base, HP Vertica, SAP, MS SQL Server, PostgreSQL, Airflow, Python, Linux/Bash, Hbase, Kafka, Prometheus, Grafana   "
"69955626","СБЕР","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['Greenplum', 'Hadoop', 'Hive', 'Spark', 'Big Data', 'Kafka', 'Airflow', 'Informatica', 'Python', 'Grafana', 'Qlik Sense']","Sber CIB – инвестиционно-банковский бизнес Сбера, крупнейший инвестиционный банк в стране. Мы входим в топ-1 инвестиционных банков в Центральной и Восточной Европе, топ-1 брокерских домов в России. Сбер входит в топ-3 по силе мировых брендов, топ-15 банков в мире по капитализации. Наша деятельность охватывает весь крупнейший бизнес страны. Каждый третий корпоративный кредит выдается нами. Проекты в команде трансформации Sber CIB – это внедрение изменений в работу с ключевыми клиентами. В список клиентов входят крупнейшие нефтегазовые, строительные, финансовые и не только корпорации РФ, которые формируют экономику страны. Мы работаем по всем продуктовым решениям Сбера: банковским, инвестиционным и экосистемным. У вас будет возможность прикоснуться к реальному бизнесу, изучить процессы по инвестициям и финансированию и сделать уникальные для этого бизнес-сегмента digital решения. Мы ищем человека, который будет участвовать в разработке одного из стратегических направлений трансформации Sber CIB. Технологический стек Greenplum, Hadoop, Hive, Spark, Kafka, Airflow, Informatica, Python, Grafana, Qlik Sense, ML/DL Libraries Что делать:  Разрабатывать и оптимизировать Data Warehouse (Greenplum, Hadoop) с использованием техник моделирования Data Vault и Star/Snowflake Scheme Разрабатывать и оптимизировать ETL/ELT потоки загрузки данных из различных источников данных (Hadoop, Greenplum, flat files, kafka, REST API, etc) Настраивать devops конвейеры и оптимизировать/автоматизировать существующие процессы разработки Писать авто-тесты и регресс тесты  Необходимые навыки:  Понимание теории баз данных (основные понятия, уровни изоляции транзакций, нормализация и т.д) Уверенное знание SQL и его расширений (T-SQL, PL/SQL etc) Понимание принципов разработки и оптимизации в одной из промышленных СУБД (Oracle, Teradata, SQL Server, Greenplum, и т.д.) Понимание принципов моделирования Data Warehouse и Data Marts (Dimensional modeling, Data Vault, etc) Способность разобраться в чужом коде (SQL, T-SQL, PL/SQL, Python, bash) Опыт разработки ETL/ELT конвейеров данных, забор данных по API, разбор не структурированных и слабоструктурированных данных Опыт работы в *nix окружении, умение применять базовые команды в терминале Опыт работы с системами контроля версий Умение работать в команде, time-management, ответственность и самостоятельность Технический английский (желательно)  Будет плюсом  Опыт оптимизации хранения и обработки данных в Greenplum Опыт реализации Лямбда-архитектуры, streaming процессов по загрузке данных Хорошее знание git, понимание существующих стратегий ветвления и способность подобрать нужную стратегию под конкретную задачу. Опыт разработки на python/java с применением парадигмы ООП Опыт разработки фреймворков и утилит для разработчиков Опыт написания unit-тестов и регресс тестирования Опыт использования практик CI/CD и автоматизации процессов разработки Опыт использования инструментов моделирования – SAP PowerDesigner, Erwin, etc.  Мы предлагаем:  Работа в крупнейшей IT-компании России; Офис, спортзал, парковка – всё в одном месте (Вавилова 19); Полностью белая з/пл: оклад + годовое премирование; Трудоустройство согласно ТК РФ; Регулярное корпоративное обучение; ДМС с первого дня работы; Материальная помощь и социальная поддержка, корпоративная пенсионная программа; Льготные условия кредитования и ипотеки; Доступ к различным курсам обучения по развитию как hard, так и soft skills. "
"67473425","СБЕР","Data-инженер\Big data developer","False","None","None","От 3 до 6 лет","Полный день","[]","Привет!Мы - команда разработки Сбер ID и мы сейчас в поисках опытного data-инженера\big data developer.Коротко про продукт: Сбер ID единый идентификатор пользователя в экосистеме Сбера. Более 100 сервисов экосистемы Сбера уже подключили функцию быстрой и безопасной авторизации по СберID. Количество активных пользователей превысило 30 млн человек.Наша гордость - это наша команда и нам действительно важно, чтобы атмосфера в команде была максимально прозрачной, доверительной и открытой. Нас уже более 130 и мы пришли из продуктовых компаний, а наш ориентир - лучшие практики Google, Яндекс и Amazon.Наша миссия простыми словами: дать пользователям максимально удобный, безопасный и незаметный способ входа пользователю вне зависимости от того, каким продуктом он хочет воспользоваться. Что будешь делать: участвовать в создании BI-системы: сбор, трансформация и анализ данных настраивать дашборды и алерты по продуктовым метрикам проектировать и разрабатывать витрины и хранилища данных создавать ETL-процессы улучшать быстродействие (работа с витринами, СУБД, изменение логики запросов) отвечать за хранение, выгрузку и миграцию данных Что ждем от тебя: опыт в направлении от 3х лет опыт работы hadoop сильные компетенции в области реляционных СУБД и хранилищ данных уверенное знание SQL: сложные запросы, аналитически функции, понимание физической реализации join’ов, оптимизация производительности запросов знание одного или нескольких ETL-инструментов: Informatica, MS SSIS, SAS, ODI понимание принципов организации хранилищ данных, подходов к проектированию логической и физической моделей, понимание основной проблематики хранилищ и подходов к решению Soft skills: не стесняешься спрашивать, умеешь и любишь вникать в суть умеешь отстаивать свою точку зрения, тебе не претит «пинговать» коллег готов(а) осваивать новые инструменты разработки и языки программирования, самообучаемость Наш стек: Bigdata: Hadoop, Hive, Impala, Spark, Scala/Java СУБД: Teradata, Greenplum ETL: Informatica, Golden Gate BI: Qlik Мы предлагаем: работа в офисе стабильный оклад и социальную поддержку сотрудников расширенный ДМС с первого дня и льготное страхование для близких корпоративное обучение за счет компании бесплатную подписку СберПрайм+ и скидки на продукты компаний-партнеров"
"69890072","СБЕР","Data engineer","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'Big Data', 'SQL', 'MongoDB', 'hadoop']","SberDevices - новое направление компании, которое занимается созданием девайсов для массового пользователя и продуктов на основе речевых и голосовых технологий и многими другими интересными проектами.Чем предстоит заниматься:  Участие в создании инфраструктуры управления корпоративными данными. Разработка процедур потоковой и пакетной загрузки данных из различных типов внешних источников:   Сырые данные по телеметрии устройств; Сырые данные по активности пользователей; Структурированные источники данных;   Разработка процедур обработки и анализа данных. Разработка процедур доставки обработанных данных в внешние системы. Разработка процедур контроля качества данных.  Профессиональные навыки:  Практический опыт работы со стеком технологий Big Data (Hadoop, Spark/Hive/Hue). Практический опыт работы с технологиями баз данных (Postgres, MongoDB, Cassandra). Практический опыт работы с облачными платформами управления данными (Yandex.Cloud, AWS, Mail.ru Cloud Solutions). Практический опыт участия в проектах по созданию DWH, Data lake, Data management platforms. Практический опыт разработки и оптимизации запросов к данным на базе SQL, Python. Практический опыт построения и развития высоконагруженных систем.  Что предлагаем:  Работа с крупнейшими массивами данных на рынке России. Амбициозный и дружный коллектив. Профессиональное обучение, семинары, тренинги, конференции, корпоративная библиотека. ДМС, страхование жизни. Самые инновационные, амбициозные проекты и задачи. Свободный дресс-код. Гибкий график для оптимального баланса работы и личной жизни. Льготные кредиты и корпоративные скидки. "
"69752639","СБЕР","Data engineer","False","None","None","От 1 года до 3 лет","Полный день","[]","Блок «Розничный бизнес» Сбера расширяет команду создания Дата-платформы. Мы создаем инструменты и базовые витрины данных для всего Розничного блока на различных платформах (Hadoop, Greenplum, Teradata, Kafka), работаем с огромными объёмами данных (десятки и сотни Тб) Перед нами стоят очень амбициозные задачи, и мы активно расширяемся. Сейчас мы ищем в команду человека, которому предстоит проектировать и разрабатывать решения на платформе MPP СУБД Greenplum Обязанности · Проектирование и разработка аналитических витрин данных для на базе СУБД Greenplum и вывод в промышленную эксплуатацию · Реализация потоков поставки данных для потребителей · Консалтинг пользователей (DS/DA/DE) по вопросам производительности и особенностей работы СУБД Greenplum · Разработка ETL-процессов по преобразованию и загрузке данных · Мониторинг и оптимизация процессов загрузки, преобразования данных и сборки витрин · Создание инструментов для автоматизации рутинных задач, связанных с обработкой данных · Создание инструментов для мониторинга и управления производительностью Greenplum · Разработка и внедрение инструментов для расширения возможностей платформы Greenplum · Разработка и поддержка сопроводительной документации и спецификаций данных, развитие и поддержка базы знаний Требования · Высшее техническое образование · Опыт работы не менее 2 лет в качестве Data Engineer / Data Analyst / ETL Developer · Опыт работы с linux-системами · Знание SQL на экспертном уровне (аналитические функции, подзапросы, хранимые процедуры, оптимизация запросов) · Знание архитектуры МРР СУБД (Greenplum, Teradata, Exadata), опыт практической работы c MPP СУБД · Опыт разработки ETL-процессов, построения хранилищ и витрин данных · Английский язык на уровне свободного чтения технической документации · Желание и возможность разбираться в сложных проблемах Будет плюсом: · Опыт практической работы с СУБД Greenplum · Знание Python/Java/Scala · Опыт работы с Informatica Power Center Условия Работа в Сбере – это: • стабильный оклад и социальная поддержка сотрудников • расширенный ДМС с первого дня и льготное страхование для близких • работа в лучшем офисе страны по версии Best Office Awards 2022 • регулярное корпоративное и внешнее обучение • спортзал прямо в офисе • бесплатная подписка СберПрайм+ и скидки на продукты компаний-партнеров • корпоративная пенсионная программа • ипотека выгоднее на 4% для каждого сотрудника."
"69658562","Mountain Ridge","Data Engineer","True","261000","None","От 3 до 6 лет","Полный день","['Python', 'SQL', 'PostgreSQL', 'ClickHouse', 'Аналитическое мышление']","Наш клиент – команда профессионалов с большим опытом сопровождения онлайн-продавцов. Мы регулярно проходим сертификацию и обладаем самыми актуальными знаниями инавыками, которые помогают начать успешно продавать даже новичку! С 2018 года мы вывели более 3000 компаний на маркетплейсы и входим в топ-3 компаний рынка. В августе 2021 мы запустили онлайн-платформу, которая значительно облегчает самостоятельную работу тысячам продавцов на маркетплейсах. Платформа интегрирована со всеми основными маркетплейсами и агрегирует в себе данные о миллионах продаж на различных площадках. В скором времени мы предоставим возможность нашим клиентам получать данные не только о собственных продажах но и сравнивать себя с конкурентами, а также получать рекомендации по развитию продаж внутри маркетплейсов. GOLD партнеро Ozon и призеры Marketplace Conf Award 2022 в номинации Операторы. Мы ищем Data Engineer в свою команду! Что будешь делать :  Разработка новых сервисов по аналитической отчетности и визуализации данных; Выстраивание системы сбора, предобработки и хранения данных; Участие в формировании и оценке требований к новым проектам; Взаимодействие с аналитиками и разработчиками; Построение новых и оптимизация существующих EТL-процессов; Написание хранимых процедур, триггеров, функций; Написание и оптимизация запросов, формирование выгрузок; Взаимодействие с другими подразделениями по вопросам обработки данных  Что мы ждем от тебя :  Опыт работы с ClickHouse - понимание возможностей и архитектуры ClickHouse - важно! Опыт работы с PostgreSQL Базовое понимание работы с Kafka Продвинутый уровень SQL(виды джойнов, подзапросы, аналитические функции) Опыт разработки SQL-запросов, процедур и функций Большим плюсом будет знание Python Желание и возможность разбираться в сложных проблемах.  Что мы предлагаем : ● Оформление по ТК РФ с первого дня работы; ● График работы 5/2, гибкий режим работы: с 9-00 до 18-00 или с 10-00 до 19-00; ● Мы исключительно за очное общение в команде, но готовы обсуждать возможность гибридной и удаленной работы; ● Интересные и амбициозные задачи формулируемые в полном соответствии со SMART-идеологией; ● Работа в команде практиков e-com рынка, широкие возможности для роста и развития; ● Гибкое начало рабочего дня; ● Приятный и комфортный офис в Москва-Сити в шаговой доступности от метро, отличный вид из окна, вкусняшки : шоколадки, сушки и божественный кофе в неограниченном количестве.  "
"70205435","ЕвроХим, Минерально-Химическая Компания","Senior Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Python', 'SQL', 'ETL', 'MS PowerPoint', 'Spark', 'Airflow', 'DWH', 'Kafka']","В направлении R&amp;D компании АО «МХК «ЕвроХим» открыта вакансия &quot;Senior Data Engineer&quot;. Основная задача данной позиции - Участие в разработке платформы обработки и анализа больших данных. Глобальная цель направления – реализация полномасштабной программы по внедрению на предприятиях группы компаний лучших научных, технических и цифровых решений и их взаимная интеграция. Обязанности:  Разработка целевых потоков данных в рамках «Платформы обработки и анализа данных»; Проработка интеграции с ключевыми системами источниками; Автоматизация процесса создания и конфигурирования ELT процессов; Документация процесса создания data pipelins для продуктовых команд; Участие в проработке архитектуры решений «Платформы обработки и анализа данных».  Требования:  Отличное знание SQL, python; Опыт выстраивания ELT/ETL процессов в DWH или Data Lake; Понимание agile и gitops практик; Опыт работы с airflow, kafka, spark, k8s; Будет плюсом опыт работы с greenplum, dbt.  Условия:  Оформление в соответствии с нормами ТК РФ, социальные гарантии; Крупные проекты в компании–лидере отрасли; Возможность модернизировать процессы реального производственного бизнеса в команде сильнейших экспертов; Полис ДМС (включая стоматологию); Удаленная работа; Доступ к корпоративной OnLine библиотеке; Обучение в программах Корпоративного университета; Корпоративный спорт, конференции, культурные мероприятия. "
"45498560","Спортмастер","Middle Data / ML Engineer (Персонализация)","False","None","None","От 1 года до 3 лет","Удаленная работа","['Python', 'SQL', 'ORACLE', 'Oracle Pl/SQL']","В Департамент управления данными, в связи с расширением инженерной команды, ищем middle специалиста на позицию Data / ML Engineer. Наш департамент проектирует и создает data сервисы, которые помогают оптимизировать взаимодействие с клиентом или внутренние процессы компании. Примеры проектов: рекомендательная система, NLU модуль диалоговой системы, прогнозирование продаж, поисковый движок и пр.Чем предстоит заниматься:  Разрабатывать, оптимизировать и поддерживать пайплайны обработки данных и машинного обучения; Реализовывать высокопроизводительные процедуры расчета витрин признаков; Сопровождать data science специалистов в продуктовых командах.  Наш стек:  Для разработки: Python, Oracle PL/SQL, Spark, Spark Streaming, Impala/Hive, Kafka, Airflow. Для организации работы: Jira, Confluence, BitBucket.  Мы ждём от будущих коллег:  Высшее техническое образование; Навыки написания сложных SQL запросов и их оптимизации; Опыт разработки и автоматизации пайплайнов обработки данных; Уверенное владение Python (структуры данных, пакеты для обработки данных); Умение работать с системами контроля версий (BitBucket/ Github/ Gitlab); Продвинутый пользователь Linux; Способность работать самостоятельно, доводить начатое до конца; Развитые problem solving skills, soft skills; "
"69775217","МТС","Data engineer в МТС Маркетолог (Big Data)","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'Spark', 'Hadoop', 'Big Data', 'SQL']","Big Data МТС – место, где телеком данные превращаются в реально работающие IT-продукты. Мы создали и протестировали несколько десятков сервисов. Самые успешные из них уже стали частью экосистемы МТС. Например, МТС Маркетолог, рекомендации в KION (МТС ТВ), услуга “Кто звонит?” или Спам blacklist. Кого мы ищем? Обязательно:  spark – уметь писать с его помощью загрузки и трансформы python – уметь с его помощью общаться со спарком, создавать spark-приложения SQL – уметь на более-менее приличном уровне ворочать данные + учитывать физику хранения данных самостоятельность (уметь эффективно работать без постоянного контроля и микроменеджмента)  Что предстоит делать?  написание spark приложений (ETL, подготовка таблиц/витрин, сбор сегментов) + смежные задачи такие как деплой, тесты, dq и т.п. при необходимости интеграция со всякими бэкендами, Kafka, HBase и много чем еще развитие python-библиотек и инструментов используемых командой разработки, так и пользовательских (для аналитиков) пилить задачи из бэклога на спарке проявлять инициативу в плане где что и как можно улучшить, отстаивать свои идеи, воплощать их в жизнь  Сейчас мы ищем Data Engineer на продукт МТС Маркетолог Мы являемся частью большого продукта МТС Маркетолог – рекламного сервиса, использующего всю мощь BigData для проведения точных и эффективных рекламных кампаний, построения аналитики и исследований. В BigData мы сконцентрированы на создании внутренних сервисов и инструментов, ориентированных на работу с данными; Среди таких сервисов:  Сервис cookie matching – для разметки пользователей в интернете и реализации рекламных механик, основанных на событиях с сайтов Self-service портал для сегментации абонентов на основе данных BigData (как пользовательский интерфейс и программный через REST API) Единая платформа сегментации, включающая:   Сервис оркестрации сбора сегментов, их постановки на регламентный расчет и доставки в целевые системы Сервис контроля качества и мониторинга всех процессов платформы Сервис аналитики по данным из каналов о проведенных РК (отчеты, модели и т.п.);  Также мы разрабатываем внутренние инструменты для работы с большими данными в экосистеме Hadoop/Spark для пользователей в BigData. Мы уделяем значительное влияние применению data science в задачах повышения качества рекламных коммуникаций – модели для более точного таргетинга, оптимизации рассылок и адаптивных разметок (сайтов, приложений и т.п.) Что вы найдете в команде Big Data? Команда: в команде Data engineer сейчас 30 человек (во всей Big Data МТС более 300 человек). Все Data инженеры разработчики поделены на группы со своими лидами. Каждую неделю мы обмениваемся опытом на совместных синках. Data инженеры работают в продуктах со своей автономной командой, в которой есть все роли: аналитики, DS, разработчики, девопсы, менеджеры продукта. Условия: каждый месяц - аванс и зарплата, дважды в год - премия. ДМС + стоматология, корпоративная связь, специальные предложения от партнеров и друзей МТС, отпуск 31 день в год. Выдаем 16” MacBook Pro или Dell на выбор. Есть ли обучение?  Конференции, митапы. Корпоративный университет МТС и масштабная виртуальная библиотека. А ещё мы регулярно обмениваемся опытом на совместных синках с лидами экспертизы  Какой график? Гибкое начало рабочего дня в промежутке с 8 до 11. Есть возможность работать несколько дней вне офиса по договоренности с командой. Сколько этапов при отборе? Не более трех:  HR + первое тех. интервью с лидом направления Тестовое задание/второе интервью - по необходимости Собеседование с PO и командой, выбор кандидатом проекта "
"70049724","МТС","Ведущий DevOps-инженер (Data Engineer)","False","None","None","От 3 до 6 лет","Полный день","[]","В дочерней компании ПАО МТС – ООО «Прикладная техника» (IT-компания) открыта вакансия ведущего DevOps-инженера (Data Engineer), специализирующегося на работе с данными на проект, где активно используется технология BigData для потоковой обработки информации. Здесь есть потоки данных со скоростью 100 Гбит/с, петабайты информации, большие системы хранения данных, сложная аналитика в области сотовой связи, множество протоколов. Также у нас есть интеграционный проект, где можно познакомиться с распознаванием текста, генерацией отчетности, протоколами и алгоритмами взаимодействия со множеством систем ландшафта МТС.Чем предстоит заниматься: тонкой настройкой:  Tez; Oozie; Hive; Aerospike; HBase; Zookeeper; Kafka; Storm; Spark; Superset; YARN; HDFS/MapReduce; Ambari (конфиги).  Что мы предлагаем:  Стабильная белая заработная плата, премии 2 раза в год; Возможность профессионального развития в разных направлениях компании: неограниченный доступ к материалам одного из лучших корпоративных университетов в стране (вебинары, книги, курсы); Возможность реализации инициатив и даже собственной бизнес-идеи (идеи по автоматизации внутренних процессов можно реализовать в программе iDA); Работу в активной команде: работа в центре инфраструктуры и качества, это не только личные KPI и результат. Можно делиться знаниями и получать экспертизу от коллег; Расширенный социальный пакет (ДМС со стоматологией с момента трудоустройства, страхование жизни, страхование при поездках за рубеж и многое другое). Хороший отдых — отпуск 28 календарных дней + 3 календарных дня дополнительно ежегодно, а также различные скидки на путешествия от наших партнеров; Компенсация сотовой связи; Комфортный график с гибким началом/окончанием рабочего дня, возможность выбора формата работы (офис/гибрид/удаленная). "
"69698680","СБЕР","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'SCALA', 'PostgreSQL', 'Python', 'Java']","Обязанности:  Знание одного из языков на уровне mid-dev: Java, Scala, Python, Go; Глубокое знание SQL (на уровне написания сложных запросов с использованием аналитических функций, желателен опыт оптимизации); Опыт работы c: ETL инструментами: NiFi, Informatica PC/BDM, Ab Initio, Airflow; РСУБД: PostgreSQL, Oracle, Teradata; Будет плюсом умение работать с индексированным поиском Apache Lucene (elasticsearch, solr).  Требования:  Проектирование и разработка ETL-процессов; Загрузка данных из различных источников (потоковая и пакетная); Формирование staging и detailed слоёв.  Условия:  Высокий уровень неформального общения несмотря на смешанный режим работы; Низкий уровень стресса – стабильная команда; Предоставление необходимого количества свободы в работе. "
"54575041","ИЦ АЙ-ТЕКО","Middle Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['SQL', 'Java', 'MS SQL', 'XML', 'Spark', 'Spark Streaming', 'RDD', 'Dataframe', 'Spark SQL']","КОМПАНИЯ ООО ИЦ «АЙ-ТЕКО» - ведущий российский системный интегратор и поставщик информационных технологий для корпоративных заказчиков. Активно действует на рынке IT России с 1997 года, входит в ТОП-400 крупнейших российских компаний, ТОП-10 крупнейших IT-компаний России. В СВЯЗИ С АКТИВНЫМ РАЗВИТИЕМ ВНУТРЕННИХ ПРОЕКТОВ В КОМПАНИИ ОТКРЫТА ВАКАНСИЯ  Data Engineer     Обязанности: * разработка кода в соответствии с требованиями к предоставлению данных подотчетной предметной области единого семантического слоя; * ревью и оптимизация ранее разработанного (/автосгенерированного) кода; * разработка и прогон автотестов на разработанный/доработанный код; * подготовка дистрибутивов и описания в соответствии с требованиями оформления Банка; * участие в проектировании логической модели данных; * оценка и внедрение новых технологий и подходов разработки;   Требования: * высшее образование; * обязательно хорошее знание SQL, написание сложных запросов; * знание scala/java или желание выучить; * опыт разработки на scala spark; * знание классических алгоритмов и структур данных; * понимание внутренней архитектуры hadoop и spark; * будет плюсом знание банковской области, корпоративных финансов, бухгалтерии, работы с балансовыми данными.     МЫ ХОТИМ, ЧТОБЫ КАЖДЫЙ СОТРУДНИК БЫЛ ДОВОЛЕН СВОЕЙ РАБОТОЙ, ПОЭТОМУ МЫ ПРЕДЛАГАЕМ:- Работу в стабильной компании, белую заработную плату- График работы 5/2, гибкое утро- Социальный пакет (медицинская страховка, включая стоматологию, собственная столовая)- Корпоративный спорт- Работу в команде, использующей гибкий подход к разработке- Оформление в соответствии с ТК РФ с первого дня работы    "
"67996146","Спортмастер","Data engineer","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'Kafka', 'Spark']","Мы находимся в поиске Data engineer&#39;a, который будет заниматься развитием направлений : Цифровая аналитика &quot;Антифрод&quot;, &quot;Инфраструктура DataLake&quot;. Ваши задачи:   Реализация ETL в Hadoop (с помощью Airflow).   Работа с различными источниками данных: Oracle MS SQL API личных кабинетов, микросервисы.   Батч и стримы с помощью PySpark и Kafka.   Подготовка витрин для анализа (Hive + Spark+ SQL)   Наш стек: Cloudera hadoop; Kafka, Spark, Airflow; Jira; Confluence; GitLab Мы ждем от будущих коллег:  Уверенное владение Python. Опыт использования эко-системы Hadoop: HDFS, Apache AirFlow, Hive, Kafka,Spark. Знание SQL Опыт работы с реляционными базами данных (Oracle). "
"49943177","Мегаполис","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['Oracle Pl/SQL', 'Python', 'SQL', 'DWH']","МЕГАПОЛИС – группа компаний, владеющая и управляющая активами в сфере логистики и дистрибуции товаров повседневного спроса (FMCG): табачная и пивобезалкогольная продукция, бакалейные товары, зажигалки, элементы питания, контрацептивы, энергетики, OTP (сигары, сигариллы, табаки).  На рынке с 1998г.. 230 филиалов от Калининграда до Петропавловска-Камчатского. Более 12 000 человек сотрудников. Доставка товаров в более чем в 160 000 торговых точек по всей России. 6 место в рейтинге Forbes в номинации «200 крупнейших частных компаний России»  ГК МЕГАПОЛИС ОБЛАДАЕТ КРУПНЕЙШИМ ПОРТФЕЛЕМ КОНТРАКТОВ, ВКЛЮЧАЯ КОНТРАКТЫ НА ДИСТРИБУЦИЮ В РФ:  Табачной продукции с производителями Japan Tobacco International, Philip Morris International, Imperial Tobacco Group; Долгосрочный дистрибьюторский договор с производителем пивобезалкогольной продукции ОАО «Пивоваренная компания «Балтика» (Carlsberg Group); Дистрибьюторские контракты с производителями чая и кофе – Dilmah Teas (ТМ Dilmah) и кофе Lavazza; Кроме этого, владеем лицензией на производство и продвижение на территории РФ и СНГ продукции компании Jacobs Douwe Egbert’s Master Blenders – под маркой кофе Moccona. Дистрибьюторский контракт с производителем энергетических напитков Red Bull GmbH (Red Bull, Bullit).   На данный момент в компании открыта позиция &quot;МЕНЕДЖЕР ПРОЕКТОВ&quot;. ЗАДАЧИ: Ищем специалиста, который сможет выстроить архитектуру базы данных (ORACLE) и усовершенствовать внутреннюю аналитическую CRM систему.  Разработка и поддержка внутренних витрин данных; Написание SQL запросов, функций, пакетов, ETL процедур; Развитие backend; Настройка пайплайнов для загрузки и трансформации данных Выявления проблем в проектировании БД и участие в процессе их решения Выполнение разовых выгрузок из БД и ad-hoc аналитики Управление качеством данных  НАВЫКИ И СТЕК:  Релевантный опыт работы от 2х лет; Знание SQL, опыт работы с реляционными СУБД (Oracle, MySQL, PostgreSQL и пр.); Умение писать качественный код и тесты на PYTHON; Понимание принципов работы REST API; Опыт построения ETL-процессов, понимание как их протестировать и проверить целостность данных; Опыт проектирования DWH; Опыт работы с Git.  МЫ ПРЕДЛАГАЕМ:  Работу в крупнейшей компании – лидере в своей отрасли; Оформление по ТК РФ с первого рабочего дня; Удобный график работы ПН-ПТ; Конкурентную белую заработную плату, выплаты 2 раза в месяц на банковскую карту; ДМС, скидка на обеды, корпоративная SIM-карта; Комфортный офис в золотой башне Москва Сити (м. Выставочная), возможен гибридный формат работы.  P.S.: для кандидатов предусмотрено тестовое задание (Python, SQL)"
"69565899","EXNESS Global Limited","Data Engineer Data Office","True","250000","350000","От 3 до 6 лет","Полный день","['Python', 'SQL', 'Английский язык']","With over 1,600 employees of more than 90 nationalities, Exness is the place for global teamwork, incredible leadership, a learning culture, and constant development. Unlimited by time zones, Exnessians from around the world have worked seamlessly together since 2008 to provide our traders with the best possible trading experience. Today, we stand proud with over 300,000 active traders and 2.5 trillion USD in monthly trading volume. Your role at Exness We are looking for a Data Engineer in the Technology department in Limassol, Cyprus. We are looking for an experienced Data Engineer who will join our Technology Department to be part of a 350+ engineers team who creates cutting-edge solutions and constantly raise the bar. We offer ambitious projects and tasks for developing our data platform. In addition to that, you will be a part of our friendly and professional team. !!! This vacancy implies a mandatory relocation to Cyprus   Key Responsibilities  Assemble large, complex data sets that meet functional / non-functional business requirements (business cases); Prepare datasets from various sources by request. Take care of data warehousing as a process; Implement and support Data model changes Develop new integrations and Data marts Recommend ways to improve data reliability, efficiency, and quality; Describe entities and lineage in Data Catalog.  Technical skills  Strong knowledge of basic DWH design methodologies; Strong experience with databases like Vertica, PostgreSQL, ClickHouse, BigQuery, and similar. MPP databases experience; Expert structured query language (SQL) knowledge; Experience with Apache Airflow/prefect. Ability to write and use DAGs as a plus; Good skills in the programming language Python. Skala or Java as a plus  Soft Skills  Good communication and problem-solving skills; Spoken and written at least Intermediate English and fluent Russian; Detail-oriented and critical thinking; Decision-making and adapting to new changes; Write concise and clear documentation; Deal with constructive critics and know how to develop relationships with the team to achieve common goals; A good work ethic.  What we offer  An outstanding relocation package with the ability to start working remotely + a competitive salary based on your expectations, skillset and internal benchmark; New branded corporate Mini Cooper Countryman S for you and applicable to family members; Medical insurance for employees and family members; Company fitness centre for employees and their spouse, parking near the office or a bus permit; Kindergarten/school compensation program; The best view to the sea from our own roof bar and a flexibility to book your desk within several office locations; We support the relocation of our candidates and do our best to help them to be successful in the company.  Sounds like you? Apply."
"69667749","Ростелеком","Data Engineer","False","None","None","От 1 года до 3 лет","Удаленная работа","['Python', 'SQL', 'Hadoop', 'Linux', 'Hive']","DataOffice - это драйвер DataDriven культуры в Ростелекоме, один из лидеров отрасли управления данными в России. DataOffice объединяет 300 профессионалов в области ИТ-технологий. Мы ищем Data Engineer в команду DataOffice! Чем предстоит заниматься:   Анализ процессов, информационных систем и существующей отчетности компании с цельюпостроения новых и поддержки текущих потоков данных.   Написание пайплайнов и автоматизация ETL процессов.   Технологии:  Реляционные базы данных (GreenPlum, Oracle) Кластер Hadoop Python Airflow PySpark  Требования:  Продвинутый уровень SQL Хороший уровень Python Хороший уровень Spark (PySpark) Хороший уровень Airflow  Желательно:  Знание экосистемы Hadoop (Hive, Sqoop), Git, Docker Базовые знания bash, linux  Бизнес задачи, которые решает наше подразделение:  Разработка предиктивных моделей оттока и снижения лояльности клиентов. Разработка рекомендательных моделей по доп. продажам. Разработка поведенческой сегментации клиентов. Расчет эластичности оттока и дохода от клиента по характеристикам продукта и сервису. Интеллектуальная автоматизация процессов компании. Разработка цифровых двойников процессов компании и их интеллектуальная оптимизация. Разработка экспериментов для A/B-тестирования гипотез бизнеса.  Что предлагаем:  Для жителей Москвы - БЦ Академик, победивший в номинации лучший проект «Бизнес-центр класса А. Москва» Офисы во всех крупных городах Гибкий график Возможна полная или частичная удаленная работа Оформление по ТК Белая и конкурентоспособная зарплата ДМС, компенсация обучения и спорта Корпоративное обучение: внутренние митапы, участие в конференциях, доступ к корпоративным библиотекам технической и бизнес литературы "
"54975410","ИНВИТРО","Senior Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'ETL', 'HTML', '.NET Framework', 'Big Data', 'C#', 'DWH', 'BI', 'Linux']","Станьте частью чего-то большего - присоединяйтесь к команде Инвитро! Международная медицинская компания, специализирующаяся на высокоточной лабораторной диагностике и оказании медицинских услуг, представленная в 6 странах и являющаяся лидером на рынке лабораторной диагностики России, приглашает на работу Системного аналитика data warehouse. Чем предстоит заниматься:  Участие в проектах по разработке ПО в качестве программиста Доработка существующих бизнес приложений компании Работа в проектах интеграции Менторство (обучение, сопровождение) разработчиков начального уровня  Вам потребуется:  Опыт работы в команде разработки от 2 лет на проектах разработки программного обеспечения Опыт работы с высоконагруженными сервисами Хорошее знание .NET, C#, Понимание принципов ETL/ELT; Навык работы с BI системами; Участие в проектах (хотя бы в одном из направлений): хранилища данных\Data Warehouse (DWH), озеро данных (data lake); Высшее образование.  Мы предлагаем:  Оформление по ТК РФ с первого дня, испытательный срок 3 месяца &quot;Белая&quot; заработная плата + премии График работы: 5/2, с 9:00 до 18:00 Корпоративные скидки на услуги компании для вас и ваших родственников Социальный пакет по формату «кафетерий» (индивидуальный набор услуг в рамках закрепленной суммы в направлениях «обучение», «медицина», «спорт») Страхование жизни от несчастных случаев и критических заболеваний Дополнительные 3 дня к отпуску ежегодно "
"69705236","Промсвязьбанк","Data engineer","False","None","None","От 1 года до 3 лет","Полный день","['Java', 'SQL', 'ETL', 'Python']","Обязанности:  Разработка процессов загрузки данных из различных источников в Greenplum (RDBMS, WebService, API) Проектирование и разработка процессов преобразования данных (ETL) для хранилища данных и платформы BigData Проектирование и реализация модели интеграции с DWH Развитие платформы DataCollector Участие в проработке архитектуры загрузки, хранения и трансформации данных Развитие системы мониторинга ETL-процессов  Требования:  Опыт работы по направлению разработчик Python от 3 лет (middle/middle+) Разработка ETL на одном из инструментов (Airflow, Informatica, Talend, Pentaho, NiFi) Хорошее знание Python, SQL, Bash Знание основных принципов разработки и построения хранилищ данных Умение работать с инструментами CI/CD (git, gitlab ci/cd) Хорошее знание Postgres и Greenplum Знание стека Hadoop (Hive, Spark, HDFS, Kafka) Опыт работы с большими объемами данных приветствуется   Условия:  ​Гибкое начало и окончание рабочего дня, свободный дресс-код Релокационный пакет (компенсация расходов на покупку билетов и аренду жилья в течение первых трех месяцев) Официальное оформление в соответствии с ТК РФ Конкурентный уровень дохода: оклад + премии Медицинская страховка, страховка для выезжающих за границу Доплата к отпускному и больничному листу Дополнительные льготы при заключении брака и рождении детей Социальная поддержка при сложных жизненных ситуациях Льготное кредитование для сотрудников Обучение в корпоративном университете банка Корпоративная библиотека "
"70220676","КОРУС Консалтинг","Data Engineer (Microsoft Azure, Middle)","False","None","None","От 1 года до 3 лет","Полный день","['Английский язык', 'SQL', 'ETL', 'Azure', 'BI', 'AirFlow', 'Python', 'Spark', 'Hive', 'Hadoop']","Мы «КОРУС Консалтинг» - IT компания, предоставляем услуги по IT-консалтингу и автоматизации бизнес-процессов. Имена всех наших заказчиков тебе точно известны - это крупнейшие игроки на российском рынке. А нас на сегодняшний день уже больше 1000 человек. Наш Департамент аналитических решений (ДАР) погружен во все современные направления в области работы с данными: внедрение BI-систем и систем аналитической отчетности; проектирование хранилищ и витрин данных; разработка в области продвинутой аналитики и больших данных; применение прикладных продуктов с использованием Machine Learning; внедрение решений в области управлениям данными (Data Governance, Data Quality); разработка методологии и стратегии работы с данными. О проекте: мы разрабатываем облачные хранилища данных и аналитические решения на облачной платформе Microsoft Azure, помогаем нашему заказчику (иностранная ритейл компания) изменять бизнес-процессы путем использования аналитики. Собираем ключевые показатели деятельности и предоставляем эти показатели в виде красивых дашбордов высшему менеджменту заказчика. Наш текущий стек: MS Azure, Python, Spark, Hive, AirFlow как оркестратор. Ищем к себе в команду Middle/ Senior Data Engineer с опытом работы в Microsoft Azure. Основные задачи:  Анализ структуры источников данных; анализ бизнес-процессов заказчика. Участие в разработке Технического задания и других проектных документов. Разработка витрин/хранилищ данных. Разработка и оптимизация ETL процессов. Регулярное изучение Azure, новых технологий, сервисов, подходов с целью оптимизации архитектур, предложения новых.  В работе поможет:  Опыт работы с Microsoft Azure от 1 года. Опыт разработки на Python от 2 лет. Опыт разработки приложений с использованием инструментария экосистемы Hadoop (Spark Streaming, Hbase, Spark SQL, Kafka, Hive, Impala, Hue и т.д.) от1 года. Опыт работы с любым ETL инструментом от 1 года (Airflow в приоритете). Опыт написания запросов для анализа и преобразования данных (на любой реляционной СУБД). Опыт в подготовке витрин данных\кубов для аналитических отчетов.  Будет плюсом:  Опыт сбора требований от функциональных заказчиков.  Что мы можем предложить:  Отличную возможность получить опыт с Microsoft Azure и другими облачными решениями, предоставим обучение.  Официальное оформление в штат компании.   Заработная плата обсуждается по итогам тех. собеседования. Знаем рынок, готовы обсуждать индивидуально.   Гибкое начало рабочего дня, возможность посещения офиса для коллег из Москвы, Санкт-Петербурга и Ярославля. Полная удалёнка для коллег из остальных городов России.   Постоянный профессиональный и личный рост, самостоятельность в работе. Как одно из направлений развития мы предлагаем рост от разработчика до ведущего разработчика, далее тим-лид /архитектор;   Открытая корпоративная культура.   Минимум бюрократии и отчётов, а совещания только по необходимости и с чёткой программой.   Корпоративный университет, наставничество и развитие.   Полис ДМС, корпоративные тарифы на фитнес и занятия английским на ведущих платформах.   Приходи к нам в команду, будем вместе создавать крутые и полезные вещи!"
"68115103","СБЕР","Data Scientist/Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'Hadoop', 'Spark', 'SQL', 'Docker']","Работа в команде платформы рекомендательных систем — создаём продукты для пользователей SberDevices.Обязанности:  Разработка и внедрение приложений с использованием алгоритмов машинного обучения Разработка, построение и обслуживание инфраструктуры работы с данными Приведение неструктурированных данных из различных динамических источников к виду, необходимому для работы ML моделей Работа с корпоративными хранилищами данных и понимание принципов их построения  Требования:  Знание теории ML (классы задач, подходы, библиотеки, фреймворки) Опыт проектирования и разработки data pipelines, ETL-процессов, API Python 3 (async, SQL alchemy) Ignite Kafka SQL Docker  Будет плюсом:  Опыт работы с рекомендательными системами Spark Hadoop, HDFS OpenShiftУсловия: Самые инновационные, амбициозные проекты и задачи Профессиональное обучение, семинары, тренинги, конференции, корпоративная библиотека ДМС, страхование жизни Свободный дресс-код Гибкий график для оптимального баланса работы и личной жизни Льготные кредиты и корпоративные скидки Конкурентная компенсация (оклад и премии по результатам деятельности).     "
"69354887","Цифромед","Data Engineer (Frontend)","False","None","None","От 3 до 6 лет","Удаленная работа","['SQL', 'BPMN', 'Python', 'UML', 'PostgreSQL', 'Visiology', 'Polymatica', 'HTML5', 'JavaScript', 'ETL', 'NiFi', 'Data Analysis', 'Data Mining', 'Dashboards', 'BI', 'BigData', 'AI']","Обязанности:  Проектирование и разработка BI и визуализации данных. Всесторонняя поддержка бизнеса со стороны данных. Ведение ETL процессов в средних и верхних слоях данных. Построение роадмапа и непосредственное участие в процессе по обмену данными. Фронтенд разработка в BI инструментарии Visiology, Polymatica и HTML5/JS.  Требования:  Опыт проектной деятельности по Agile (SCRUM, Kanban). Стек технологий: JS, SQL, Python, nosql, Postgre, приветствуется знание оркестровки ETL Airflow/NiFi, Data Analysis, Data Mining, Dashboards, Java платформы, микросервисная архитектура, REST и SOAP, BPMN, UML, иные нотации и инструментальное ПО описания процессов, моделей и потоков данных Data Governance и Data Lineage, BI и OLAP инструментарий. Приветствуется опыт BigData и AI. Приветствуется опыт в информационной безопасности (ФСТЭК, ФСБ). Приветствуется опыт межведомственного и межсистемного взаимодействия (СМЭВ, ПГУ, Электронное правительство).  Условия:  Официальное трудоустройство в соответствии с ТК РФ. Конкурентная заработная плата и ежегодные бонусы. ДМС + стоматология. Гибкое начало рабочего дня и удаленный формат работы. "
"69771713","Uchi.ru","Data Engineer","False","None","None","От 3 до 6 лет","Удаленная работа","['SQL', 'Python', 'ETL']","Мы Учи.ру — одна из крупнейших российских EdTech-компаний с аудиторией порядка более 10 млн пользователей —- это ученики, их родители и учителя. Мы создаем уникальные образовательные онлайн-продукты, которые помогают в интерактивном формате изучать школьную программу и развивать софт-скиллы. Такая большая пользовательская база и трафик позволяют использовать честный data-driven подход при запуске новых направлений, а CustDev в Учи.ру является естественным этапом разработки продукта. В стенах Учи.ру собрана сильнейшая команда выпускников ведущих вузов России. Чтобы сделать нашу команду еще лучше, мы ищем Middle Data Engineer. Чем предстоит заниматься?   Участие в проектах по автоматизации сбора и анализа данных из различных источников.   Обработка и парсинг &quot;сырых&quot; данных.   Обеспечение процесса ETL/ELT.   Написание сервисов для работы с внешними источниками данных.   Сопровождение хранилища данных.   Администрирование сервера и мониторинг нагрузки.   Что мы ожидаем от кандидатов?   Опыт в проектировании и реализации хранилищ данных.   Уверенное знание SQL.   Опыт работы с Postgres.   Знание подходов и лучших практик ETL / ELT.   Консоль Linux.   Опыт разработки ETL процессов на Python - предпочтительно Luigi, Airflow.   Git.   Будет плюсом:  Опыт работы с BI системами (Tableau, Qlik, MS PowerBI). Мониторинг Grafana/Kibana. Организация пайплайнов для моделей машинного обучения. Опыт работа с ClickHouse или другими колоночными СУБД. Опыт работы с очередями - Kafka, RabbitMQ и т.п.  Что есть у нас?   Возможность сделать школьное образование современным и комфортным и изменить повседневную жизнь учеников и учителей к лучшему.   Официальное оформление и &quot;белая&quot; заработная плата с компенсацией больничных до текущего дохода.   ДМС со стоматологией после окончания испытательного срока.   Пятидневная рабочая неделя с возможностью выбрать удобное начало рабочего дня.   Материальную помощь в случае важных событий в жизни сотрудников.   Возможность работать удаленно.   "
"70036630","Axenix (ранее Accenture)","Data Engineer (PySpark)","False","None","None","От 3 до 6 лет","Полный день","['PySpark', 'Spark', 'Hadoop']","Команда Axenix (ex-Accenture) продолжает работу на российском рынке и аккумулирует 30-ти летний консалтинговый опыт внедрения инновационных решений. Наша экспертиза - стратегия и консалтинг, технологии и операции, направленные на цифровизацию бизнеса. В России мы работаем в офисах в Москве, Твери и Ростове-на-Дону, а также удаленно. Постоянно обмениваемся опытом и экспертизой. Чем тебе придется заниматься: В рамках пилотного проекта создание пробных версий загрузки и сравнение результатов разных технологий:  Обработка инкремента данных из кафки и применение к таблицам CLickHouse Загрузка данных в GP через External Tables Сравнение результатов Presto/GP/Clickhouse Создание артефактов для интеграции c Oracle(Batch-PULL) Создание артефактов для интеграции с Teradata (JDBC Batch-PULL) Создание артефактов для интеграции c MSSQL(Batch-PULL) Создание артефактов для интеграции c Postgres(Batch-PULL)  Мы ждем что у тебя есть:  В идеале Стэк - pySpark, Python, Trino/Presto/Athena, Microsoft Azure Databricks или AWS EMR с поддержкой DeltaLake / Apache Iceberg, Yandex.Cloud, Airflow, git, JIRA/Confluence Желательный минимум - PySpark (или любой Spark) + Hadoop  Мы предлагаем:   Динамичную работу без рутины в ведущей IT компании;   Возможность использовать передовые технологии и стратегии и менять бизнес наших клиентов к лучшему;   Проекты в разных индустриальных направлениях с передовыми технологическими решениями и современной архитектурой приложений;   Конкурентоспособный уровень дохода, годовые бонусы и регулярное повышение по результатам Performance Review;   Культуру непрерывного обучения: сертификация, online и offline обучение в России, менторство в профессиональном развитии;   ДМС с первого дня работы, включая стоматологию, в лучших клиниках Москвы и МО для cотрудника и его семьи (жена/муж, дети до 18 лет);   Страхование жизни в размере годового оклада сотрудника;   Программу поддержки сотрудника и его родственников по психологическим, юридическим, финансовым вопросам;   Дополнительные дни оплачиваемого отпуска в год;   Ежемесячную денежную компенсацию на питание   Программу корпоративных привилегий PrimeZone (более 1000 ведущих поставщиков продуктов и услуг);   Спортивные и развлекательные мероприятия за счет компании.  "
"69624868","СБЕР","Data engineer/ Дата инженер","False","None","None","От 3 до 6 лет","Полный день","['SQL', 'Python', 'Hadoop', 'Spark', 'Hive']","О проекте: Лидер команды оперативного обеспечение Data Science специалистов блока Риски выборками из различных информационных систем Банка, необходимыми для построения новых моделей, а также для оценки качества и корректировки существующих моделей. Результатом работы дата-инженера и дата-сайнтиста становится прототип математической модели исследуемого бизнес-процесса,который становится основой для внедрения модели в промышленные системы. Технологический стек:  Hadoop, Hive, Spark, Scala, Python управление требованиями: Jira, Confluence, BitBucket  Задачи:   проектирование и разработка прототипов витрин данных   проектирование и реализация инструментов автоматизации разработки   участие в доработке промышленных витрин данных   Мы ожидаем, что у тебя есть:   высшее, техническое либо финансовое образование   опыт работы от 3х лет с одной или несколькими СУБД: Oracle, MS SQL, Postgre SQL или cо стеком Hadoop (Spark)   опыт работы с хранилищами данных от 1го года   опыт работы в роли аналитика с функцией подготовки выгрузки данных для заказчика   сильные компетенции в области реляционных СУБД и хранилищ данных:   уверенное знание SQL: сложные запросы, аналитически функции, понимание физической реализации join’ов, оптимизация производительности запросов   знание одного или нескольких языков программирования: Python, Scala, Java   понимание принципов организации хранилищ данных, подходов к проектированию логической и физической моделей, понимание основной проблематики хранилищ и подходов к решению:   Мы предлагаем:  офис рядом со станцией метро Кутузовская бесплатный спортзал; места для отдыха - настольный теннис, несколько playstation, кикер, бильярд возможность работать с современным стеком технологий социальный̆ пакет (ДМС) огромный каталог образовательных программ, возможность обучения и сертификации за счет компании программа льготного кредитования в Сбербанке дисконт-программы от множества компаний партнеров возможность принять участие в других крупных и уникальных проектах Банка "
"68958646","Tele2","Data engineer\analyst (развитие источников данных Big Data)","False","None","None","От 3 до 6 лет","Полный день","['Python', 'Spark', 'SQL', 'Английский язык', 'Статистический анализ']","Ваши будущие задачи  Основной миссией нашей команды является создание прототипов таблиц фичей и данных для команд Data Scientists и Data Analysts Разработка прототипов витрин с признаками, развитие и поддержание процесса формирования слоев данных на сырых источниках, необходимых для разработки продуктов аналитики больших данных Развитие методологии качества данных и признаков Формирование требований для команды разработки для внедрения функционала в продуктив, взаимодействие и консультация в процессе внедрения Поддержка кросс-функционального взаимодействия с архитектором данных для анализа новых источников, прототипов, витрин, признаков и Data Quality.  Чтобы стать кандидатом, нужно:    Отличное владение PySpark, Python, SQL (продвинутый уровень) Уверенные знания в оптимизации Spark приложений Опыт работы c Airflow Знание принципов feature engineering в машинном обучении Общее понимание цикла построения ML модели Желание разобраться в процессе оценки предиктивной способности признаков, создании методологии мониторинга признаков, которые уже рассчитываются в продуктиве Как плюс: знание статистики на уровне основных описательных статистик, распределений, статистических тестов и их применимости Как плюс: опыт в обогащении признакового пространства внешними источниками. Английский – upper intermediate – чтение профильной литературы Высшее образование (техническое, математическое, экономическое)    Плюсы для вас:  Интересная работа в быстроразвивающейся компании Уникальная система обучения для каждого сотрудника на основе индивидуальных планов развития Зеленый свет для новых идей и предложений: мы часто делаем то, на что другие не отваживаются Возможности профессионального и карьерного роста Индексируемая заработная плата, годовые бонусы Полное соответствие ТК РФ Расширенная медицинская страховка в России и за пределами страны Компенсация затрат на мобильную связь Дополнительные материальные выплаты (пособия при рождении ребенка, вступлении в брак и т.п.) Компенсация занятий спортом через год работы.   "
"55308054","СБЕР","Data Engineer (Hadoop)","False","None","None","От 1 года до 3 лет","Полный день","[]","В SberData мы создаем централизованное хранилище данных всего Сбера. Это более 350 источников данных и 100+ Пб информации, заказ и получение данных за 15 минут и современный технологический стек работы с данными, включая собственные сборки СУБД на базе Hadoop и Greenplum.Наши решения отмечены международной премией Data Award в 2021г, а лидеры, обладающие уникальными знаниями в разработке кода и современном технологическом стеке С, Scala, Java, Python, Hadoop, Teradata, Oralce и др., являются участниками организации-фонда Apache Foundation. Масштаб задач, объемы данных, сложности финансовых процессов — мы все время на передовой современных технологий, а где-то и создаем их. Будем рады видеть в нашей команде системных разработчиков ClickHouse, DevOps-инженеров, системных администраторов, специалистов по разработке Scala/Java, разработчиков Python. Задачи, над которыми предстоит работать:  разработка дополнительной функциональности для компонентов продукта проработка решений и e2e тестирование ETL интеграций R&amp;D, реализация пилотов по выбору технологий и решений разработка документации для SDP Hadoop поддержка промышленной эксплуатации разработанных решений    Что для нас важно:   опыт разработки на одном из языков (Java/Scala/Python/Go);   опыт разработки ETL в экосистеме Hadoop;   понимание механизмов работы систем контроля версий;   понимание механизмов работы систем управления сборкой;   опыт и понимание механизмов построения модульных тестов   опыт и понимание механизмов работы систем функционального и автоматизированного тестирования   опыт администрирования Linux (предпочтение RHEL/CentOS);   желание развиваться в направлении Big Data.     Работа в Сбере - это:  амбициозные проекты и задачи профессиональный рост в дружной команде профессионалов бесплатный фитнес-зал в БЦ профессиональное обучение, конференции ДМС, страхование жизни свободный дресс-код "
"69592181","Почта России","Data engineer","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'ETL', 'SQL', 'ClickHouse', 'Data Engineer']","Обязанности:  Разрабатывать и поддерживать ELT/ETL процессы (Pentaho DI), витрины данных (clickHouse), OLAP кубы (Pentaho); Поиск и предоставление данных по запросу (ad-hoc запросы); Рефакторинг и оптимизация существующих ETL/ELT; Формировать требования к DQ проверке данных; Участие в постановке задач для подрядчиков.  Требования:  Опыт работы с любым ETL-инструментом (Pentaho, Talend, Informatica, SSIS или другие) от 2х лет; Опыт работы с большими массивами данных от 2х лет; Отличное знание SQL; Python, понимание принципов DWH и *nix окружения.  Условия:  Работа в стабильной компании федерального уровня. Премирование по итогам года. Соблюдение требований ТК РФ. Социальный пакет: ДМС и страхование при выезде за рубеж (по истечении трех месяцев с даты приема на работу); страхование от несчастных случаев и болезней 24/7 (с даты приема на работу); корпоративные скидки на членство в фитнес-клубах, страхование и путевки от туроператоров; специальные программы на банковские продукты. График работы 5/2, дом/офис. Офис БЦ &quot;ВЭБ Арена&quot; "
"69658696","Банк ВТБ (ПАО)","Data Engineer/ Аналитик данных","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'SQL', 'MS SQL Server', 'Hadoop', 'Big Data']","В кросс-функциональную команду аналитиков данных банка ВТБ требуется Data Engineer для работы с BigData хранилищем Обязанности:  построение витрин данных (Hive-SQL, Spark) с погружением в бизнес-процессы и аналитику данных; уточнение требований заказчика, системный анализ, изучение бизнес-процессов систем источников; автоматизация процессов обновления данных (AirFlow); 3 линия поддержки; тестирование нового функционала и витрин в DataLake; выстраивание процессов Data Governance на инфраструктуре Hadoop совместно с CDO Банка; контроль качества данных в хранилище (Hadoop) – постановка задач, анализ; участие в проектах связанных с витринами данных, банковским хранилищем, MLOps.  Требования:  высшее образование; уверенные знания основных банковских процессов (достаточно одного): кредитование, процессинг, депозиты; работа системным аналитиком будет преимуществом; опыт работы в ИТ компаниях, подразделениях ИТ или рисков банка от 1 года; опыт работы с реляционными и noSQL базами данных; уверенные знания SQL; знание Python или Scala; знание планировщиков процессов (AirFlow, Oozie); опыт работы с BI системами и средствами визуализации; знания в области BigData (Hadoop, Spark, Hue); желание развиваться в направлении data engineering.  Условия:   трудоустройство согласно Законодательству;   конкурентная заработная плата;   профессиональное обучение и развитие;   добровольное медицинское страхование, льготные условия кредитования;   корпоративная пенсионная программа, материальная помощь;   спортивная жизнь и корпоративные мероприятия;   возможность построить карьеру в ведущем банке России.  "
"69882715","Unilever","Инженер данных / Data engineer","False","None","None","От 3 до 6 лет","Полный день","['Английский язык', 'Cистемы управления базами данных', 'Архитектура данных', 'Big Data', 'ETL', 'Microsoft Azure']","IT Unilever по всему миру создают технологическую инфраструктуру и современные IT – продукты, позволяющие нам использовать Digital подход во главе всех инициатив. С нами вы сможете заниматься исследованием развивающихся рынков, разрабатывать онлайн-инструменты, улучшать различные процессы и помогать всем сотрудникам технологически оптимизировать их задачи!  IT Unilever открыты к новому и всегда обмениваются лучшими практиками с коллегами из разных стран; IT Unilever объединяют людей с огромным желанием учиться и развивать свои навыки; IT Unilever поддерживают самые смелые инициативы и эксперименты;  Сегодня Unilever в поиске Инженера данных. Обязанности   Проектировать и создавать активы данных, обеспечивающие точность, надежность, производительность, безопасность и построение высококачественных наборов данных для бизнес-аналитики; Проектирование Хранилища данных, Озера данных и баз данных; Обеспечение качества данных; ELT, ETL , оптимизация запросов; Моделирование данных; Кодирование и тестирование (SQL, Databricks, Python) POC и эксперименты; Построение промышленных интерфейсов обмена данными в т.ч. с SAP; Построение витрин данных; Оптимизация/автоматизация подготовки данных для отчетности; Использование технологий для обработки больших объемов данных (Apache Spark; Databricks).  Требования  Глубокое знание Microsoft Azure (включая ADF, Logic Apps) и понимание функций продукта, плюс знакомство с другими облачными решениями; Подтвержденный опыт проектирования, разработки и внедрения моделирования данных на концептуальном, логическом и физическом уровнях; Понимание дизайна ИТ-систем (сеть, протоколы авторизации и аутентификации, безопасность данных, аварийное восстановление); Глубокое понимание инструментов и методов ETL и ELT; Опыт работы с потоковой передачей данных в режиме реального времени и быстрыми данными, а также такими технологиями, как Kafka/Confluent, является плюсом; Знакомство с разработкой/реализацией микросервисов, управлением API, контейнерными технологиями, Kubernetes, бессерверными технологиями, NoSQL также является преимуществом.  Мы предлагаем:  Конкурентная заработная плата Fix + бонус; ДМС и страхование жизни; Возможность принять участие в уникальных проектах компании; Профессиональная команда с самыми талантливыми специалистами для достижения ваших амбициозных целей; Уникальная корпоративная культура. "
"68596700","Ozon","Ведущий Data Engineer, Ozon Fresh","False","None","None","От 3 до 6 лет","Полный день","['Hadoop', 'Python', 'SQL', 'SCALA', 'Java', 'Vertica', 'Kafka', 'ClickHouse', 'HDFS', 'Hive']","ЧТО ВАМ ПРЕДСТОИТ:  Проектировать и разрабатывать аналитические витрины данных. Разрабатывать и развивать платформу хранения и обработки больших данных. Настройка ETL процессов.  ЧТО МЫ ОЖИДАЕМ ОТ ВАС:  Высшее математическое или техническое образование. Опыт в области работы с базами данных от 2-х лет, умение проектировать базы данных. Уверенное знание SQL на уровне построения сложных аналитических запросов (window functions, рекурсивные запросы и т.п.), навыки оптимизации запросов. Опыт работы с экосистемой Hadoop (HDFS, Hive, Spark). Знание и опыт разработки на Python и/или на Scala/Java.  БУДЕТ ПЛЮСОМ:  Опыт работы с MS SQL, Vertica, Kafka, ClickHouse. Опыт разработки микросервисных архитектур.  ЧТО МЫ ПРЕДЛАГАЕМ:  Динамичный и быстроразвивающийся бизнес, ресурсы, возможность сделать вместе лучший продукт на рынке e-commerce. Свободу действий в принятии решений. Достойный уровень заработной платы. Профессиональную команду, которой мы гордимся. Возможность развиваться вместе с нашим бизнесом. "
"68891469","MY.GAMES","Data engineer (Business Solutions)","False","None","None","От 3 до 6 лет","Полный день","['Python', 'Linux', 'Hadoop', 'SQL', 'ClickHouse']","Мы создаём инструменты для решения бизнес-задач MY.GAMES. Внедряем новые технологии и разрабатываем кросс-платформенные программные решения для игровых студий и департаментов компании.   разрабатываем аналитические системы, которые умеют строить прогнозные модели выручки и окупаемости со множеством метрик и переменных;   автоматизируем рекламные кампании с помощью собственного робота закупки трафика;   развиваем рекомендательные системы в игровых проектах и платформах с помощью Machine Learning;   автоматизируем бизнес-процессы, чтобы маркетинговые стратегии игровых студий работали по максимуму;   храним и визуализируем игровые данные;   разрабатываем наборы сервисов и библиотек, облегчающих создание и поддержку мобильных игр.   Специалисты департамента — эксперты в разработке, ML, DWH, DevOps, аналитике и маркетинге. В штате 150 человек. Актуальный стек технологий: Python, SQL, Spark, Scala, FastAPI, Vue.js, Django, Hadoop, ClickHouse, Airflow, Jupyter, Docker, Kubernetes. Мы всегда рады инициативным и талантливым людям. Если вы готовы разбираться в нетипичных задачах, создавать и поддерживать стратегические проекты, открывать новое — присоединяйтесь к нам! Добро пожаловать в команду MY.GAMES Business Solutions! Задачи:  участие в роли BI-DWH разработчика в развитии хранилища и сервисов для анализа данных; создание новых сервисов для анализа данных; построение и оптимизация data-pipelines, ETL процессов; проектирование и разработка витрин.  Требования:  опыт работы с хранилищами данных от трех лет; опыт программирования на языке Python от трёх лет.   отличное знание SQL; отличное знание Python; опыт работы с Hadoop от года; навыки работы с Airflow; навыки работы с ClickHouse; понимание принципов построения DWH; знания в области ETL/ELT процессов; умение разбираться в сложных процессах преобразования данных.  Будет плюсом:  опыт работы со spark/pyspark; знание Scala или Java; навыки в BI.  Мы предлагаем:  возможность удаленной работы или гибридный график; расширенный ДМС со стоматологией — с первого рабочего дня; компенсация занятий спортом после прохождения испытательного срока; техника на привычной операционной системе; программа благополучия сотрудников, включающая работу с психологом, финансовые и юридические консультации; материальная помощь при рождении ребенка, ДМС и подарки детям; корпоративные скидки — от застройщиков жилья до доставки еды; занятия английским языком, профессиональное обучение и конференции; внутренние митапы и демо-дни. "
"67136636","СБЕР","Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Python', 'Hadoop', 'SQL', 'Spark', 'Big Data']","Мы ищем Middle/Senior Data Engineer для реализации амбициозного проекта перехода на новую платформу кампейнига Сбера.   Наша задача – обеспечить перевод legacy процессов на современный стек технологий, разработать решения по обработке и доставке данных, создать с 0 процессы загрузки обратных потоков данных для задач аналитики и развить MLOps практику в команде для вывода DS моделей в пром.   Что нужно делать:    Разрабатывать процессы сборки, обработки и поставки данных в Hadoop на Spark Развивать архитектуру проекта и адаптировать ее под новые требования Собирать и анализировать требования бизнеса по получению новых данных, оценивать наличие и качество данных Проводить системное, функциональное и интеграционное тестирование Разрабатывать и внедрять эффективные методики контроля качества данных Участвовать в code review и создавать инструменты для упрощения своей работы    Мы ждем, что ты:    Имеешь опыт разработки на Spark (желательно Spark/Scala) Знаком с инструментами экосистемы Hadoop: Hive, Spark Хорошие знания SQL, опыт оптимизации запросов Общий стаж работы не менее 3 лет Высшее техническое образование Будешь проактивен в работе с внешними и внутренними контрагентами      Ты получишь знания и опыт работы со стеком BigData-разработки и аналитики:    Scala, Apache Spark Apache Kafka, Apache Hadoop (HDFS, HBase, Hive), Apache Airflow, Python Инструменты DevOps GreenPlum и Teradata SQL    Мы предлагаем:    Deep diving в предметную область, много разработки по задачам имеющим прямой эффект на бизнес Возможность привносить новые идеи и нестандартные решения Сообщество D-people– поддержка, развитие и возможность учиться у профессионалов Достойную оплату труда ДМС, сниженные ставки по кредитованию, программы лояльности для сотрудников Современный офис рядом с метро Кутузовская, бесплатный фитнес в офисе    Приходи к нам, расти вместе с нами!  "
"69024037","ПартКом","Business Data Engineer","True","160000","None","От 1 года до 3 лет","Удаленная работа","['SQL', 'хранилище данных', 'MS SQL', 'OLAP']","В настоящее время мы создаём новую платформу данных, которая включает в себя несколько блоков: получение данных из различных источников, загрузка данных в единое ХД, витрины, качество данных, доступ к данным и т.д.Несмотря на то, что платформа только начинает свою работу, через неё уже проходят около 1000 объектов в ХД, десятки миллионов ежедневно обновляемых записей, больше 20 источников данных.В ближайшее время объём обрабатываемых данных будет расти, поэтому нам нужны дата-инженеры, которые умеют качественно направить поток данных в нужное русло к потребителям. Чем придется заниматься:  Поиск источников данных и исследование данных на корректность перед загрузкой в ХД Настройка ETL/ELT процессов для выгрузки данных в ХД Подготовка витрин данных для аналитиков Обучение аналитиков работе с витринами данных Доступ к данным ХД/витрин (column_level, row_level) Оркестрация ETL/ELT Визуализация data lineage Качество данных (Data Quality).  Ожидания от опыта:  Знание методологий моделирования ХД (3NF, DataVault, AnchorModeling и т.д.) Понимание, что такое &quot;платформа данных&quot;, какие инструменты взять готовые, какие создаём сами Отличное владение SQL (основной объём данных в MSSQL) Богатый опыт ETL/ELT Python Умение решать сложные и нестандартные задачи Будет плюсом опыт работы с MPP субд (greenplum, clickhouse).  Мы предлагаем:  Дистанционная работа, не нужно тратить время на поездки в офис Трудоустройство в штат компании, работа в группе Поддержку инициатив Возможность оплаты курсов и конференций Возможность приобретать автозапчасти со скидкой Возможность работать в команде, которая двигает рынок автозапчастей в РФ. "
"69529173","СБЕР","Data Engineer","False","None","None","От 3 до 6 лет","Гибкий график","[]","В SberData мы создаем централизованное хранилище данных всего Сбера. Это более 350 источников данных и 100+ Пб информации, заказ и получение данных за 15 минут и современный технологический стек работы с данными, включая собственные сборки СУБД на базе Hadoop и Greenplum. Наши решения отмечены международной премией Data Award в 2021 г., а лидеры, обладающие уникальными знаниями в разработке кода и современном технологическом стеке С, Scala, Java, Python, Hadoop, Teradata, Oracle и др., являются участниками организации-фонда Apache Foundation. Масштаб задач, объемы данных, сложности финансовых процессов — мы все время на передовой современных технологий, а где-то и создаем их. Мы создаем платформу данных Экосистемы Сбера, которая позволит компаниям Экосистемы применять передовые «облачные» практики в своих процессах обработки данных и предлагаем вам погрузиться в самые инновационные и амбициозные проекты и задачи. Обязанности:  Разработка и поддержка процессов работы с данными в экосистеме Hadoop; Разработка ETL-процедур на Spark; Техническая поддержка пользователей; Диагностика и решение проблем с производительностью; Участие в разработке и документировании витрин данных; Взаимодействие с заказчиками, аналитиками, архитекторами в процессе проектирования и реализации задач; Формирование базы знаний по продукту.  Требования:  Опыт работы с приложениями из Hadoop-стека (Hbase, Spark, Kafka, Hive, Impala, Hue и т.д.); Опыт разработки на Python/Java/Scala; Знание форматов данных (Parquet, Avro, Iceberg и т.д.) Понимание принципов модели распределенных вычислений, принципов организации Data Lake/DWH; Понимание подходов к организации разработки (CI/CD, DevOps), практический опыт работы с инструментами Jenkins, SonarQube, TeamCity, Anisble, Git (BitBucket/GitLab).  Плюсом будет:  Знание средств автоматизации развертывания (Ansible, Puppet, etc.); Опыт работы с известными ETL-инструментами; Опыт работы с Jupyter Notebook; Опыт работы с Spark on Kubernetes; Опыт администрирования РСУБД.  Условия:  Профессиональное обучение, семинары, тренинги, конференции, корпоративная библиотека; ДМС, страхование жизни; Самые инновационные, амбициозные проекты и задачи; Свободный дресс-код; Гибкий график для оптимального баланса работы и личной жизни; Льготные кредиты и корпоративные скидки; Конкурентная компенсация (оклад и премии по результатам деятельности). "
"69628087","Х5 Group","Data engineer (интеграция с источниками)","False","None","None","От 3 до 6 лет","Удаленная работа","['Hadoop', 'Hive', 'ETL', 'SQL', 'MySQL', 'ORACLE', 'MS SQL', 'Postgres', 'Python', 'Groovy', 'Nifi', 'Airflow', 'Informatica', 'Kafka']","В команде ETL X5 Технологии, в связи расширением и появлением новых задач по загрузке новых данных, открыта позиция Data engineer (интеграция с источниками)На данный момент, у нас построен кластер Hadoop общей емкостью 1 петабайт. Команда ETL отвечает за интеграцию кластера с источниками данных.Мы загружаем данные из различных источников данных (как внутри компании, так и извне) в кластер Hadoop.Результат работы разработанных нашей командой механизмов - стабильно обновляемые базы данных в Hive. На основе этих баз данных ДБД разрабатывает различные продукты для широкого круга заказчиков внутри X5 Group и снаружи. Наш стек: Apache Nifi, Kafka, Hadoop, Hive, Sqoop, Postgres, Java Чем предстоит заниматься:  Разработка приложений загрузки данных; Валидация данных; Отладка потоков данных; Исправление ошибок загрузки данных.  Наш кандидат:  Знание SQL (индексы, функции, умение читать планы запросов). Опыт работы с любой реляционной БД (Oracle, Postgres, MySQL, MsSQL, DB2 и т.п.). Умение писать на любом языке (Python, Groovy, Java и т.п.). Умение работать с Git в консоли. Опыт работы с любым графическим ETL инструментом (Apache Nifi, Airflow, Talend, Informatica, SAS и т.п.). Опыт работы с Apache Kafka и системами хранения и визуализации логов (примеры - EFK, Graylog). Опыт работы с Grafana. Опыт работы с Hadoop. Понимание устройства HDFS, форматов данных. Опыт работы с Hive или любым другим хранилищем на основе Hadoop. Желание самостоятельного усовершенствования своих навыков и повышения квалификации.  Будет плюсом  Опыт работы с NoSQL (Mongo, Redis, Kassandra, Clickhouse и т.п.) Опыт работы в смежных областях (Саппорт, системный анализ и т.п.) Опыт работы в больших компаниях  Мы предлагаем:  возможность работать удалённо или ездить в офис на м. Волгоградский проспект / м. Добрынинская/ м. Парк Культуры возможность роста: план развития, регулярная обратная связь, 2 раза в год оценка персонала яркую корпоративную жизнь с большим количеством мероприятий, конкурсов и возможностей для творческой реализации: X5 Tech Bar, регулярные внутренние митапы, демо-дни, открытые микрофоны, обмен опытом через внутренние комьюнити, день IT-специалиста, программы корпоративного волонтерства, корпоративное предпринимательство X5 Idea Challenge широкий пакет ДМС (включая выезд за рубеж и стоматологию), страхование жизни и здоровья; забота о благополучии сотрудников: регулярная онлайн-йога, ежедневные онлайн-зарядки по утрам, ЗОЖ-марафоны. скидки в экосистеме бизнесов Х5 («Пятёрочка», «Перекрёсток», «Карусель», «ОКОЛО», «Много лосося», «5post», «Перекресток Впрок») программа привилегий Prime-zone (скидки на товары и услуги и специальные предложения от компаний-партнёров); материальную помощь сотрудникам, попавшим в сложную жизненную ситуацию    Узнать о нас больше вы можете в соц.сетях #X5TECH  "
"69591337","Aston","Data Engineer (ETL)","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'PostgreSQL', 'ETL', 'DWH', 'ORACLE', 'MS SQL', 'Click House']","IT-компания Aston приглашает к сотрудничеству Data Engineer (ETL) на проект в сфере FinTech. Задача – разработка финансовых сервисов в сфере FinTech. Заказчик – консалтинговая компания, которая является одним из лидеров мирового рынка профессиональных услуг и цифровых технологий, уже 15 лет входит в рейтинг Fortune Global 500. Чем предстоит заниматься:   разработка различных слоев данных DWH согласно ТЗ.   Каким мы видим подходящего кандидата:  опыт работы в роли Data Engineer (ETL) от 2-х лет; владение SQL (фильтрация по агрегатам, аналитические, оконные функции); знание внутренних механизмов работы БД: способы физического соединения, чтение плана запроса, распределение данных, партиционирование, индексация данных, понимание разницы между delete, truncate, drop; опыт работы с одной из БД: Hive/ClickHouse/GreenPlum/PostgreSQL/Oracle/MS SQL; знание отличий MPP архитектуры БД; опыт работы с продуктами SAS: Data Integration Studio, Enterprise Guide.  Будет плюсом:  знание о моделях данных Data Vault/Data Vault 2.0/Anchor Modelling.  Что мы предлагаем:  долгосрочные проекты от наших российских заказчиков и партнеров, с которыми мы сотрудничаем с 2007 года; возможность выбора формата работы (дистанционно или из офиса в Питере/Казани/Ростове-на-Дону); оплачиваемый бенч; наставника, соответствующего вашему техническому уровню; системы менторства и адаптации для новых сотрудников; прозрачные системы performance review; возможность добавить в СV работу в команде с лидерами FinTech, Healthcare, Retail, Telecom и других; возможность выбора/смены проекта; у нас своя школа архитекторов, а также корпоративный обучающий портал для любого стека вашей специализации; ﻿медицинское страхование (+стоматология); возможность дополнительного заработка через участие в активностях компании.  Присоединяйтесь к нашей команде!"
"69791866","Х5 Group","Data engineer (Python Hadoop Spark)","False","None","None","От 1 года до 3 лет","Удаленная работа","['Python', 'Spark', 'Hadoop']","X5 Group - лидер офлайн- и онлайн-рынка продуктов питания. Мы управляем портфелем брендов сетевых магазинов «Пятёрочка», «Перекрёсток», «Карусель», «Чижик», цифровыми бизнесами «Перекрёсток Впрок», «Около», 5Post, а также собственными службами логистики, прямого импорта и рядом цифровых сервисов для партнёров. Х5 Технологии — это отдельная бизнес-единица Х5 Group, которая отвечает за создание комплексных цифровых решений для бизнес-единиц Х5. Наша команда — это 3000+ специалистов по информационным технологиям и большим данным. Мы разрабатываем решения, которые помогают десяткам миллионов людей. «Прогнозирование спроса»: модели машинного обучения и алгоритмы, на основе данных по чекам и остаткам предупреждают сотрудников магазинов о том, что надо проверить/положить товар на полки. Продукт прошел стадию пилот (600 магазинов) и теперь дорабатывается и масштабируется на 15+ тыс. магазинов. Команда: ≈10 человек Стек: Python 3.5-3.8., Fastapi, Hadoop, PySpark + Airflow. PostgreSQL, Kafka, Kibana, ElasticSearch. Docker, K8s, GitLab. Основные задачи (детальнее расскажем на собеседовании):  Написание пайплайнов по работе с данными с использованием Airflow, Spark (пример: регулярный расчет фичей для модели, построение витрин с данными. их обновление, расчет метрик качества данных для мониторинга) Оптимизация расчета фичей для модели для выдерживания SLA при масштабировании продукта под большее количество магазинов Настройка мониторинга и алертинга качества данных и фичей с использованием Zabbix, Grafana Написание unit тестов (pytest), тестов для различных участков ML пайплайна, участие в код ревью Не обязательно, но будет большим плюсом: опыт работы с ML, выведения ML моделей в продакшн  Мы сможем рассмотреть на вакансию кандидатов, у которых есть:  Опыт работы с Python от 2 лет Опыт с Hadoop, Hive, Spark Знание SQL (чтобы писать и оптимизировать запросы)   Знание классических алгоритмов и структур данных Опыт работы с docker (kubernetes, pyspark, airflow и ds фреймворков python– не обязательно, но будет плюсом) Умение пользоваться git&#39;ом и работать в команде  Мы предлагаем:  Схему мотивации: Fix + годовой бонус (20% годового оклада) Удобный офис у м. Парк Культуры или удаленную работу Гибкий график работы (с 8/9/10 утра) Обратная связь, возможность профессионального и карьерного роста (2 раза в год оценка персонала) Возможность обучаться и сертифицироваться за счёт компании: внешние тренинги и семинары по профессиональным тематикам, отраслевые конференции, программа развития управленческих навыков, очные мастер-классы, платформы онлайн-образования и многое другое Яркую корпоративную жизнь с большим количеством мероприятий, конкурсов и возможностей для творческой реализации Широкий пакет ДМС (включая выезд за рубеж и стоматологию), страхование жизни и здоровья Скидки в магазинах сети Х5 («Пятёрочка», «Перекрёсток», «Карусель») Программу привилегий Prime-zone (скидки на товары и услуги и специальные предложения от компаний-партнёров) Материальную помощь сотрудникам, попавшим в сложную жизненную ситуацию  Присоединяйся к одной из самых быстрорастущих цифровых команд России! X5 Retail Group. Создавай новый ритейл"
"69537454","Иннотех, Группа компаний","Data engineer (ДИР)","False","None","None","От 1 года до 3 лет","Удаленная работа","['Python', 'airflow', 'ETL', 'Data Vault', 'greenplum', 'PostgreSQL']","Data engineer Вместе с нам ты будешь:   Выполнять функциональные обязанности в рамках проекта организации хранилища данных на Arenadata Greenplum DB;   Осуществлять разработку и проверку работоспособности ETL процессов по загрузке данных из систем-источников в слои хранилища (разработка DAG-ов в Airflow, процедур в Greenplum, работа с ETL фреймворком);   Выполнять оптимизация существующего кода ETL-процессов;   Выполнять тестирование ETL;   Участвовать в сборке поставки и выката на продуктивный контур.   Какие знания и навыки для нас важны:    Опыт работы в роли разработчика DWH не менее 3 лет; Опыт работы с Data Vault 2 / Anchor; Уверенные знания SQL; Опыт работы с Apache Airflow; Опыт работы с Greenplum, PostgreSQL, Oracle; Опыт работы с Git, bash; Желательно знание python. "
"67996180","Спортмастер","Senior Data engineer","False","None","None","От 3 до 6 лет","Полный день","['Data Engineer', 'Python', 'SQL', 'Hadoop', 'ORACLE']","Мы находимся в поиске Ведущего Data engineer&#39;a, который будет заниматься развитием направлений :Цифровая аналитика &quot;Антифрод&quot;, &quot;Инфраструктура DataLake&quot;. Ваши задачи:   Реализация ETL в Hadoop (с помощью Airflow).   Работа с различными источниками данных: Oracle MS SQL API личных кабинетов, микросервисы.   Батч и стримы с помощью PySpark и Kafka.   Подготовка витрин для анализа (Hive + Spark+ SQL)   Наш стек: Cloudera hadoop; Kafka, Spark, Airflow; Jira; Confluence; GitLab Мы ждем от будущих коллег:  Уверенное владение Python. Опыт использования эко-системы Hadoop: HDFS, Apache AirFlow, Hive, Kafka,Spark. Знание SQL Опыт работы с реляционными базами данных (Oracle). "
"69746860","Aston","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'SQL', 'ETL']","Аутсорсинговая IT-компания Aston приглашает к сотрудничеству Data Engineer на масштабный проект в сфере FinTech. Проект – HR-хранилище. Разработка на PostgreSQL, построение визуализации на Power BI. Заказчик – входит в десятку банков на территории РФ по величине активов, кредитного портфеля и средств клиентов, имеет более 400 офисов, 27900 банкоматов, а также активно развивает онлайн-сервисы и работает по стратегии Mobile first. Чем предстоит заниматься:  разработка хранилища данных; разработка хранилища по методологии DWH на базе PostgreSQL; разработка витрин данных, ядра, стейдж разработка; разработка пайплайнов для загрузки данных в стейдж.  Каким мы видим подходящего кандидата:  опыт работы в роли Data Engineer от 2-х лет; знание SQL, Python; понимание принципов построения хранилищ данных; опыт работы с реляционными базами данных; понимание ETL/ELT процессов; опыт работы c CI/CD; понимание принципов CDC; чтение плана выполнения запросов.  Будет плюсом:  опыт работы с Apache Airflow/Apache Nifi; знание методологий построения хранилищ данных.  Что мы предлагаем:  долгосрочные проекты от наших российских заказчиков и партнеров, с которыми мы сотрудничаем с 2007 года; возможность выбора формата работы (дистанционно или из офиса в Питере/Казани/Ростове-на-Дону); оплачиваемый бенч; наставника, соответствующего вашему техническому уровню; системы менторства и адаптации для новых сотрудников; прозрачные системы performance review; возможность добавить в СV работу в команде с лидерами FinTech, Healthcare, Retail, Telecom и других; возможность выбора/смены проекта; у нас своя школа архитекторов, а также корпоративный обучающий портал для любого стека вашей специализации; медицинское страхование (+стоматология); возможность дополнительного заработка через участие в активностях компании.  Присоединяйтесь к нашей команде!"
"69656003","red_mad_robot","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","[]","В red_mad_robot мы создаём цифровые продукты, которыми пользуются миллионы людей, от маркетплейсов и экосистем до мобильных приложений и веб-порталов — и, возможно, ты один из них! Помогаем компаниям в технологической трансформации и поддерживаем стартапы, которые создают наше общее будущее. Для наращивания технической экспертизы мы ищем Data Engineer в наш бизнес-юнит Research Data Lab, который специализируется на создании продуктов на базе технологий машинного обучения (ML) для промышленного сектора России. Что предстоит делать:  Проектировать и реализовывать хранилища данных (Data Lake и Data Warehouse). Заниматься написанием обработчиков входных данных. Настраивать мониторинг и логирование. Заниматься конфигурированием и тестированием сети в кластере, где развернуто решение. Общаться с пользователями системы для выявления потребностей (BA, DS, Backend), формулировать требований к системе.  От тебя:  Опыт построения Data Ware House (DWH).  Опыт работы с posgresql, tableau, опыт проектирования хранилищ.  Опыт разработки в Java, Python от 2-х лет. Навыки администрирования unix-систем, знания компьютерных сетей. Практический опыт работы с Docker, k8s или другими оркестраторами. Стек технологий: DataLake (принципы работы с ними), Hadoop ecosystem (Hadoop, Hive), ETL framework - Airflow, Data processing - Spark (pySpark).  Будет плюсом:  Опыт в роли Data Analyst (DA). Навык построения архитектуры, основанной на событиях (event driven architecture). Знания model deploy - MLflow.  Навык использования Airbyte, dbt, airflow.   Чего ждать от роботов:  Работа в сильной команде, где ценят инициативу, договорённости и честную обратную связь. Влиться и освоиться тебе поможет ментор. Позже ты тоже сможешь стать ментором для других. Развитие — наш пунктик: берёмся за сложные задачи, которые нас продвинут. Проводим школы и интенсивы, чтобы прокачаться. Зовём интересных гостей для развития кругозора. Пользуемся бесплатной электронной библиотекой. Всё по-честному: у нас белая зарплата и оформление в штат с первого дня, прозрачная система развития с персональным планом и ДМС с хорошими условиями через полгода работы. И телу приятно: работаем в удобном БЦ рядом с парком им. Горького с классным видом из окна, уважаем гибкий график и возможность совмещать офис с работой из дома. "
"69485128","Банк Открытие","Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Python', 'SQL', 'Pandas', 'Machine Learning', 'Data Science', 'Data Engineer']","Обязанности:  Решение задач в области машинного обучения и анализа данных в подразделениях Банка и компаниях группы, не имеющих собственных ресурсов в области Data Science; Специфика работы команды диктует необходимость решать задачи очень широкого спектра: от распознавания речи и обработки естественного языка до классических моделей классификации.  Требования:  Опыт DS от 3-х лет. Желательно в банке или в страховой компании; Языки: Python, SQL; Опыт разбора бизнес задач с заказчиками; Умение коммуницировать сложные технические концепции простыми словами; Умение подбирать оптимальные инструменты ML для решения задач.  Мы предлагаем Вам:   Стать частью крупного, динамично развивающегося банка;   Работу в команде единомышленников, неравнодушных к передовым технологиям и практикам;   Неограниченные возможности для совершенствования Ваших навыков и знаний, обучаясь у лучших и решая сверхамбициозные задачи;   Позитивную рабочую атмосферу, дружный коллектив;   Удобное расположение офисов;   Достойную и конкурентную заработную плату, квартальные и годовые премии, размер которых зависит от результатов Вашей работы;   Разнообразный социальный пакет: ДМС, страхование жизни, страхование для выезжающих за границу, льготные условия приобретения банковских продуктов, скидки в фитнес-клубах и в компаниях-партнерах.  "
"69774849","МТС","Data engineer на продукт B2B (Big Data)","False","None","None","От 1 года до 3 лет","Полный день","['Hadoop', 'Hive', 'Python', 'SQL', 'Spark']","Big Data МТС – место, где телеком данные превращаются в реально работающие IT-продукты. Мы создали и протестировали несколько десятков сервисов. Самые успешные из них уже стали частью экосистемы МТС. Например, МТС Маркетолог, рекомендации в KION (МТС ТВ), услуга “Кто звонит?” или Спам blacklist. Кого мы ищем?  Опыт работы с данными в одной из отраслей: телеком, интернет-компании, банки, страхование, ритейл; Понимание моделей данных и принципов устройства хранилищ данных Хорошее знание SQL; Опыт работы хотя бы с одной промышленной БД; Знание стека Hadoop/Hive/Spark и опыт работы с большими объемами данных Знание Python; Навыки загрузки данных из неструктурированных источников (data exploration, data wrangling, data cleansing); Умение работать в Unix консоли, базовое понимание (как минимум умение читать) shell scripts; Хорошее знание технического английского, навыки поиска информации.  Что предстоит делать?  Поддержка текущих регламентов команды (внесение изменений в сбор витрин и моделей); Разработка процессов по поиску объектов коммерческой недвижимости (парсинг сайтов, алгоритм обогащения данных, интеграция с системами компании); Построение процессов сбора и репликации витрин для построения дашбордов по конкурентам (сбор на hive и терадата, репликация в greenplum).  Сейчас мы ищем Data Engineer на продукт B2B. B2B-портал - это веб-инструмент для сотрудников отделов продаж b2b, на котором сотрудник может посмотреть информацию о тех компаниях, с которыми ему необходимо работать. Портал помогает эффективно доставлять рекомендации по работе с клиентами на всех сегментах жизненного цикла для менеджера по его портфелю. Комплексно оценивать состояние клиентов, оптимизировать работу с портфелем. Цель: оптимизировать управление жизненным циклом клиента через рекомендации на всех его этапах (привлечение, развитие, отток). Доставка рекомендаций до сотрудников продаж b2b. Что вы найдете в команде Big Data? Команда: в команде Data engineer сейчас 30 человек (во всей Big Data МТС более 300 человек). Все Data инженеры разработчики поделены на группы со своими лидами. Каждую неделю мы обмениваемся опытом на совместных синках. Data инженеры работают в продуктах со своей автономной командой, в которой есть все роли: аналитики, DS, разработчики, девопсы, менеджеры продукта. Условия: каждый месяц - аванс и зарплата, дважды в год - премия. ДМС + стоматология, корпоративная связь, специальные предложения от партнеров и друзей МТС, отпуск 31 день в год. Выдаем 16” MacBook Pro или Dell на выбор. Есть ли обучение?  Конференции, митапы. Корпоративный университет МТС и масштабная виртуальная библиотека. А ещё мы регулярно обмениваемся опытом на совместных синках с лидами экспертизы  Какой график? Гибкое начало рабочего дня в промежутке с 8 до 11. Есть возможность работать несколько дней вне офиса по договоренности с командой. Сколько этапов при отборе? Не более трех:  HR + первое тех. интервью с лидом направления Тестовое задание/второе интервью - по необходимости Собеседование с PO и командой, выбор кандидатом проекта "
"69707884","Группа компаний С7","Data Engineer middle/senior","False","None","None","От 3 до 6 лет","Полный день","['Python', 'SQL', 'PostgreSQL', 'Linux', 'ClickHouse']","S7 TechLab – IT компания, реализующая инновационные IT продукты для подразделений группы компаний S7. NLP чат-боты для поддержки пользователей, компьютерное зрение для бортов самолетов, прогнозирование загрузки самолета – это лишь некоторые примеры типовых задач, которые реализуются силами команды S7 TechLab и помогают авиакомпании занимать лидирующее место на рынке авиаперевозок России. В команду платформы данных S7 ищем Data Engineer уровня middle+ и выше. Цель команды платформы данных – дать пользователям инструменты для работы с данными. Команда развивает data lake, sql query engine, data catalog, streaming data bus, что помогает пользователям искать, использовать и передавать данные внутри компании. Как инженер данных, вы будете разрабатывать компоненты платформы на базе инструментов с открытым исходным кодом, включая разработку и поддержку пайплайнов загрузки данных, сервисов взаимодействия с каталогом данных, кластером Kafka, СУБД и внутренними системами компании. ✈️Задачи:  Автоматизация процессов загрузки данных в хранилище Создание и поддержка инфраструктуры для движения и использования данных Разработка собственных API и интеграция с API внешних систем; Мониторинг операционных потоков данных Повышение качества данных  Пожелания к кандидату:  Python Пайплайны и стримы со Spark, Kafka, Kafka Connect, Airflow Kubernetes, Docker Понимание, как работает parquet, avro Знание базовых различий СУБД (Oracle, Postgres, Vertica, Clickhouse).  Плюсом будет:  Опыт работы с Scala Опыт DevOps Опыт с DQ  Если вы кроме дата инженерных задач не чужды развитию open source, продукт поможет развиться в этой области. Что мы предлагаем:  Приветственный мерч новичкам; ДМС; Корпоративную жизнь, насыщенную разнообразными коллективными событиями; Подарки для детей к первому классу и на Новый год; Профессиональное развитие: проходить обучение в компаниях-партнерах, участие и выступление в конференциях. И да! У нас есть партнёр, который готовит к выступлениям (поможет определиться с тематикой, посылом, составить доклад и подготовит к выступлению); Онлайн трансляции в режиме вопрос-ответ с приглашенными интересными гостями, например Алексей Водовозов, главы ИТ-подразделений Nvidia, Dodo pizza, ученые. У нас в компании Agile практика. Если интересно данное направление вместе с нашим экспертом готовы помочь нарастить эти скиллы; Возможность путешествовать по специальным корпоративным тарифам для вас и вашей семьи. Например, туда-обратно: Сочи - 3 200 рублей. Корпоративные скидки в PrimeZone: скидки в рестораны, на обучающие курсы, товары и тд.; Самостоятельность и широкий диапазон влияния на продукт. Возможность существенно улучшать и изменять, предлагать идеи — всё это можно и нужно; Команда распределенная, работаем удаленно, либо у нас есть 2 уютных офиса: г. Москва Петровка 7 или г. Иннополис :) Полное соблюдение Законодательства Российской Федерации. "
"69515064","Axenix (ранее Accenture)","Data Engineer (PySpark/AWS)","False","None","None","От 3 до 6 лет","Полный день","['Английский язык', 'Python', 'Деловое общение', 'Обучение и развитие', 'aws', 'Spark']","Суть проекта – миграция данных из различных источников (РСУБД, файлы, On-Premise кластера Hadoop, специализированные БД) в облако AWS. Миграция проводится с помощью кастомных фреймворков, разрабатываемых либо клиентом, либо нашими европейцами. Обязанности:  Разработка потоков по загрузке данных на основе кастомных фреймворков (зачастую сводится к правильной конфигурации) Доработка самих фреймворков    Требования:   Разговорный английский не ниже B2 – так как общения/встреч очень много   Опыт разработки на Python\Scala   Знание PySpark, Python AWS – опционально.   Условия:  Возможность профессионального и карьерного роста в международной компании от стартовой позиции до управляющего директора.  Конкурентоспособный уровень дохода и регулярное повышение по результатам Performance Revie  ДМС с ервого дня работы, включая стоматологию, в лучших клиниках Москвы и МО для сотрудника, его партнера и детей до 18 лет. Страхование жизни в размере годового оклада сотрудника. Дополнительные 5 дней оплачиваемого отпуска в год. Ежемесячная денежная компенсация на питание/содержание автомобиля. Корпоративный iphone :) Программа корпоративных привилегий PrimeZone (более 1000 ведущих поставщиков продуктов и услуг). Возможность обучения и сертификации за счет компании – до 80 часов тренингов в год, в том числе в зарубежных центрах Accenture. Участие в корпоративных спортивных и развлекательных мероприятиях за счет компании. Бесплатные занятия йогой в офисе; психологическая, финансовая и юридическая консультации. "
"69624856","Sitronics Group","Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['ETL-процессы', 'Kafka', 'Apache NiFi', 'Docker']","Обязанности:  Разработка/сопровождение/документирования ETL-процессов, в т.ч. написание собственных дата-процессоров в Apache NiFi; Реализация процессов обработки в кластере в Apache NiFi, в т.ч. организация мониторинга и контроля качества. участие в проектировании, реализации и документировании хранилища данных для разрабатываемой информационной системы  Требования:  Опыт разработки ETL-процессов в Apache NiFi , в т.ч. в кластере и территориально распределенных сегментах; Опыт настройки и администрирования Apache NiFi, в .т.ч. конфигурирования для высокой доступности и масштабирования; Знание и опыт применения протоколов интеграции REST, SOAP (GraphQL как плюс); Опыт обработки неструктурированных данных; Опыт разработки собственных дата-процессоров , навыки программирования на Java; Навыки работы с СУБД PostgreSQL( Greenplum и стек Arenadata как плюс), опыт проектирования БД, хорошие знания SQL ; Опыт работы с брокерами сообщений Kafka,Docker или Docker-compose.  Условия:  Интересная работа в динамичной ИТ компании; Комфортный офис в шаговой доступности от м. Угрешская; Пятидневная рабочая неделя пн-пт. с 10:00 до 19:00; Формат работы: удаленный формат работы или гибридный формат работы; Оформление в соответствии с ТК РФ (бессрочный трудовой договор, оплата больничных листов, отпуск 28 к.д.); Система бонусов по итогам работы за год по результатам kpi; Полис ДМС после испытательного срока;  Компенсация затрат на мобильную связь;    "
"69709830","Группа компаний С7","Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Kubernetes', 'Git', 'Python', 'PostgreSQL', 'Kafka', 'Java', 'SCALA', 'Golang']","S7 TechLab – IT компания, реализующая инновационные IT продукты для подразделений группы компаний S7. NLP чат-боты для поддержки пользователей, компьютерное зрение для бортов самолетов, прогнозирование загрузки самолета – это лишь некоторые примеры типовых задач, которые реализуются силами команды S7 TechLab и помогают авиакомпании занимать лидирующее место на рынке авиаперевозок России. В команду Aircraft Data Warehouse S7 ищем Data Engineer уровня middle+ и выше. ADW (Aircraft Data Warehouse) - доменное хранилище данных с информацией по воздушным судам. Мы подключаем интересные технические источники данных авиакомпании (информация по рейсам, телеметрия с бортов самолетов, данные из системы бронирования) и передаем их бизнесу, чтобы им было проще решать свои важные задачи. Задачи:  Разработка процессов получения, проверки, обработки, хранения и транспортировки данных;  Проектирование баз данных;  Разработка сервисов обработки данных; Разработка API; Участие в настройке и отладке CI/CD (как с нуля, так и существующих); Участие в настройке мониторингов, аллертов (как с нуля, так и существующих); Участие в проектировании системы и процессов обработки, валидации данных, выравнивания данных;  Пожелания к кандидату:  Опыт работы с PostgreSQL – обязательно – разработка SQL-запросов разного уровня сложности (от SELECT * до оконных функций с самописными процедурами), настройка триггеров, процедур до использования сторонних библиотек, опыт администрирования PostgreSQL приветствуется; Опыт работы с ORACLE – умение разобраться в SQL-запросах разного уровня сложности; Опыт работы с колоночными БД – желателен; Владение Python – обязательно, Java/Scala/Go – желательно, опыт оптимизации кода - приветствуется; Опыт разработки ETL Airflow/Spark; Знание Git/GitLab; Понимание принципов и алгоритмов обработки данных; Опыт работы с разнообразными источниками данных (FTP/SFTP, S3, Samba, Kafka, сайты, API) Опыт разработки KAFKA (топики, коннекторы), K-Streams; Опыт парсинга текста, сайтов, XML, HTML, бинарных файлов; Опыт обработки данных и конвертации разнообразных форматов – CSV, XLSX, TXT, PARQUET, PDF и др., HDF5 – желательно; Знакомство с KSQL, Flink; Опыт работы с K8S, опыт разворачивания приложений в Kubernetes (Желательно - OpenShift), Опыт развертывания Airflow в K8S от сборки контейнера до настройки конфигурации; Опыт разработки Operators, Hooks Airflow – приветствуется; Опыт работы с HELM – желательно. Опыт настройки CI/CD; Опыт разработки API (REST); Знакомство с Zabbix, Prometheus; Знакомство с KSQL, Flink; Опыт разработки разнообразных тестов – приветствуется;  Что мы предлагаем:  Приветственный мерч новичкам; ДМС; Корпоративную жизнь, насыщенную разнообразными коллективными событиями; Подарки для детей к первому классу и на Новый год; Профессиональное развитие: проходить обучение в компаниях-партнерах, участие и выступление в конференциях. И да! У нас есть партнёр, который готовит к выступлениям (поможет определиться с тематикой, посылом, составить доклад и подготовит к выступлению); Онлайн трансляции в режиме вопрос-ответ с приглашенными интересными гостями, например Алексей Водовозов, главы ИТ-подразделений Nvidia, Dodo pizza, ученые. У нас в компании Agile практика. Если интересно данное направление вместе с нашим экспертом готовы помочь нарастить эти скиллы; Возможность путешествовать по специальным корпоративным тарифам для вас и вашей семьи. Например, туда-обратно: Сочи - 3 200 рублей. Корпоративные скидки в PrimeZone: скидки в рестораны, на обучающие курсы, товары и тд.; Самостоятельность и широкий диапазон влияния на продукт. Возможность существенно улучшать и изменять, предлагать идеи — всё это можно и нужно; Команда распределенная, работаем удаленно, либо у нас есть 3 уютных офиса: г. Москва Петровка 7, г. Краснодар или г. Иннополис :) ИТ аккредитованная компания; Полное соблюдение Законодательства Российской Федерации. "
"67381072","Зетта Страхование","Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'Python', 'Sas', 'Анализ данных', 'Работа с базами данных']","Обязанности:   Систематизация и подготовка данных для актуарных моделей;   Выгрузка актуарных отчетов для всех линий бизнеса;   Построение дополнительных отчетов и аналитик для андеррайтинга;   Поиск и подготовка факторов для построение актуарных моделей;   Построение и ведение внутренней БД для нужд актуариев;   Обработка и приведение внутренних данных кампании и внешних источников в нужный для анализа вид.   Требования:   Опыт обработки данных с использованием SQL/Python/SAS   Опыт внедрения и поддержки БД на основе внутренних источников и получаемых внешних данных   Опыт работы с современными форматами хранения данных (json, xml и тд)   Ответственность, внимательность, аккуратность, энтузиазм к работе, желание развиваться профессионально, аналитический склад ума, умение работать в команде, стремление к оптимизации процессов, целеустремленность, исполнительность, самостоятельность, практичность, умение искать свои и чужие ошибки;   Умение проверять свою работу и находить ошибки;   Умение находить нестандартные / компромиссные решения.   Условия:  Оформление полностью в соответствии с ТК РФ - трудоустройство с первого рабочего дня, выплата заработной платы два раза в месяц, оплачиваемый отпуск и больничный; Реальные возможности профессионального и карьерного роста; Стабильная окладная часть и квартальные премии; Наставничество, корпоративное обучение и тренинги; График работы: пятидневная рабочая неделя; Полис Добровольного медицинского страхования - бесплатное медицинское обслуживание в лучших клиниках города; Корпоративные скидки на страхование для сотрудников; Активная корпоративная жизнь: конкурсы, тимбилдинги, участие в социальных мероприятиях; Подарки для детей сотрудников на Новый год. "
"69329472","SberAutoTech","Middle Data Engineer (AI Driving Data)","False","None","None","От 3 до 6 лет","Гибкий график","['Linux', 'Python', 'Git', 'Spark', 'ETL', 'ML']","У команды SberAutoTech есть цель, и это – революция в мире автопрома в самом ближайшем будущем. Нам срочно нужны единомышленники! Мы осознаем, что прорыв всегда делает меньшинство, потому что не все готовы рискнуть творить историю и двигать вперед целую индустрию. Общающиеся между собой автомобили, сервисы и приложения, беспилотный транспорт и электрокары – это даже не будущее, а настоящее! Следующий шаг – сделать такие автомобили обычным явлением в нашей жизни. Если интересны технологии на стыке автопрома и интернета, если мечтаешь быть причастным к созданию чего-то нового и масштабного – присоединяйся к нашей команде. Мы ищем Data Engineer~a, который поможет нам вывести работу с большими данными на новый уровень. Результаты твоей работы будут напрямую влиять на скорость развития беспилотной технологии. Наш стек: Spark/Spark Structured Streaming, SQL, Python, Hadoop. Проект: AI Driving Data. Тебе предстоит организовать обработку данных, которые генерируют наши беспилотники, а также участвовать в создании хранилища для этих данных. Классы задач, которые необходимо будет выполнять:   Участие в проектирование и разработке ETL пайплайнов для Spark/Spark Structured Streaming;   Расчет требуемых ресурсов для Spark джобов, оптимизация нагрузки на кластер;   Разработка новых и оптимизация действующих Spark SQL запросов, в том числе Spark Structured Streaming queries;   Обеспечение качества данных в хранилище, построенном на базе Hadoop HDFS;   Взаимодействие с ML специалистами в процессах построения витрин данных/инструментов для работы с данными.   Что мы ждем от кандидата:  Опыт разработки на Python от 3х лет; Опыт создания и поддержки ETL-процессов; Способность писать чистый и понятный код, покрытый тестами; Знание алгоритмов; Понимание модели MapReduce.  Будет плюсом++  Опыт работы с Airflow/Dagster; Опыт работы с Kafka; Опыт разработки на C++/Scala; Опыт проектирования и разработки распределенных отказоустойчивых систем.  Мы ценим своих сотрудников и предлагаем:  Крутой технологический домен; Работа в команде топовых разработчиков, возможность разрабатывать уникальные и крупные проекты масштаба нашей страны; Конкурентные условия труда (белая индексируемая заработная плата, оклад+годовая премия); График работы – стандартный, но с гибким подходом к началу/окончанию рабочего дня. На период онбоардинга - офисный формат; Доступ к огромным возможностям повышения квалификации в СберУниверситете и Виртуальной школе, а также к другим формам обучения; Возможность посещения (как в качестве слушателя, так и в качестве выступающего) всероссийских и международных конференций; ДМС со стоматологией для сотрудников с первого дня и скидки на медицинскую страховку для родственников; Обеды для сотрудников, бесплатный кофе и другие напитки в кафетерии; Зарплатный проект, бесплатная подписка Сберпрайм+, субсидия на ипотеку и другие продукты Экосистемы Сбера на особых условиях; Широкий спектр дисконт–программ, скидок и привилегий от компаний-партнеров; Возможность уже сейчас использовать беспилотный транспорт, чтоб добраться до работы от МЦК ЗИЛ. Большой и комфортный офис со спортзалом, кинозалом, библиотекой, столами для пинг-понга, кафе для сотрудников; Льготное кредитование в Сбербанке – возможность пользоваться премиальными продуктами Банка на специальных условиях. "
"69616664","WILDBERRIES","Senior Data Engineer (Antifraud Team)","False","None","None","От 3 до 6 лет","Полный день","['Python', 'ClickHouse', 'Redis', 'Kafka', 'k8s', 'NATS']","Привет! Мы - Wildberries. Мы - это крупнейший маркетплейс Европы, 30 млн пользователей ежедневно и технологические задачи, у которых нет аналогов. Сейчас мы ищем опытного инженера данных в нашу команду антифрода. С какими задачами придется столкнуться:  управлять инфраструктурой команды; разрабатывать, поддерживать и развивать как текущие, так и новые решения по доставке ML моделей в тест\прод среду; участвовать в проектировании системы и принятии архитектурных решений совместно с командой разработки; разрабатывать API для высоконагруженных сервисов получения доступа к данным, участвовать в разработке data-pipeline на всех этапах - от обсуждения с источниками данных до предоставления фичей из данных в рабочую модель.  Будет классно если ты:  имеешь опыт проектирования систем обработки данных; хорошо знаешь Python; умение не только следовать чужим решения, но и предлагать свои; знаком с микросервисной архитектурой; имел опыт в Clickhouse\k8s\Kafka\Redis\NATS и подобных технологиях.  Что есть у нас и чем готовы делиться:  Гибрид или офис с диванчиками, гамаками, качелями, кафе и зелеными зонами. Мы находимся на метро Автозаводская; ДМС после испытательного срока; Полностью белая зарплата (да, обыденность, но все же). Работа над интересными проектами, которые прокачают твои технические навыки и аналитическое мышление; Корпоративные скидки у партнеров компании; Возможность попасть в сильную команду крупнейшего маркетплейса России. Мы готовы выслушать любые твои предложения и самые смелые идеи! "
"69813289","Volna.tech","Senior Data engineer","False","None","None","От 3 до 6 лет","Удаленная работа","['DWH', 'ETL', 'ORACLE', 'MPP', 'MS SQL', 'PostgreSQL']","Мы российская быстрорастущая платформа, созданная для взаимодействия разработчиков из России и ведущих компаний России, Европы и Америки.Сейчас мы в поиске Data Engineer (DWH 3.0) на новый банковский проект Одно из наших преимуществ - это оперативный процесс найма. Обычно 1-2 собеседования независимо от компании. А также большой пул проектов. Обязанности: —Профилирование/Понимание данных в системах источниках; —Разработка ETL/ELT; —Разработка слоев данных DWH, витрин DM; —Описание разработок в Confluence; Оформление работ в Jira; —Участие в выборе технологий, компонентов, архитектуры решений; —Внедрение реализованных решений; Требования: — Уверенные навыки работы с одной современной СУБД (GreenPlum, PostgreSQL, MS SQL Server, Oracle, Teradata, и т.д.); — Хорошее знание SQL, опыт написания и оптимизации SQL запросов, процедур, представлений; — Желателен опыт разработки и сопровождения ETL процессов; — Желателен опыт разработки и внедрения корпоративных хранилищ данных (DWH); — Рассматриваем кандидатов с опытом развития решений в банковских АБС, желающих развиваться в области хранилищ данных (DWH); — Желателен опыт работы в банковской сфере; — Умение анализировать поставленные задачи. У Вас будет возможность участвовать в разработке/развитии решений как на базе современных Open Source решений, так и на базе &quot;фундаментальных&quot; Enterprise продуктов, лидирующих в &quot;квадрате&quot; Gartner. Условия:   удалённая работа с первого дня;  карьерный рост; job offer, отталкиваясь от ваших финансовых пожеланий; оформление по ТК РФ "
"69494207","ОКБ","Senior Data Engineer","False","None","None","От 3 до 6 лет","Удаленная работа","['Hadoop', 'Python', 'Big Data', 'Linux', 'GitLab', 'SCALA', 'SQL', 'Kafka', 'Apache Spark', 'ORACLE', 'Greenplum', 'Apache NiFi']","Объединенное Кредитное Бюро – крупнейшее Бюро кредитных историй в России. Мы не выдаем кредиты, мы храним и обрабатываем крупнейший в России массив данных о кредитных историях. Наш уникальный ресурс – самая большая база данных, в которой более 560 миллионов кредитных историй, и мы уделяем большое внимание не только количеству данных, но и качеству их обработки. Мы помогаем нашим частным клиентам контролировать свою кредитную историю и получать лучшие финансовые предложения, а компаниям – принимать взвешенные решения и оценивать риски на основе аналитики данных. Мы строим нашу инфраструктуру данных без легаси с использованием современного стека и подходов. Мы занимаемся имплементацией и развитием DataLake gen 3 и аналитической платформы данных, строим витрины данных, внедряем лучшие практики обработки и доставки данных в компании, реализуем MLOPS, следим за качеством и «чистотой» данных. Чем предстоит заниматься:  Разрабатывать пайплайны загрузки данных из различных источников (РСУБД, NoSQL, files, streaming) в Datalake; Реализовать PoC (MVP) с использованием новых инструментов и технологий; Оптимизировать процессы хранения и обработки данных с использованием современных технологии и подходов; Реализация распределенных алгоритмов на больших данных; Продукционализация моделей машинного обучения.  Наши ожидания:  Профильное образование: информационные технологии, статистика, математика; Опыт коммерческой разработки от 5 лет; Основной язык программирования – Scala/Python; Отличные навыки - SQL; Опыт работы с – Apache Spark, Apache Spark structured streaming, Flink, Kafka; Опыт работы с экосистемой кластера Hadoop (CDP, Arenadata, Hortonworks); Опыт работы с БД: Oracle, Postgre, Greenplum, HBase; Опыт работы с Apache Airflow и Apache NiFi.  Будет плюсом:  Опыт работы с табличным форматом хранения данных Delta lake; Опыт работы разработки приложений распределенных вычислений и обработки данных на Apache Flink; Опыт работы с Data science в части доставки данных и продукционализации моделей машинного обучения; Опыт построения Data lineage; Опыт работы с БД: Cassandra, Redis; Опыт работы с Feast.  Мы предлагаем:  Достойную твоего профессионального уровня зарплату и очень приятный годовой бонус; Возможности для твоего неизбежного развития: тебя ждут сложные и интересные задачи с использованием большого массива данных, внешнее и внутреннее обучение; Комфортные условия труда: работа из дома или из офиса ст. м. Павелецкая (5 минут от метро); Гибкий график: начало рабочего дня - с 8:00 до 11:00 - выбираете сами; Современную технику для комфортной работы: выдаём ThinkPad; Заботу о здоровье: оформим полис ДМС со стоматологией, дадим 3 дополнительных дня к отпуску, компенсируем затраты на абонемент в зал; По-настоящему дружескую атмосферу: поддерживаем, когда вы приводите своих друзей, и выплачиваем реферальный бонус. "
"69465846","Hi, Rockits!","Middle/Senior Data Engineer","True","None","230294","От 1 года до 3 лет","Удаленная работа","[]","Мы ищем Middle/Senior Data Engineer в международную компанию в сфере Digital Health, которая занимается разработкой формул лекарственных препаратов, поиском биомаркеров старения и увеличением продуктивного долголетия. Мы создаем централизованное хранилище данных всей компании. Много источников бизнес данных, биологических, биоинформатических и научных данных, каждые из которых имеют свою специфику. Стек: AWS, Airflow, RDS (Postgres), DocumentDB (MongoDB), OpenSearch (ElasticSearch). Задачи:  • Разработка pipelines обработки данных;• Анализ новых источников данных, настройка процессов их загрузки в Data Lake;• R&amp;D, реализация пилотов по выбору технологий и решений;• Создание витрин данных;• Развитие платформы данных, в т.ч. Data Quality. Мы ждём от кандидатов: • Знание Python;• Опыт работы на позиции Data Engineer от 2-3 лет;• Опыт разработки ETL пайплайнов;• Уверенные знания Linux, Git, Docker, понимание CI/CD процессов;• Опыт работы с AWS и Airflow;• Знание SQL и умение писать оптимальные запросы. Мы предлагаем: • удалёнку и гибкий график;• вилку до $4500 gross;• ДМС со стоматологией;• курсы английского и китайского;• оплата доп. образования и конференций;• бонусы по итогам успешной работы. НЕОБХОДИМО НАХОЖДЕНИЕ ЗА ПРЕДЕЛАМИ РФ  "
"69607303","Aston","Data Engineer (Middle)","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'DWH', 'ETL', 'PostgreSQL', 'Sas', 'ClickHouse', 'ORACLE', 'Data Engineer']","IT-компания Aston приглашает к сотрудничеству Data Engineer.   Задача – развитие корпоративного хранилища данных для различных бизнес-задач.   Заказчик – консалтинговая компания, которая является одним из лидеров мирового рынка профессиональных услуг и цифровых технологий, уже 15 лет входит в рейтинг Fortune Global 500. Технологический стек: DWH, SQL, GreenPlum, ETL\ELT. Обязанности:   анализировать данные SAS, определять источники и подготавливать требования для разработки витрин в EDW (Greenplum); готовить прототипы витрин и согласовывать с клиентом; оформлять сопутствующую документацию; тестировать результаты разработки.  Каким мы видим подходящего кандидата:  опыт работы в роли Data Engineer от 2-х лет; опыт работы в DWH от 2-х лет; владение SQL (фильтрация по агрегатам, аналитические, оконные функции); опыт работы с Hive/ClickHouse/GreenPlum/PostgreSQL/Oracle/MS SQL; опыт в разработке конечных витрин с данными согласно требованиям; опыт в разработке ETL\ELT-решений.  Что мы предлагаем?  долгосрочные проекты от наших российских заказчиков и партнеров, с которыми мы сотрудничаем с 2007 года; возможность выбора формата работы (дистанционно или из офиса в Питере/Казани/Ростове-на-Дону); оплачиваемый бенч; наставника, соответствующего вашему техническому уровню; системы менторства и адаптации для новых сотрудников; прозрачные системы performance review; возможность добавить в СV работу в команде с лидерами FinTech, Healthcare, Retail, Telecom и других; возможность выбора/смены проекта; у нас своя школа архитекторов, а также корпоративный обучающий портал для любого стека вашей специализации; медицинское страхование (+стоматология); возможность дополнительного заработка через участие в активностях компании.  Присоединяйтесь к команде Aston!"
"70508862","ЕВРОЦЕМЕНТ груп","IT Специалист/ Data Science Engineer (RPA)","False","None","None","От 1 года до 3 лет","Полный день","['Управление проектами', 'Базы данных', 'Data Analysis', 'Предиктивная аналитика', 'Машинное обучение', 'PRA', 'Python', 'SQL', 'Аналитическое мышление', 'API', 'Machine Learning', 'Анализ данных', 'Автоматизация процессов']","Чем предстоит заниматься:  ​​​​Участие в кросс-функциональных проектах компании, взаимодействие с внутренними командами, отраслевыми IT-вендорами и консультантами по вопросам разработки, внедрения наиболее перспективных цифровых решений, основанных на технологиях машинного обучения и других методах анализа данных; Разработка моделей и алгоритмов, оптимизирующих процессы работы по ключевым производственным переделам цементных предприятий; Автоматизация/роботизация рутинных процессов (операций) с применением платформы роботизации; Создание автоматизированных отчётов на языке программирования Python; Написание запросов SQL; Участие в проверке гипотез с помощью статистических методов для поиска ключевых инсайтов, идентификации трендов, измерения производительности, подтверждение экономических эффектов; Тиражирование разработанных решений (роботов, моделей) и алгоритмов по предприятиям Холдинга.  Что мы ждем от кандидата:  Высшее техническое образование (автоматизированные системы управления производством, инженерное); Опыт работы не менее 2 лет в решение прикладных DS задач, аналитических задач; Опыт реализации не менее 2-3 проектов по машинному обучению и/или предиктивной аналитике; Опыт разработки моделей машинного обучения для решения промышленных задач (желательно); Опыт реализации проектов RPA; Уверенное владение Python и его стека библиотек для машинного обучения; Опыт работы с базами данных, уверенное знание SQL, API и навык писать оптимизированные запросы к БД; Наличие реального опыта построения предиктивных моделей и запуска на prod-контуре; Понимание специфики решения индустриальных задач; Умение интерпретировать результаты и понятно презентовать результаты анализа; Опыт в исследовательских проектах (желательно); Ориентация на практику, а не только на теорию; Понимание технологий обработки больших данных (желательно); Нейросети (желательно); Навыки коммуникации, желание работать в кросс-функциональных командах.  Что мы предлагаем:  Достойная заработная плата; Стабильная заработная плата 2 раза в месяц; Корпоративное обучение, возможности профессионального и карьерного роста; ДМС широкого спектра со стоматологией после испытательного срока; Возможность выбрать начало дня с 8/9/10 утра; Комфортабельный офис класса А на территории которого находятся фитнес-клуб/аптеки/магазины/столовые/кафе/парковые зоны; Корпоративные мероприятия, подарки для детей сотрудников; Исключается возможность удаленной работы.  ​​​​​​​Локация и транспортная доступность:  Офис м. Славянский бульвар/ БКЛ Давыдково БЦ &quot;Верейская Плаза 4&quot;; От м.Славянский бульвар доставка корпоративным транспортом – 10 минут; От БКЛ Давыдково общественным транспортом - 10 минут. "
"67136889","СБЕР","Junior Data Engineer","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'Hive', 'Hadoop', 'Аналитическое мышление', 'Spark']","Ключевые задачи: - Проектирование и разработка витрин данных на Greenplum/Teradata (и в перспективе Hadoop (Hive, Spark)) - Расчет сложных аналитических показателей в витринах данных - Разработка и поддержка ETL-процессов загрузки данных в/из хранилища с использованием Python - Контроль качества загружаемых данных (реализация сложных проверок качества, проведение корректировок данных большого объема) - Поиск и предоставление данных по запросу внутренних и внешних заказчиков   Навыки: - Опыт построения корпоративных хранилищ данных (DWH, DL), включая разработку ETL-процессов - Понимание планов запросов, навык их оптимизации. - Python (или хороший опыт в других языках программирования высокого уровня) - Продвинутые аналитические навыки и вариативное видение решения задач   Мы предлагаем:  Deep diving в предметную область, много разработки по задачам имеющим прямой эффект на бизнес; Возможность привносить новые идеи и нестандартные решения; Сообщество D-people– поддержка, развитие и возможность учиться у профессионалов; Достойную оплату труда; Возможность расти в самой крупной DS-команде – пересматриваем доход каждые полгода)); Возможности саморазвития: оплата курсов популярных платформ, периодика; ДМС, сниженные ставки по кредитованию, программы лояльности для сотрудников, льготная ипотека; Скидки на услуги экосистемы Сбера и компаний-партнеров.    Приходи к нам, расти вместе с нами!  "
"70483731","РОСБАНК","Data engineer (Data Dictionary)","False","None","None","От 1 года до 3 лет","Удаленная работа","[]","Приглашаем Data engineer присоединиться к команде ПАО Росбанк. В Росбанке возможен полностью дистанционный формат работы: откуда работать - решать только тебе! Приходи к нам, чтобы работать над амбициозными и интересными проектами и наслаждаться балансом между работой и личной жизнью! Росбанк – частный универсальный банк, работающий на российском рынке с 1993 года. Росбанк обслуживает порядка 1,5 млн активных розничных клиентов, около 83 тысяч активных клиентов малого бизнеса и около 10 тысяч активных клиентов корпоративного бизнеса более чем в 60 регионах России. Росбанк входит в ТОП-3 рейтинга самых надежных российских банков по версии журнала Forbes и включен Банком России в число 13 системно-значимых кредитных организаций России. Наш банк имеет наивысшие кредитные рейтинги российских и международных рейтинговых агентств. Обязанности:  Разработка и поддержка in-house решения для дата каталога; Исследование данных в банковских информационных системах и базах данных; Анализ и разработка алгоритмов проверки качества данных (ETL, справочники, сущности, взаимосвязи); Разработка алгоритмов повышения качества данных за счет переиспользования данных из других источников или привлечения поставщиков данных; Формулировка задач по доработкам систем с целью улучшения качества данных; Разработка автоматических контролей; Разработка структуры и сопровождение справочной информации; Разработка дашбордов для мониторинга метрик качества данных; Исследование перспективы развития продукта и формировать предложения по улучшениям.  Требования:  Высокий уровень знания SQL, Python; Опыт работы с большими объемами данных; Опыт работы с СУБД Oracle/MS SQL/PostgeSQL, знание PL SQL/T-SQL; Владение алгоритмами проверки качества данных; Ответственность за результат, гибкость, системное мышление, внимание к деталям, клиентоориентированность, креативность.  Условия:  Уникальная возможность – полностью дистанционный формат работы. Откуда работать – решать только тебе! Стабильный и прозрачный доход: зарплата и премии по результатам работы; График 5/2; 31 календарный день оплачиваемого отпуска; Забота о здоровье сотрудников: программа ДМС, включая стоматологию и страхование при выезде за рубеж, корпоративные спортивные команды и скидки на абонементы в фитнес-клубы; Дополнительная оплата за больничный лист: доплата сверх пособия по болезни до оклада (до 5 рабочих дней в год); Личностное развитие и рост: корпоративная электронная библиотека Альпина, возможность прохождения бесплатного обучения и тренингов, участия в регулярных встречах корпоративных клубов и скидки на курсы он-лайн школ; Скидки и привилегии по кредитам, ипотека на льготных условиях, льготные условия обслуживания и т.п.; Программа корпоративных скидок и привилегий Primezone: развлечение, отдых, товары и услуги, обучение; Программа волонтерства и благотворительности; Сообщества по интересам сотрудников. "
"68093905","Строительный Двор","Data Engineer (Junior)","True","40000","100000","Нет опыта","Полный день","['Python', 'SQL', 'Sas', 'Linux', 'PostgreSQL', 'ETL', 'DWH']","Ищем специалиста в области Data Engineering для решения задач в области DIY ритейла, е-commerce для крупной компании с развитой сетью филиалов. Среди задач – участие в построении архитектуры и развертывании новой аналитической платформы, разработка и оптимизация ETL процессов, построение витрин данных, подготовка данных для моделей машинного обучения во взаимодействии с командой Data Science, создание отчетов и дашбордов для бизнес-пользователей и др. Приветствуются умение работать в команде, стрессоустойчивость, умение объяснить полученные результаты, желание развиваться в различных аспектах Data Engineering: от администрирования сервисов, разработке ETL пайплайнов до построения витрин данных и отчетов. Обязанности:  Анализ исходных данных в различных системах для решения бизнес-задач (оценка структуры, качества, полноты и применимости данных) Разработка комплексных ETL процессов для разнородных систем и СУБД Мониторинг, оптимизация и поддержка ETL процессов Обеспечение и контроль качества данных Построение и развитие витрин данных Участие в разработке аналитической отчетности Презентация результатов работы бизнесу  Требования:  Высшее / незаконченное высшее техническое образование Владение языками программирования Python или SAS Base Уверенное знание SQL (Join`ы, агрегаты, группировки, вложенные запросы, аналитические функции); Знание основ дискретной математики и теории реляционных СУБД Понимание принципов аналитической обработки данных в хранилищах данных Аналитический склад ума, системное мышление, ответственность, хорошие коммуникационные навыки Желание быстро овладевать новыми навыками и знаниями Готовность работать неполную или полную рабочую неделю: 30-39 ч  Будет плюсом:  Наличие выполненных проектов в области Big Data, BI Опыт работы с СУБД; Понимание процессов ETL, ELT; Понимание общей архитектуры хранилищ данных (DWH) наличие e-learning сертификатов (Coursera, Udemy, Edx) о прохождении обучения по технологиям data engineering  Стек используемых технологий:  SAS Viya, SAS Base, HP Vertica, SAP, MS SQL Server, PostgreSQL, Airflow, Python, Linux/Bash, Hbase, Kafka, Prometheus, Grafana   "
"69722002","Х5 Group","Data Quality Engineer / Data Analyst","False","None","None","От 3 до 6 лет","Удаленная работа","['SQL', 'Python']","В Дирекцию по управлению данными X5 Технологии открыта позиция Data Quality Engineer / Data Analyst. Данные — это основа нашей цифровой трансформации. Мы создаем, обогащаем и обрабатываем петабайты данных каждый день. На их основе мы принимаем сотни важных решений оптимизируем товарные остатки, прогнозируем спрос, подбираем наилучшие цены для товаров и сотни других кейсов. Мы ждём коллег, которые готовы менять этот мир к лучшему, используя большие данные! У нас ты сможешь поработать с Apache BigData Stack (HDFS, Hive, Spark, Airflow, Kafka), с базами данных (Hive, ClickHouse, Greenplum, PosgreSQL) и таким ПО как Ataccama (Data Quality). Задачи, которые тебе предстоят:  Поиск, анализ, профилирование данных; Разработка прототипа конечной витрины данных; Участие в разработке автоматизированного процесса поставки данных (data pipeline); Развитие системы мониторинга проверок качества данных; Реализация инновационных продуктов компании; Проработка входящих требований к витринам данных и подготовка сопровождающей технической документации; Взаимодействие с ключевыми IT и бизнес-подразделениями компании.  Мы ждем тебя, если ты имеешь:  Аналитический склад ума; Высокий уровень владения SQL (join, группировки, фильтрация по агрегатам, аналитические функции); Опыт работы с одним, или несколькими из перечисленных баз данных: Hive, ClickHouse, GreenPlum, PostgeSQL, Oracle, MS SQL; Опыт в разработке конечных витрин с данными согласно требованиям заказчика; Опыт в роли аналитика данных приветствуется; Опыт в разработке ETL\ELT-решений приветствуется; Знание стека технологий hadoop, Spark, Python, Airflow, Kafka, как преимущество.  Мы предлагаем:  Удобный офис у м. Парк Культуры или удаленную работу; Гибкий график работы (с 8/9/10/11 утра); Возможность обучаться и сертифицироваться за счёт компании: внешние тренинги и семинары по профессиональным тематикам, отраслевые конференции, программа развития управленческих навыков, очные мастер-классы, платформы онлайн-образования и многое другое; Яркую корпоративную жизнь с большим количеством мероприятий, конкурсов и возможностей для творческой реализации; Развитую систему компенсаций и льгот; Широкий пакет ДМС (включая выезд за рубеж и стоматологию), страхование жизни и здоровья; Скидки в магазинах сети Х5 («Пятёрочка», «Перекрёсток», «Карусель»); Программу привилегий Prime-zone (скидки на товары и услуги и специальные предложения от компаний-партнёров); Материальную помощь сотрудникам, попавшим в сложную жизненную ситуацию; Оформление по ТК РФ с официальной заработной платой.    Присоединяйся к одной из самых быстрорастущих цифровых команд России!  "
"69614008","Звук","Senior Data Engineer (DataPlatform team)","False","None","None","От 3 до 6 лет","Полный день","['Python', 'Spark', 'SQL', 'Контроль качества', 'Обучение и развитие']","Звук - аккредитованная ИТ компания Мы в поиске сильных DE в уютную команду, которая развивает мультиплатформенный аудиосервис на благо слушателя. Сейчас мы движемся от централизованной работы с данными в монолитных командах к платформенно-продуктовой парадигме (DataMesh) и нам предстоит большая работа по созданию этой самой платформы. Чем нужно будет заниматься?  Участвовать в трансформации ETL из родного overnight batching в модный молодёжный realtime streaming. Разрабатывать self-service инструменты платформы данных для реализации DataMesh. Интегрировать хранилища, каталог данных, dbt, CDC движок, DQ, мониторинги и прочее. Обучать младших коллег, помогать аналитикам, продвигать лучшие практики работы с данными.  А вот это нужно уметь:  Релевантный опыт работы от 2-х лет. Опыт создания ETL-процедур. Опыт работы с batch обработкой данных. Знание Python, SQL. Практический опыт работы с Hadoop, Spark, AirFlow, Presto. Знакомство с kubernetes и опыт его использования в работе.  Будет плюсом:  Опыт работы со stream обработкой данных. Знание Scala, опыт работы с Kafka и Flink. Опыт работы с CDC. Опыт внедрения инструментов контроля качества данных. Опыт создания self-service инструментов и платформенных решений. Опыт работы c datascience командой.  Работа должна быть в удовольствие, поэтому:   Мы аккредитованная ИТ-компания, все преимущества распространяются на наших сотрудников в полном объеме  Возможность участвовать в развитии продуктов, которыми пользуются миллионы Стильное рабочее пространство рядом с м. Кутузовская (7 минут от метро и МЦК) Возможность работать удаленно или в гибридном графике, выбирать начало и окончание рабочего дня, брать дополнительные дни отпуска ДМС с первого рабочего дня Заботу о психологическом здоровье и компенсацию затрат на платформе «Ясно» Полезные завтраки и перекусы в офисе каждый день Занятия в офисном спортзале, йогу и фитнес, компенсацию затрат на спорт Оплату участия в конференциях и помощь в подготовке выступлений, скидки на изучение английского, доступ к корпоративным обучающим ресурсам Возможность делиться новостями и экспертизой с коллегами на ежемесячных митапах и внутренних мероприятиях Ламповые вечеринки с пиццей и настолками под гитару, вечеринки артистов в офисе каждый месяц!  А еще, у нас есть своя музыкальная группа)  "
"67932769","HeadHunter","Data Quality Engineer","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'Python', 'PostgreSQL', 'СУБД', 'MS SQL', 'it']","Обязанности:  Проведение аудита качества данных и устранение недостатков Выявление проблем качества данных Настройка контролей качества данных Контроль исполнения инцидентов по качеству данных Разработка и внедрение системы метрик качества данных Консолидация, обновление, валидация необходимых метаданных Внедрение динамического расчёта пороговых значений для трендовых проверок Формирование документации Разработка системы контроля качества данных, моделей источников, потоков, конфиденциальности данных. Внедрение системы визуализации данных по качеству Консультирование владельцев данных по выявленным ошибкам Администрирование баз данных Оптимизация, проектирование, разработка и поддержка аналитических решений промышленного масштаба. Проектирование и разработка интеграционных решений  Требования:  Знание SQL на продвинутом уровне Опыт работы с MS SQL от 3 лет Опыт разработки пакетов SSIS Опыт работы с СУБД Postgres, Hive; Понимание принципов проектирования хранилищ данных Опыт работы с системами визуализации данных (Tableau, PowerBI и т.д.) Базовые знания Python Опыт написания ТЗ для доработок аналитических сервисов  Условия:  корпоративный ДМС (решаем вопросы со здоровьем быстро и удобно); корпоративная библиотека My Book; вам есть, что рассказать и чему научить, или хочется поучиться, — мы даём возможность участвовать; если ты активный участник корпоративной жизни, то несмотря на удаленку мы имеем массу активностей от лекций и коллективных занятий спортом онлайн до детского клуба и школы экологии;). "
"70326288","Playrix","Senior Data Engineer/Team Lead","False","None","None","От 3 до 6 лет","Удаленная работа","[]","Наша команда занимается разработкой и поддержкой программных продуктов для извлечения, трансформации, выгрузки и анализа данных. Объем данных в нашем Data Lake больше 2.5 петабайт: маркетинговые метрики, игровые события, параметры операционной деятельности. Мы делаем всё возможное, чтобы не возникало ни малейших сомнений в полноте, актуальности и достоверности информации, которую мы предоставляем. Особое внимание уделяем скорости обработки и качеству данных. Это позволяет нам принимать верные решения относительно развития наших игр! Для нас Tech Lead — это сильный технический специалист, который совместно с Product Owner может обеспечить стабильную работу команды, создающей внутреннюю платформу по автоматизации бизнес-процессов компании.   Наши задачи  архитектурная проработка, проведение PoC, разработка и улучшение наших сервисов; контроль выполнения задач, работа с рисками; найм, развитие, оценка и мотивация сотрудников; участие в составление Roadmap продуктов совместно с Product Owner; внедрение показателей для команды, сбор и анализ статистики, выявление узких мест; развитие процессов и их прозрачности для команды и внешних стейкхолдеров.    Наш стек Наш основной язык программирования - Python. Для доступа к данным используем SQL. Datalake построен на S3, Parquet и Glue. В качестве DWH используем Redshift/Postgresql. Работаем в облачной инфраструктуре AWS. Используем решения мейнстримовых вендоров (Databricks, MonteCarlo, DBT). Практикуем serverless подход работы с ресурсами и горизонтальное масштабирование, используем предиктивные модели. Уделяем внимание рефакторингу и кода, и архитектуры. CI/CD - TeamCity. .   Мы ожидаем  опыт управления командой от 2 лет (6+ человек); отличное знание Python, опыт разработки на нем, умение читать и анализировать чужой код; навыки написания и оптимизации SQL запросов, готовность прокачивать свои знания и писать более сложный код на SQL; широкие представления о средах и инструментариях разработки, технологиях, фреймворках; опыт разработки архитектуры.    Будет плюсом  опыт работы с большими данными или BI; знание предметной области маркетинга; опыт работы в любой облачной инфраструктуре (AWS, Google Cloud, Azure); опыт работы с системами управления ETL (Luigi, Airflow); знание Tableau, опыт работы с backend web-приложений (Django/Flask); аналитические навыки работы с данными.    Мы предлагаем Комфортные условия Работайте там, где удобно: удаленно, в офисе или совмещайте форматы. Без согласований и трекинга рабочих часов. Заботу о здоровье Компенсируем онлайн-сессии с психологом, открываем для вас и ваших детей ДМС со стоматологией и лечением от COVID-19. Организуем вакцинацию для тех, кому это важно. Спорт и фитнес Поддерживаем здоровый образ жизни и компенсируем покупку любых спортивных абонементов, подписку на фитнес-приложение или участие в марафонах. Заботу о благополучии Сохраняем 100% зарплату во время отпуска или больничного без лишних справок. А для особых случаев предоставим дополнительные выходные. Удобное рабочее место У нас есть все для комфортной работы, где бы вы не находились. В офисах – зоны отдыха и горячее питание, для удаленных сотрудников — доставка современной техники. Развитие и обучение Оплачиваем участие в профильных конференциях и курсах, регулярно проводим внутренние буткемпы и стажировки. А для личного развития предлагаем доступ к тысячам книг в онлайн-библиотеке и скидки на курсы английского языка. Развлечения и мерч Конкурсы, спортивные челленджи, вечеринки, хакатоны и внутренние офлайн-ивенты для команд — каждый год мы проводим сотни мероприятий по всему миру. Социальные проекты Запускаем благотворительные проекты и поддерживаем идеи сотрудников в конкурсе грантов."
"70679795","Rubytech","Администратор систем больших данных (Data Engineer)","False","None","None","От 1 года до 3 лет","Удаленная работа","['Linux', 'Hadoop', 'Kafka', 'Ansible', 'Zookeeper', 'Arenadata', 'Apache Kafka', 'ClickHouse', 'HBase']","Rubytech — российская ИТ-компания, которая помогает крупнейшим корпорациям и государственным организациям определять вектор развития, опираясь на современную масштабируемую технологическую платформу. Мы входим в двадцатку сильнейших игроков на отечественном ИТ-рынке (CNews топ-100). В нашей команде более 400 экспертов, за плечами которых больше 120 масштабных проектов. Компанию в 2020 году создал Сергей Мацоцкий, известный на ИТ-рынке инвестор, член совета директоров «Альфа-Банка», предприниматель, который стоял у истоков российского рынка интеллектуальных технологий и участвовал в создании известных технологических корпораций. Основатель и совладелец группы ИТ-компаний «ГС-Инвест», в состав которой входит Rubytech. Чем предстоит заниматься:  Сопровождение в части администрирования крупных государственных и частных проектов на базе продуктов Arenadata Hadoop, Arenadata Streaming, Arenadata QuickMarts, Arenadata Grid; Участие в коммерческих и пресейл-проектах по внедрению ландшафтов больших данных; Стендирование и тестирование новых сборок продуктов, участие в функциональных и нагрузочных тестированиях, документирование финальных технических решений; Решение инцидентов, анализ производительности, оценка работы сервисов, предоставление консультаций по обращениям заказчиков; Участие в комплексных проектах по миграции и адаптации больших данных из внешних источников информационных систем.  Наш кандидат должен иметь:  Уверенное знание ОС семейства Linux; Понимание и базовый опыт администрирования Apache Kafka, Zookeeper, ClickHouse, HBase или подобных решений и фреймворков; Базовый опыт сопровождения кластеров и экосистем Hadoop.  Будет преимуществом:  Опыт работы с AirFloW, Spark Streaming, Kafka, Clickhouse, Tarantool, Phoenix, Hbase, Arenadata Streaming, Arenadata Hadoop, Arenadata Quick Marts, Arenadata Grid, Arenatada DB; Знание методов развертывания конфигураций и владение инструментами Ansible, Puppet; Знание протоколов аутентификации Kerberos для ландшафта больших данных; Практический опыт в роли инженера\DevOps на интеграции систем; Базовое владение Java, SQL, Python; Опыт работы с noSQL базами данных Redis, MongoDB, Tarantool приветствуется.  Что мы предлагаем:  Комфортный офис в пешей доступности от м. Алексеевская; Стабильный конкурентный доход, который мы обсудим при встрече индивидуально; Прозрачную схему профессионального и карьерного роста; Профессиональное обучение и возможность прохождения сертификации за счёт компании; Индивидуальный план развития в рамках ежегодной аттестации; ДМС (поликлиника, стоматология, госпитализация); 50% компенсация Онкострахования; Компенсация ДМС для ваших детей; Скидки на фитнес в ближайших фитнес-клубах; Парковка / вело парковка на территории БЦ; Кафе и столовая на территории БЦ. "
"69493779","СБЕР","Data engineer, Транскрипции & витрины","False","None","None","От 1 года до 3 лет","Полный день","[]","Мы - команда корпоративно-инвестиционного блока Сбербанка Транскрипции &amp; витрины, которая занимается созданием алгоритмов автоматической аналитики коммуникаций с клиентом. Сервис, используя алгоритмы Machine Learning, анализирует миллионы коммуникаций: звонки, чаты и обращения. В нашу команду ищем стажера, который сможет подготовить и внедрить витрину для Облака данных. Обязанности Разработка и поддержка ETL пайплайнов по обработке данных. Требования  Высшее техническое образование / незаконченное высшее техническое образование, Опыт программирования Java/Scala, Понимание принципов функционального программирования, Опыт работы с базами данных, знание SQL, Пользователь Linux (работа в терминале, bash скрипты).  Будет плюсом:  опыт работы на стэке Python, Spark, Hadoop.  Условия  Место работы: м. Кутузовская, Кутузовский проспект 32 к1; Доступны режимы работы: офис, смешанный, дистанционный Оформление согласно ТК РФ; Социальный пакет: ДМС, спортзал (фитнес, йога), спортивные и культурно-массовые мероприятия, возможность обучения за счет компании, льготные условия кредитования; Отличная профессиональная команда "
"70696785","Cloud","Middle Python Developer / Data Engineer (Python) в команду ML Space Data Catalog","False","None","None","От 1 года до 3 лет","Удаленная работа","['Spark', 'Hadoop', 'Python', 'Linux', 'Docker']","Вам предстоит:  Разработка и поддержка существующих решений, связанных с ETL/ELT-процессами; Участие в проектировании, разработке и поддержке высоконагруженных сервисов/микросервисов. Code review, юнит-тесты; Взаимодействие со смежными командами для проработки общего технического решения. Документирование разрабатываемых продуктов.  Требования:  Навыки разработки cloud-native сервисов; Знакомство с Kubernetes, хорошие навыки работы с Docker; Отличное владение Python, умение работать с асинхронным кодом; Знание стека Big Data, понимание ETL/ELT процессов (ключевые слова: Apache Airflow, Spark Streaming, Hadoop, HDFS, Kafka); Опыт работы с Apache Airflow;  Большой плюс:  Навыки работы с Golang; Опыт создания пайплайнов данных / моделей машинного обучения. Ключевые слова: StreamSets/Ni-Fi, AWS SageMaker; Опыт интеграции решений Big Data.  Что мы предлагаем:  Оформление в соответствии с трудовым законодательством РФ; Конкурентный уровень дохода (оклад + годовой бонус); ДМС со стоматологией и возможностью подключения к программе своих детей и родственников; Прозрачную систему мотивации, которая позволяет влиять на уровень дохода; Работу в команде профессионалов; Участие в создании инновационных продуктов; Гибкое начало рабочего дня, пятница - сокращённый рабочий день; Возможность работать удаленно; Офис в центре Москвы; Корпоративную мобильную связь; Льготную программу ипотечного и потребительского кредитования.  Ещё у нас:  Возможность вертикального и горизонтального роста; Бонусные программы от компаний партнёров; Возможность получения бонуса за закрытие вакансии по вашей рекомендации; Материальная помощь при рождении детей и др. семейных обстоятельствах; Обучение в Корпоративном университете за счёт компании; Участие в профильных конференциях в качестве спикера или слушателя; Корпоративная жизнь: спортивные комьюнити, клубы по интересам (настолки, интеллектуальные игры. "
"70696901","Cloud","Senior Python Developer / Data Engineer (Python) в команду ML Space Data Catalog","False","None","None","От 3 до 6 лет","Удаленная работа","['Spark', 'Hadoop', 'Python', 'Linux', 'Golang']","Вам предстоит  Разработка и поддержка существующих решений, связанных с ETL/ELT-процессами; Участие в проектировании, разработке и поддержке высоконагруженных сервисов/микросервисов. Code review, юнит-тесты; Анализ инцидентов; Взаимодействие со смежными командами для проработки общего технического решения; Документирование разрабатываемых продуктов.  Требования:  Навыки разработки cloud-native сервисов; Знакомство с Kubernetes, хорошие навыки работы с Docker; Отличное владение Python, умение работать с асинхронным кодом; Знание стека Big Data, понимание ETL/ELT процессов (ключевые слова: Apache Airflow, Spark Streaming, Hadoop, HDFS, Kafka); Опыт работы с Apache Airflow; Опыт интеграции решений Big Data.  Большой плюс:  Навыки работы с golang; Опыт проектирования и разработки сервисов с большой нагрузкой с нуля; Понимание бизнес-процессов, умение преобразовывать бизнес-задачи product-менеджера в декомпозированные задачи; Опыт создания пайплайнов данных / моделей машинного обучения. Ключевые слова: StreamSets/Ni-Fi, AWS SageMaker.  Что мы предлагаем:  Оформление в соответствии с трудовым законодательством РФ; Конкурентный уровень дохода (оклад + годовой бонус); ДМС со стоматологией и возможностью подключения к программе своих детей и родственников; Прозрачную систему мотивации, которая позволяет влиять на уровень дохода; Работу в команде профессионалов; Участие в создании инновационных продуктов; Гибкое начало рабочего дня, пятница - сокращённый рабочий день; Возможность работать удаленно; Офис в центре Москвы; Корпоративную мобильную связь; Льготную программу ипотечного и потребительского кредитования.  Ещё у нас:  Возможность вертикального и горизонтального роста; Бонусные программы от компаний партнёров; Возможность получения бонуса за закрытие вакансии по вашей рекомендации; Материальная помощь при рождении детей и др. семейных обстоятельствах; Обучение в Корпоративном университете за счёт компании; Участие в профильных конференциях в качестве спикера или слушателя; Корпоративная жизнь: спортивные комьюнити, клубы по интересам (настолки, интеллектуальные игры). "
"70680695","Rubytech","Data Engineer (Большие данные)","False","None","None","От 1 года до 3 лет","Удаленная работа","['Linux', 'Hadoop', 'NiFi', 'Kafka', 'Ansible', 'Zookeeper']","Rubytech — российская ИТ-компания, которая помогает крупнейшим корпорациям и государственным организациям определять вектор развития, опираясь на современную масштабируемую технологическую платформу. Мы входим в двадцатку сильнейших игроков на отечественном ИТ-рынке (CNews топ-100). В нашей команде более 400 экспертов, за плечами которых больше 120 масштабных проектов. Компанию в 2020 году создал Сергей Мацоцкий, известный на ИТ-рынке инвестор, член совета директоров «Альфа-Банка», предприниматель, который стоял у истоков российского рынка интеллектуальных технологий и участвовал в создании известных технологических корпораций. Основатель и совладелец группы ИТ-компаний «ГС-Инвест», в состав которой входит Rubytech. Чем предстоит заниматься:  Участие в коммерческих и пресейл-проектах по внедрению ландшафтов больших данных; Стендирование и тестирование новых сборок продуктов, участие в функциональных и нагрузочных тестированиях, документирование финальных технических решений; Решение инцидентов, анализ производительности, оценка работы сервисов, предоставление консультаций по обращениям заказчиков; Участие в комплексных проектах по миграции и адаптации больших данных из внешних источников информационных систем.  Наш кандидат должен иметь:  Опыт администрирования и эксплуатации Linux систем от 2 лет, хорошее понимание принципов работы ОС и знание аппаратных серверных платформ HPE, Lenovo, Huawei, Yadro, Dell; Понимание и базовый опыт администрирования Apache Kafka, Apache NiFi, AirFloW, Zookeeper, Apache Solr, Spark Streaming, ClickHouse, HBase, Solar или подобных решений и фреймворков; Знание методов развертывания конфигураций и владение инструментами Ansible, Puppet; Базовый опыт сопровождения кластеров и экосистем Hadoop.  Будет преимуществом:  Знание протоколов аутентификации Kerberos для ландшафта больших данных; Опыт работы с реляционными СУБД PostgreSQL, MySQL и тд.; Практический опыт в роли инженера\DevOps на интеграции систем; Базовое владение Java, SQL, Python приветствуется; Опыт работы с noSQL базами данных Redis, MongoDB, Tarantool приветствуется.  Что мы предлагаем:  Комфортный офис в пешей доступности от м. Алексеевская; Стабильный конкурентный доход, который мы обсудим при встрече индивидуально; Прозрачную схему профессионального и карьерного роста; Профессиональное обучение и возможность прохождения сертификации за счёт компании; Индивидуальный план развития в рамках ежегодной аттестации; ДМС (поликлиника, стоматология, госпитализация); 50% компенсация Онкострахования; Компенсация ДМС для ваших детей; Скидки на фитнес в ближайших фитнес-клубах; Парковка / вело парковка на территории БЦ; Кафе и столовая на территории БЦ. "
"69498443","СБЕР","Senior Data engineer (Цифровой кредитный мониторинг)","False","None","None","От 3 до 6 лет","Полный день","[]","Мы приглашаем вас как Дата-инженера стать частью команды «Фабрика залогового мониторинга» (ФЗМ) Сбербанка. Залоговый мониторинг - направление корпоративного бизнеса, создающее инфраструктуру для анализа изменений в обеспечении, переданном банку юридическими лицами. Информация о проекте: Мы ищем человека, способного вместе с нами создавать и развивать сервисы Залогового мониторинга юридических лиц. Наши задачи – автоматизация процессов залогового мониторинга, интегрируясь с множеством внутренних и внешних источников и автоматизированными системами банка. Над продуктом работает кросс-функциональная команда (Scrum), в которой представлены компетенции: front-/back-end разработчики, data-инженеры, системные аналитики, бизнес аналитики (CJE) Обязанности · разработка кода подготовки данных для моделирования и различных систем Банка · разработка витрин данных на стеке Hadoop/Hive/Spark. Требования  опыт разработки на одном из языков: Python/Java/Scala от 1 года знание основных алгоритмов и структур данных отличное знание SQL - Join`ы, агрегаты, группировки, вложенные запросы, индексы, хранимые процедуры, оконные функции знание форматов данных: сsv и разновидности, json, yaml, parquet, orc инструменты для организации и автоматизации работы: ide, git, Jira знание OLTP, OLAP, ACID, Теорема CAP знакомство с bash: базовые команды Unix.  Приветствуется: · знакомство с bash: базовые команды для работы с файлами и процессами (cd, cp, mv, ls, rm, grep, head, tail, tr, du, df, free, top, ps, kill, locate) - нужно для работы на кластере · опыт работы с Hadoop - Hive, Spark, HBase понимание работы со стеком DevOps будет плюсом."
"68074595","СБЕР (ООО еАптека)","Senior Data Engineer/Разработчик баз данных","False","None","None","От 3 до 6 лет","Гибкий график","['Hadoop', 'Big Data', 'Agile', 'Разработка продукта', 'MS SQL', 'SQL', 'Python', 'ETL', 'OLAP']","СБЕР ЕАПТЕКА – одна из крупнейших интернет-аптек в России*. Мы меняем индустрию, чтобы сделать здоровье доступным для всех. Сейчас компания работает по всей России от Калининграда до Владивостока. Собственная сеть аптек-хабов превышает 100, а сеть пунктов самовывоза – 8000. Сервис федерального масштаба в сегменте e-pharm строит сильная ИТ команда, разрабатывая надежную технологическую платформу, которая помогает клиентам заказывать лекарства и товары для здоровья онлайн. Мы ждем в команде ответственных, проактивных и надежных людей, чтобы вместе развивать сервис, который делает жизнь лучше. Сейчас в поиске специалиста, который усилит CORE-команду. Задачи, которые Вы будете решать в CORE-команде DWH: создание единого хранилища данных на стеке MSSQL, проработка существующей архитектуры и доработка новой модели хранения данных, организация ci/cd процесса и мониторинга процессов обработки данных. Над чем предстоит работать:   Работа с DWH, проектирование схем для хранения данных; Разработка процедур загрузки данных из различных источников (как внутренние БД, так и внешние аналитические/ маркетинговые платформы и т.д.) в DWH; Автоматизация и контроль ETL процессов; Автоматизация проверок качества данных; Формирование витрин данных; Составление технической документации по разрабатываемым витринам данных; Подготовка и поддержание в актуальном состоянии каталога метаданных; Организация CI/CD и мониторинга процессов обработки данных.  Нам важно:  Опыт в организации и разработке витрин данных от 3 лет; Понимание принципов организации хранилищ данных, принципов работы колоночных БД; Уверенное знание SQL, умение строить сложные запросы, понимать как работать с планом запроса и как на его основе оптимизировать производительность (MS SQL обязательно); Опыт автоматизации ETL; Опыт работы с OLAP; Git.  Будет плюсом:   Опыт работы с 1C;   Опыт работы с BigQuery (Google Cloud Platform), ClickHouse, PostgreSQL;   Опыт работы с Python;   CI/CD;   Понимание рынка ecommerce/ритейла;   Опыт работы со стеком Hadoop (HDFS, Hive);   Опыт работы с Apache Spark и Spark Structured Streaming;   Опыт работы с Airflow;   Опыт работы с Grafana.   Чем хороша работа в еАптеке:   Мы аккредитованная продуктовая IT/Digital - компания (мы работаем только над своим продуктом и ежедневно его улучшаем);   Прокачка профессиональных скиллов, работа с интересными онлайн - проектами и большими объемами данных;   Развитая инженерная культура, каждая задача проходит код-ревью;   Работа в команде с лидерами профессиональных сообществ;   Идеи приветствуются и реализуются, проводим эксперименты и всегда развиваемся;   Мы не ограничиваем тебя в использовании любых других удобных для тебя технологий, которые могли бы решать конкретные задачи нашей компании;   Работа строится по методологии Agile;   Минимум бюрократии, гибкие бизнес-процессы и оперативное принятие решений.   Что мы предлагаем:  Работа в аккредитованной ИТ компании; Заработная плата, адекватная уровню квалификации кандидата (уровень обсуждаем на собеседовании); График: 5/2, гибкое начало рабочего дня с 8:00 до 11:00 (гибридный формат работы, офис в стиле лофт, г. Москва); Перспективы профессионального роста: пополняемая коллекция профессиональной литературы, оплата посещения профильных конференций, обучения; ДМС со стоматологией в лучших клиниках города; Современная техника; Корпоративные мероприятия и тимбилдинги. "
"69379894","ГБУЗ «Научно-практический клинический центр диагностики и телемедицинских технологий ДЗМ»","Data Scientist / Machine Learning Engineer","False","None","None","От 3 до 6 лет","Полный день","['Медицинское оборудование', 'Написание научный статей', 'Научная деятельность', 'Научные исследования']","НПКЦ диагностики и телемедицины («Радиология Москвы»), – государственная компания с более чем 20-летним опытом работы в здравоохранении. Наш Центр - ведущая экспертная организация по развитию и повышению эффективности службы лучевой и инструментальной диагностики в России. Обязанности:  Проведение тестирований сервисов на основе технологий искусственного интеллекта (ИИ-Сервисов). Подготовка протоколов тестирования. Аналитика результатов работы ИИ-Сервисов. Подготовка ответов на запросы от компаний-разработчиков ИИ-Сервисов. Подготовка отчетных и демонстрационных материалов по работе ИИ-Сервисов. Работа с врачами-пользователями ИИ-Сервисов     Требования:  Опыт работы frontend разработчиком (средний или базовый уровень);  Опыт создания пользовательских интерфейсов; Высшее образование (предпочтительные направления - медицинская техника, медицинская кибернетика, биофизика, медицинская физика, инженерное дело), ученая степень будет преимуществом; Базовые навыки программирования (Python) Работа с медицинскими данными (принципы получения данных радиологических исследований, формат DICOM) Ведение и составление технической и отчетной документации  Мы ценим:  Стремление достигать результата; Желание постоянно развиваться; Готовность к динамичным изменениям; Искренность и открытость; Умение работать в команде; Навыки самоорганизации.  Мы предлагаем:  Официальное трудоустройство; График 5/2 с 9.00 до 17.30, сб и вс выходные; Годовая премия, белая зарплата! Выплаты 2 раза в месяц; Возможность прохождения дополнительного образования по направлению от центра! Участие в амбициозных проектах в масштабах отрасли и страны; Сопричастность к решению «сверхзадач» в системе здравоохранения; Наставничество и поддержка в развитии на международном уровне; Возможность развиваться опережающими темпами, участвуя в программах обучения; Стать частью яркой профессиональной команды; Дружный коллектив молодых и талантливых специалистов; Карьерный рост. "
"68926493","СБЕР","Data-инженер (проект SberID)","False","None","None","От 3 до 6 лет","Полный день","[]","Мы — Сбер ID. Будущий личный кабинет всей экосистемы Сбера, а также удобный способ авторизации в различные сервисы. Обязанности Что будешь делать: — Участвовать в создании BI-системы: сбор, трансформация и анализ данных — Настраивать дашборды и алерты по продуктовым метрикам — Проектировать и разрабатывать витрины и хранилища данных — Создавать ETL-процессы — Улучшать быстродействие (работа с витринами, СУБД, изменение логики запросов) — Отвечать за хранение, выгрузку и миграцию данных Требования Что ждем от тебя: — Опыт в направлении от двух лет — Сильные компетенции в области реляционных СУБД и хранилищ данных — Уверенное знание SQL: сложные запросы, аналитически функции, понимание физической реализации join’ов, оптимизация производительности запросов — Знание одного или нескольких ETL-инструментов: Informatica, MS SSIS, SAS, ODI — Понимание принципов организации хранилищ данных, подходов к проектированию логической и физической моделей, понимание основной проблематики хранилищ и подходов к решению Soft skills: — Не стесняешься спрашивать, умеешь и любишь вникать в суть — Умеешь отстаивать свою точку зрения, тебе не претит «пинговать» коллег — Готов(а) осваивать новые инструменты разработки и языки программирования, самообучаемость Наш стек: — Bigdata: Hadoop, Hive, Impala, Spark, Scala/Java — СУБД: Teradata, Greenplum — ETL: Informatica, Golden Gate — BI: Qlik Условия — Мощная команда. Ядро из Яндекса и интеграторов. С нами можно прокачать как soft skills так и hard skills — Можешь положить себе в портфолио интересный кейс. Мы в начале пути создания BI-системы. — Плюшки Сбера. Фитнес, мед страховка, подземный паркинг, хорошие годовые бонусы — Новый офис класса А с шикарным видом в 4 минутах от метро Кутузовская, гибридный режим работы"
"67482739","СБЕР","Data Engineer - играющий тренер (Аудиторные технологии)","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'hadoop', 'Big Data', 'Spark', 'airflow', 'ETL']","Мы - новая быстрорастущая команда, создающая сервис, который будет управлять цифровыми рекламными поверхностями и платформой по размещению рекламы на поверхностях Банка и Экосистемы. Используя данные СБЕРа и технологические решения его экосистемы, мы сможем реализовать уникальный сервис на рынке digital marketing. Он позволит нам показывать пользователям рекламу, которая будет нравиться и которая будет делать рекламные продукты эффективными и измеримыми для рекламодателей. Если вы талантливый и амбициозный специалист в сфере ИТ, если вы хотите быть причастными к созданию лучших продуктов для лучших клиентов и уверенно отвечать за результаты, то мы рады принять вас в команду. В рамках данной позиции предстоит:  разрабатывать и поддерживать микросервисы для работы с данными в экосистеме Hadoop развивать и сопровождать DWH на базе Hadoop быть лидером поставки сервиса, отвечать за техническую целостность продукта и его работу в промышленном контуре  Наши ожидания от вас:    не менее 2 лет опыта работы на лидирующей позиции в проекте, связанном с обработкой данных готовность принимать решения и брать на себя ответственность   практический опыт разработки и хорошее знание Python опыт промышленной разработки на стеке Java/Scala/Python практический опыт разработки приложений с использованием инструментов Spark Streaming, Hbase, Spark, Kafka, Hive, Impala, Hue и т.д. хорошие знания SQL, опыт работы с одной из реляционной БД - Oracle/PostgreSQL/mySQL/MS SQL Server понимание подходов к организации разработки (CI/CD, DevOps), практический опыт работы с инструментами Jenkins, SonarQube, TeamCity, Anisble, Git (BitBucket/GitLab).  Мы предлагаем:  интересные задачи по продуктам, влияющим одновременно на всю Экосистему: мы пишем продукты с 0, а значит никакого legacy и свобода творчества команда специалистов из топовых ИТ компаний регулярное обучение и профильные конференции, современное оборудование для работы   официальное трудоустройство согласно ТК РФ белая заработная плата ДМС оздоровительные программы для детей сотрудников возможность обучения за счет компании выплаты материальной помощи в особых/чрезвычайных случаях дисконт-программы от компаний партнеров льготное кредитование комфортный офис и гибкий график. "
"66529425","СБЕР","Data инженер (Рекламная платформа)","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'Hadoop', 'Git', 'SQL', 'PostgreSQL']","Мы - новая быстрорастущая команда, создающая сервис, который будет управлять цифровыми рекламными поверхностями и платформой по размещению рекламы на поверхностях Банка и Экосистемы. Используя данные СБЕРа и технологические решения его экосистемы, мы сможем реализовать уникальный сервис на рынке digital marketing. Он позволит нам показывать пользователям рекламу, которая будет нравиться и которая будет делать рекламные продукты эффективными и измеримыми для рекламодателей. Если вы талантливый и амбициозный специалист в сфере ИТ, если вы хотите быть причастными к созданию лучших продуктов для лучших клиентов и уверенно отвечать за результаты, то мы рады принять вас в команду. Команда &quot;Продуктовая аналитика&quot; отвечает за построение рекламных аудиторий, создание и проведение A/B тестирования и ad-hoc аналитики. Основной фокус направлен на создание различных сегментов на основе анализа поведения пользователей в Интернете, подготовки обучающих выборок для алгоритмов машинного обучения и анализ трафика по счётчику Топ-100. В рамках данной позиции предстоит:  Разработка и поддержка микросервисов для работы с данными в экосистеме Hadoop (Cloudera, Hortonworks, Apache BigTop и др.); Развитие и сопровождение DWH на базе Hadoop.  В вашем опыте нам важно:  Поиск, обработка и построение витрин данных на инфраструктуре Hadoop; Опыт промышленной разработки на стеке Java/Scala/Python; Опыт реализации REST, SOAP. Понимание принципов работы SSO, Kerberos; Опыт разработки приложений с использованием инструментов Spark Streaming, Hbase, Spark, Kafka, Hive, Impala, Hue и т.д.; Хорошие знания SQL, опыт работы с одной из реляционной БД - Oracle/PostgreSQL/mySQL/MS SQL Server; Понимание принципов модели распределенных вычислений, принципов организации Data Lake/DWH; Понимание подходов к организации разработки (CI/CD, DevOps), практический опыт работы с инструментами Jenkins, SonarQube, TeamCity, Anisble, Git (BitBucket/GitLab).  Мы предлагаем:  Интересные задачи по продуктам, влияющим одновременно на всю Экосистему: мы пишем продукты с 0, а значит никакого legacy и свобода творчества; Команда специалистов из топовых ИТ компаний; Регулярное обучение и профильные конференции, современное оборудование для работы; Уровень дохода, который готовы обсуждать и отталкиваться от ваших пожеланий, плюс премии; Комфортный офис и гибкий график; Множество плюшек от Сбера. "
"54040016","СБЕР","Data инженер (Рекламная платформа)","False","None","None","От 1 года до 3 лет","Полный день","['Git', 'PostgreSQL', 'Java', 'Python', 'SQL']","Мы - новая быстрорастущая команда, которая разрабатывает платформу для создания и управления рекламными коммуникациями на поверхностях Банка и Экосистемы. Это платформа, которая объединит в себе последние технологические решения AdTech и возможности больших данных экосистемы. Наша цель - создать конкурентоспособный сервис на рынке рекламных технологий, который позволит показывать эффективную и полезную рекламу. Нам нужны талантливые и амбициозные специалисты в сфере ИТ, которые хотят создавать лучшие продукты. Команда отвечает за создание и развитие набора сервисов и решений, которые позволят идентифицировать пользователя в цифровой среде на различных площадках экосистемы Сбера и внешних партнеров. В задачи команды входит сбор и анализ различных пользовательских данных, которые позволят точно определить пользователя и обогатить профиль пользователя. Ключевой задачей команды является создание единого идентификатора и профиля пользователя, что позволит правильно подбирать и показывать рекламу на web и mobile ресурсах. Также нам придется решать несколько важнейших задач для всей рекламной индустрии в период privacy-ограничений: идентифицировать пользователя на ограниченном наборе данных, подбирать рекламу неидентифицируемому пользователю. В рамках данной позиции предстоит:  Разработка и поддержка микросервисов для работы с данными в экосистеме Hadoop (Cloudera, Hortonworks, Apache BigTop и др.); Развитие и сопровождение DWH на базе Hadoop.  Мы ожидаем:  Поиск, обработка и построение витрин данных на инфраструктуре Hadoop; Опыт промышленной разработки на стеке Java/Scala/Python; Опыт реализации REST, SOAP. Понимание принципов работы SSO, Kerberos; Опыт разработки приложений с использованием инструментов Spark Streaming, Hbase, Spark, Kafka, Hive, Impala, Hue и т.д.; Хорошие знания SQL, опыт работы с одной из реляционной БД - Oracle/PostgreSQL/mySQL/MS SQL Server; Понимание принципов модели распределенных вычислений, принципов организации Data Lake/DWH; Понимание подходов к организации разработки (CI/CD, DevOps), практический опыт работы с инструментами Jenkins, SonarQube, TeamCity, Anisble, Git (BitBucket/GitLab).  Мы предлагаем:  Интересные задачи по продуктам, влияющим одновременно на всю Экосистему: мы пишем продукты с 0, а значит никакого legacy и свобода творчества; Команда специалистов из топовых ИТ компаний; Регулярное обучение и профильные конференции, современное оборудование для работы; Уровень дохода, который готовы обсуждать и отталкиваться от ваших пожеланий, плюс премии; Комфортный офис и гибкий график; Множество плюшек от Сбера. "
"70547140","СИБУР, Группа компаний","Инженер данных (Data Office)","False","None","None","От 3 до 6 лет","Полный день","['Python', 'PostgreSQL', 'Java', 'SQL', 'ORACLE']","ЧЕМ ВАМ ПРЕДСТОИТ ЗАНИМАТЬСЯ:  Развитием платформы данных, созданием инструментов обработки данных. Участие в проектах с реальной ценностью для бизнеса компании. Загрузкой данных из систем источников в узел данных и обеспечением их доступности. Автоматизацией аналитических задач. Разработкой и оптимизацией алгоритмов операционного анализа данных. Развитием практик DevSecOps, автомтизацией миграций и разработческих пайплайнов. Взаимодействием с аналитиками данных (data scientists), производственными и функциональными экспертами для определения требований к выгрузке, конвертации и представления данных.  ЭТА ВАКАНСИЯ ДЛЯ ВАС, ЕСЛИ ВЫ: Самое важное – вы должны хотеть развиваться и развивать платформу данных, как продукт. Нам хотелось бы, чтобы кандидат сам умел искать решение задач, пытливо изучать техническую и технологическую сторону вопроса, прототипировать и собирать обратную связь. Совершенно точно необходимы эти навыки:  Знание хотя бы 1 языка программирования (желательно что-то из Scala, Java, Python, C++/C#) на хорошем уровне: опыт разработки более года, есть примеры проектов или продуктов. Знание SQL, опыт работы с реляционными СУБД (Oracle, MySQL, PostgreSQL и пр.); Навыки проектирования и реализации системы сбора и обработки данных; Опыт работы с *nix системами  И хотя бы пару пунктов из нижеперечисленного:  Опыт работы хотя бы с одной MPP СУБД, понимание архитектуры и оптимизацией запросов. Знание Vertica будет дополнительным плюсом. Знания алгоритмов в сфере высоконагруженной обработки данных при помощи распределённых вычислений (MR). Умение работать с Avro, Parquet форматом данных (типы, сжатие, выбор сортировок). Умение работать с распределенным хранилищем HDFS. Знание и умение работать с ETL/ELT инструментами, Apache NiFi будет дополнительным плюсом. Знание и умение работать с AMQP (Kafka, RabbitMQ). Понимание как работать со стеком продуктов на платформе Hadoop. Желательно Spark, Spark Streaming, HBase, Impala, Hive; другие технологии тоже пойдут в зачёт. Умение разрабатывать UDF для различных сред анализа данных, контейнеры и оркестрация контейнеров. Управление кластером Hadoop (развёртывание, мониторинг, оптимизация). Знание интерфейсов, методов управления разработкой и конфигурациями.  Мы отдельно готовы учесть вашу способность открыто мыслить, сильную мотивацию на результат или просто хорошую теоретическую основу, полученную вами во время учёбы. ЧТО МЫ ВАМ ПРЕДЛАГАЕМ:  Возможность удаленной работы (график обсуждается индивидуально в зависимости от роли). Заработную плату по результатам собеседования, премии за эффективную работу и результат; Нестандартные задачи, которые требуют креативного подхода; Команду, с которой приятно работать, и поэтому мы любим собираться в офисе для командных встреч, а также интересно проводить время после; Современный стек и гибкие методологии разработки, работа в команде высококлассных профессионалов из разных технологических областей; Возможность обучения и участия в жизни IT-сообщества: большой выбор курсов в нашем корпоративном университете, посещение митапов и конференций; Корпоративные льготы: ДМС, льготное страхование родственников, большой выбор внутренних спортивных секций, скидки на абонементы сети World Class.   "
"70537362","СБЕР","Middle Data Engineer (развитие внешнеэкономического партнерства)","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'Hadoop', 'Java', 'Git', 'PostgreSQL']","Наша задача реализовать новое ПКАП(прикладное корпоративное аналитическое приложение) и перенести все сервисы в него из легаси систем, дополнив современными техническими возможностями, позволяющими ускорить, облегчить, автоматизировать работу пользователей с данными. Наша команда Data - инженеров и аналитиков занимается проектированием, разработкой , тестированием и выводом в промышленную эксплуатацию целевых решений (витрин данных и реализация интеграционных процессов между аналитическими и продуктовыми системами Банка). Наша цель собрать данные из различных Источников данных Банка, мы будем работать с данными как с актуальностью Т-1 так и Near Real Time репликами. В ходе работ вы столкнетесь с огромными данными (таблицы в 10-ки Тб и сотни миллиардов записей), извлечением и загрузкой данных, методами их обработки и трансформации, современными инструментами DEVOPS (CI/CD). Источники данных – продуктовые системы Банка (реляционные СУБД Oracle, MS SQL, PostgreSQL), работать предстоит с реализованными в Hadoop репликами источников. Команда работает c применением методологии Agile, используя инструменты Jira и Confluence. Обязанности:  разработка и поддержка витрин данных в Облаке данных (используются SQL, Java, Scala) вывод витрин в промышленную эксплуатацию управление качеством данных.    Требования:  отличное знание SQL (аналитические функции, подзапросы, хранимые процедуры, оптимизация запросов) навыки разработки витрин данных на Big Data с использованием описанного ниже стека инструментов и знанием соответствующего окружения и языков программирования: экосистема Hadoop (Hive, Spark, Kafka, Oozie); языки программирования (Scala, Java, Pyton); инструменты DEVOPS (Git, Nexus, Jenkins); понимание принципов ETL-процессов. опыт проектирования, разработки витрин данных, тестирования и вывода решений в промышленную эксплуатацию опыт от 1 лет работы с экосистемой Hadoop в роли Data инженера.   "
"69761302","Лига Цифровой Экономики","Data Engineer/Разработчик Java/Hadoop","False","None","None","От 1 года до 3 лет","Полный день","[]","Проект по разработке АС Аналитики клиентских данных для крупнейшего Банка России, включающую в себя: DWH на СУБД GreenPlum/ Hadoop; ETL (Informatica); UI приложение; ML - аналитику Твои задачи:  Разработка промышленных ETL-процессов витрин данных на Hadoop с использованием JAVA; Тестирование разработанного функционала; Подготовка и прохождение приемо-сдаточных испытаний разработанного функционала; Сопровождение и поддержка разработанного функционала, устранение сбоев.  Что мы ждем:  Опыта работы с технологиями (возможно не со всеми):  -Java (Spring-*), Maven, Junit, Cucumber, OpenAPI\Swagger\Rest\JSON-Kafka, IBM MQ-Greenplum/PostgreSql-Hadoop/Hive-ELK-Git Условия:  Работа в офисе на Кутузовской, гибридный график   "
"70455943","Tele2","DevOps-инженер Big Data","False","None","None","От 1 года до 3 лет","Полный день","['Linux', 'Bash', 'Docker', 'Kubernetes', 'Unix Shell Scripts', 'Настройка VPS/VDS серверов']","Мы делаем будущее. Развиваем продукт T2 AI Platform – это единая платформа создания, управления и оптимизации интеллектуальных систем, аналитики и ML. Мы создаем передовой продукт, цель которого создать полностью автоматизированный self-сервис для разработки произвольных систем искусственного интеллекта, их запуска, поддержки жизненного цикла и оптимизации. Это ставит перед нами задачи, которые сейчас решают инженеры Amazone, Google, у нас Yandex и Сбер. В этих задачах еще нет производственного стандарта и мы изобретаем, придумывая лучшее решение. Мы строим платформу как облачное решения для наших внутренних и внешних клиентов. Наши принципами являются качество, простота и целесообразность. Ваши будущие задачи:  Администрирование и развитие кластера ИИ платформы (Kubernetes, Airflow, Helm) Управление инструментами интегрированными с платформой (GitLab, Gitlab CI) Управление системами безопасности платформы (Hashicorp Vault) Управление ресурсами платформы Kubernetes, VPS серверами, организация сетевого окружения Поддержка и развитие системы управления, мониторинга ресурсов платформы (ELK, Grafana, Prometeus) Чтобы стать кандидатом, нужно желание работать на самом передовом стеке технологий в мире и уважение к своей профессии.  Требование к знаниям:  Знание и управление Kubernetes, Helm, GitLab, Hashicorp, ELK, Grafana, Prometeus Знание и опыт участия в организации CI/CD процессов разработки систем и релизных циклов (GitLab CI, Helm, Ansible)  Будет плюсом:  Высшее техническое образование Знание и опыт работы с системами контейнеризации Kubernetes, Docker Знание систем совместной разработки и хранения кода (GitLab) Опыт работы с ОС Linux и терминалом, а также знание Bash Языки программирования Shell, Scala, Python, Java Опыт работы с инструментами CI/CD Jenkins или Gitlab CI, Ansible, Nexus/JFrog, Sonarqube Опыт работы с инструментами мониторинга Prometheus, Grafana, ELK Опыт работы в телекоммуникационной отрасли  Все навыки опциональны и дают лишь дополнительные очки при приеме. У нас работает хорошо отлаженная система подготовки инженеров, которая позволяет сотруднику быстро получать нужный опыт. Плюсы для вас:  Интересная работа в быстроразвивающейся компании Уникальная система обучения для каждого сотрудника на основе индивидуальных планов развития Зеленый свет для новых идей и предложений: мы часто делаем то, на что другие не отваживаются Возможности профессионального и карьерного роста Ежегодный пересмотр заработной платы на основе твоей результативности, годовые бонусы Полное соответствие ТК РФ Расширенная медицинская страховка в России и за пределами страны Компенсация затрат на мобильную связь Дополнительные материальные выплаты: пособия при рождении ребенка, вступлении в брак и т.д. Частичная компенсация занятий спортом через год работы. "
"68737835","СБЕР","Data-инженер (Продукт онлайн кредитование)","False","None","None","От 1 года до 3 лет","Полный день","[]","Описание команды: Data-платформа кредитования Корпоративного Блока. Бизнес-приложения. Над продуктом работает большая и дружная команда инженеров, аналитиков данных и сопровождения, размещенная в Москве и Новосибирске. Обязанности  развивать бизнес-продукты на базе платформы (Хранение: HDFS + Hive MetaStore, PostgreSQL; обработка Apache Spark 2.4; ядро на Java/Scala), а именно: создавать прототипы новых продуктов вместе с заказчиком проектировать и разрабатывать промышленное решение принимать участие в тестировании и приемке, обеспечивать процесс передачи в эксплуатацию нового функционала формулировать задачи на доработку и улучшение платформы исправлять обнаруженные при эксплуатации и тестировании дефекты работать в распределенной команде и принимать активное участие в командных церемониях (daily в 10:30, «груминг», планирование и demo).  Требования  отлично знаете SQL имеете опыт работы с разными реляционными/распределенными/NoSQL и прочими СУБД умеете проектировать базы данных и процессы обработки данных понимаете принципы работы банковского бизнеса и основных банковских продуктов имеете непреодолимую тягу к обучению и саморазвитию.  Будет здорово, если вы:  понимаете принципы распределённой обработки данных и имеете опыт работы с компонентами экосистемы Hadoop версии Cloudera (Apache Spark, HDFS, YARN) обладаете навыками программирования и работы с системами контроля версий (Java/Scala/Python, Github/Bitbucket) умеете презентовать результаты анализа заказчикам.  Условия  красивый и комфортный офис: Кутузовский проспект, 32 ст1 возможность профессионального роста оформление по ТК РФ профессиональное обучение, семинары, тренинги, конференции льготные кредиты сотрудникам банка конкурентная компенсация (оклад и премии по результатам деятельности) дмс, страхование жизни бесплатный фитнес-зал в БЦ. "
"70405033","Райффайзен Банк","Data Engineer в команду автоматизации кредитной платформы","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'Spark', 'Python', 'AirFlow', 'DataLake']","Мы разрабатываем платформу микросервисов для департамента работы с проблемными клиентами.1. Выявление сигналов, свидетельствующих о потенциальных проблемах у действующих клиентов банков2. Выбор оптимальной стратегии по работе с проблемными клиентами3. Формирование документации, необходимой в процессе работы с проблемными клиентами.В целом команда работает в удалённом режиме, но в последнее время начала чаще собираться в офисе и после работы. Тебе доверим:  Участие в разработке интерактивных сервисов для реализации кредитного анализа корпоративных клиентов. Разработка сервисов по сбору данных на платформе DataLake с использованием Python, Spark, AirFlow. Разработка Lambda-функций на внутренней платформе.  От тебя ждем:  Опыт разработки с использованием SQL(Оконные аналитические функции, оптимизация запросов) от 2х лет. Опыт разработки на высокоуровневых языках от 1 года (Python, Java etc). Плюсом будет:  Хорошее знание PySpark, AirFlow Опыт работы с Docker и k8s  Предлагаем:  Высокий уровень свободы и влияния при внедрении изменений, возможность влиять на конечный результат; Атмосферу развития, вдохновленная принципами Agile и Scrum; Гибридный график работы; Индивидуально готовы обсуждать полностью удаленный формат; Работу в сплочённом IT-сообществе, где коллеги становятся настоящими друзьями; Возможность попробовать себя в смежных ролях; Развиваться: мы оплачиваем профессиональные тренинги и образовательные курсы, участие в конференциях, а также внутрибанковских митапах с приглашёнными экспертами; Получать корпоративные льготы: ДМС с первого рабочего дня, предоставляем скидки на банковские продукты, а также услуги и товары от компаний-партнеров. "
"66938324","СБЕР","Data engineer (Рекламная платформа)","False","None","None","От 1 года до 3 лет","Полный день","['Git', 'SCALA', 'PostgreSQL', 'Python', 'Hadoop']","Мы - новая быстрорастущая команда, создающая сервис, который будет управлять цифровыми рекламными поверхностями и платформой по размещению рекламы на поверхностях Банка и Экосистемы. Используя данные СБЕРа и технологические решения его экосистемы, мы сможем реализовать уникальный сервис на рынке digital marketing. Он позволит нам показывать пользователям рекламу, которая будет нравиться и которая будет делать рекламные продукты эффективными и измеримыми для рекламодателей. Если вы талантливый и амбициозный специалист в сфере ИТ, если вы хотите быть причастными к созданию лучших продуктов для лучших клиентов и уверенно отвечать за результаты, то мы рады принять вас в команду. Чем предстоит заниматься:  Разрабатывать и поддерживать микросервисы для работы с данными в экосистеме Hadoop (Cloudera, Hortonworks, Apache BigTop и др.); Разрабатывать и оптимизировать пайплайны обработки данных на python / scala - от логов nginx до записи в ClickHouse; Решать сложные технические задачи в ETL слое - развивать и сопровождать DWH на базе Hadoop/ Greenplum/ ClickHouse; Быть одним из драйверов развития архитектуры и инфраструктуры проекта.  Мы ожидаем:  Поиск, обработка и построение витрин данных на инфраструктуре Hadoop; Опыт промышленной разработки на стеке Java/Scala/Python; Опыт реализации REST, SOAP. Понимание принципов работы SSO, Kerberos; Опыт разработки приложений с использованием инструментов Spark Streaming, Hbase, Spark, Kafka, Hive, Impala, Hue и т.д.; Хорошие знания SQL, опыт работы с одной из реляционной БД - Oracle/PostgreSQL/mySQL/MS SQL Server; Понимание принципов модели распределенных вычислений, принципов организации Data Lake/DWH; Понимание подходов к организации разработки (CI/CD, DevOps), практический опыт работы с инструментами Jenkins, SonarQube, TeamCity, Anisble, Git (BitBucket/GitLab).  Мы предлагаем:  Интересные задачи по продуктам, влияющим одновременно на всю Экосистему: мы пишем продукты с 0, а значит никакого legacy и свобода творчества; Команда специалистов из топовых ИТ компаний; Регулярное обучение и профильные конференции, современное оборудование для работы; Уровень дохода, который готовы обсуждать и отталкиваться от ваших пожеланий, плюс премии; Комфортный офис и гибкий график; Множество плюшек от Сбера. "
"70366942","СБЕР","Data Engineer / Data Analyst (Клиентская аналитика)","False","None","None","От 1 года до 3 лет","Полный день","['SQL', 'Python']","Мы ищем специалиста по работе с данными в Корпоративный блок Сбера для аналитической поддержки лидеров бизнес-подразделений и топ-менеджмента. Нужно искать инсайты в данных и выстраивать систему показателей, чтобы ответить на вопросы:  в чем сильные и слабые стороны конкурентов; что происходит с компаниями и как это сказывается на бизнесе Сбера; насколько хорошо мы удовлетворяем потребности клиентов; и другие, предполагающие интересную аналитическую работу.  Что мы делаем:  Помогаем росту бизнеса через анализ корпоративной клиентской базы Сбера и конкурентов; Выстраиваем системные аналитические решения.  Функциональные обязанности:  Проводить анализ корпоративных клиентов в Сбере и на рынке в целом; Находить слабые места и вырабатывать рекомендации; Визуализировать результаты анализа и переводить на бизнес-язык; Предлагать направления дальнейшего исследования; Быть центром компетенций по используемым данным; Развивать систему показателей для мониторинга.  Навыки и опыт работы:  Опыт работы аналитиком в крупном банке/аналогичной компании; Понимание работы банковского бизнеса; Уверенное знание SQL (уровень выше среднего); Опыт работы с BI (как минимум одна из платформ: MS Power BI, Qlik Sense, Tableau, Oracle BI); Умение систематизировать данные и понятно описывать полученную в ходе исследований информацию. Перевод бизнес-постановки задачи от заказчика в соответствующий SQL-код, анализ требуемых источников, проверка качества результата. Навыки презентации результатов анализа бизнес-заказчику.  Как преимущество:  Хорошее знание математической статистики и теории вероятностей; Опыт работы с GreenPlum, Hadoop; Знание ml-стека на python: numpy, pandas, sklearn; Знание Spark, PySpark, использование UDF, Hive, Scala; Знание бизнес-метрик, финансовой и управленческой отчетности банков.  Условия:  Интересные и масштабные задачи, интеллектуальный вызов и возможность выбрать карьеру по интересам; Работу в быстро развивающейся команде; Офис Agile Home в центре Москвы, классную столовую, отсутствие дресс-кода; Тренажерный зал премиум-класса в офисе; ДМС с первого дня.  Работа в офисе, Кутузовский проспект, 32к1"
"70364815","Банк Хоум Кредит","DevOps инженер Smart Data","False","None","None","От 1 года до 3 лет","Удаленная работа","['Linux', 'Bash', 'Nginx', 'Docker', 'SQL', 'DevOps', 'Elasticsearch', 'Jenkins', 'Groovy', 'Kubernetes', 'Grafana', 'CI/CD', 'Unix', 'Zabbix', 'Hadoop', 'Big Data', 'Kafka', 'Python', 'Ansible']","Мы ищем DevOps инженера в команду Smart Data.Система для сбора и анализа больших данных, которая выдаёт информацию о пользователях в онлайн-режиме. Она помогает Банку строить индивидуальные отношения с каждым клиентом, предугадывать его потребности, предлагать лучший сервис здесь и сейчас. О ПРОДУКТЕ:  С помощью SmartData создан Профиль 360 — набор данных о клиенте. Он позволяет разработать персонализированный цифровой сервис — личный web- и мобильный банковский офис, усовершенствовать процессы одобрения и антифрод, сформировать индивидуальные предложения продуктов в режиме реального времени и увеличить NPS Данные собраны более чем из 40 источников, их количество постоянно растет. Из последних пополнений: VKPay, Цифровой профиль гражданина, данные из банковского чат-бота, модуля V2T и войс-бота У многих ребят есть опыт построения систем Big Data в крупных международных компаниях Все компетенции: архитектура, аналитика, разработка, тестирование, установка на продуктив и поддержка сред — собраны внутри команды, поэтому инициативы реализуются быстро Способы решения задач выбираются совместно Запущены международные проекты с командами Home Credit Group  ТЕХНОЛОГИИ:  Smart Data — полностью внутренняя разработка Дистрибутив Hadoop от Cloudera: Kafka, Solr, HBase, Spark, HDFS, Kudu, Hive, Impala, Sqoop, в связке с Apache NiFi (актуальные версии) 90% функционала покрыто автотестами Строится принципиально новый Профиль 360 на базе Ignite, Cassandra с фасадом из микросервисов Активно развиваются микросервисы на базе Kubernetes, Docker, Spring В больших данных мощность системы увеличена в два раза Совместно с чешскими коллегами создаем «горячую» копию системы в контуре DRC (Disaster Recovery Center), чтобы поддержать высокий уровень доступности И еще много-много новых фич: DevOps, DataQuality, Бизнес-мониторинг, Ролевая модель доступа к данным, – так что скучно не будет!  ЧЕМ ВЫ БУДЕТЕ ЗАНИМАТЬСЯ:  Переделал бы в Развитие DevOps в команде единомышленников Подготовка предложений по оптимизации платформы Hadoop в рамках задач сопровождения; Автоматизация установки разработок на среды (прод, тест, нагрузка и т.д.); Присоединиться к текущему процессу разработки Работа в команде  ЧТО ВАМ ДЛЯ ЭТОГО НЕОБХОДИМО:  Опыт работы в качестве инженера DevOps от 2-х лет Опыт работы с экосистемой Hadoop - не менее 1 года Разработка скриптов автоматизации процессов настройки и обновления систем (Bash, Python, Groovy и др.) Развертывание и настройка тестовых и продуктивных сред Создание CI/CD-пайплайнов в Jenkins для стека Java / Spring или Python Опыт организации и реализации DevOps процессов знание кубера, развертывания и настройка приложения Знания Ansible, Git  ЖЕЛАТЕЛЬНЫЕ ЗНАНИЯ:  Опыт работы в банковской сфере или в проектах интеграторов для банков Опыт работы с контейнерами и оркестраторами (Docker, Kubernetes, Rancher и др.) Опыт работы с системами сбора, анализа и визуализации логов (ElasticSearch, Kibana, LogStash, GrayLog, Grafana, Splunk, Prometheus и др.) Экспертные знания Unix, сетевых протоколов  МЫ ПРЕДЛАГАЕМ:  Фиксированный оклад + система премирования Социальный пакет Офис: м. Белорусская Удаленный формат работы Команда, открытая самым смелым идеям Внутренние программы обучения и развития Высокий уровень ответственности и возможность самостоятельно принимать решения Атмосфера, где легко оставаться собой: минимум формализма, открытые коммуникации и отсутствие дресс-кода "
"70359656","МТС","QA engineer на продукт Smart Rollout (Big Data)","False","None","None","От 1 года до 3 лет","Полный день","['Git', 'Python', 'SQL', 'Atlassian Jira', 'Docker', 'UI', 'Linux', 'Allure', 'Selenium', 'Aquas']","Big Data МТС – место, где телеком данные превращаются в реально работающие IT-продукты. Мы создали и протестировали несколько десятков сервисов. Самые успешные из них уже стали частью экосистемы МТС. Например, МТС Маркетолог, рекомендации в KION (МТС ТВ), услуга “Кто звонит?” или Спам blacklist. Кого мы ищем?  Опыт от 2 лет в QA + желание развиваться как full stack QA специалист Опыт тестирования Web UI продуктов и web сервисов Опыт разработки автоматизированных тестов с использованием одного или нескольких языков программирования (предпочтительно Python) Знание и понимание методологий и видов тестирования Опыт разработки тест-кейсов, понимание методик тест-дизайна Владение SQL на базовом уровне (select с подзапросами, join, группировками) Знание систем контроля версий Git  Что предстоит делать?  Проектирование и автоматизация функциональных тестов на интеграционном и e2e уровне для REST API + Web UI компонентов (на python) Проведение нефункциональных видов тестирования (нагрузочное, отказоустойчивости) Документирование найденных ошибок в системе баг-трекинга, контроль их исправления Контроль тестового покрытия и актуализация тестовой документации – матрица покрытия, чек-листы, тест-кейсы Участие в релизном процессе и ручное тестирование business-critical функциональностей, если по каким-то причинам их не удалось автоматизировать  Сейчас мы ищем QA engineer на продукт Smart Rollout Наша команда занимается разработкой комплексного решения для планирования и оптимизации сети сотовой связи на базе BigData. Наш продукт сочетает нескольких комплексных модулей и планируется, что он станет центральным звеном планирования сети, связывающим работу многих подразделений компании – от технического блока до финансового. Нам нужен грамотный открытый к общению и замотивированный QA инженер, который обеспечивал бы качество и слаженную работу нашего продукта на всех этапах планирования сети Что вы найдете в команде Big Data? Стек:   Jira, Confluence, Allure TestOps Gitlab, Jenkins, Ansible, Docker Автоматизация: Python, PyTest, Allure, Selenium, Aquas (внутренний фрэймворк для автоматизации тестов ETL) Нагрузочное тестирование: Locust, JMeter  Команда: На данный момент в Центре BigData 12 QA инженеров. Каждый работает на своем продукте в кроссфункциональной команде (где есть аналитики, разработчики, DE, DS и т.д.), 1-3 QA на продукт. Вместе мы формируем Центр компетенции обеспечения качества. Внутри ЦК мы регулярно синхронизируемся, развиваем общие подходы и инструменты автоматизации тестирования, помогаем друг другу по техническим и организационным вопросам, и стараемся переиспользовать удачные решения. Планируем различные мероприятия, включая обучение и выезды на внешние конференции. Условия: каждый месяц - аванс и зарплата, дважды в год - премия. ДМС + стоматология, корпоративная связь, специальные предложения от партнеров и друзей МТС, отпуск 31 день в год. Выдаем 16” MacBook Pro или Dell на выбор. Есть ли обучение?  Конференции, митапы. Корпоративный университет МТС и масштабная виртуальная библиотека. А ещё мы регулярно обмениваемся опытом на совместных синках с лидами экспертизы  Какой график? Гибкое начало рабочего дня в промежутке с 8 до 11. Есть возможность работать несколько дней вне офиса по договоренности с командой. Сколько этапов при отборе? Не более трех:  HR + первое тех. интервью с лидом направления Тестовое задание/второе интервью - по необходимости Собеседование с PO и командой, выбор кандидатом проекта "
"69746052","СБЕР","Middle Data Engineer (Торговое финансирование и документарные операции)","False","None","None","От 1 года до 3 лет","Полный день","[]","Информация о проекте: · проект: «Единый Семантический Слой» (ЕСС) - единое облако данных · цель: предоставление потребителям интегрированных данных не зависимо от АС-источника · модель: данные в ЕСС объединены в единую модель, имеют сквозную идентификацию объектов и размечены единой нормативно-справочной информацией · основные драйверы создания ЕСС: формирование регуляторной отчетности и миграция риск-инфраструктуры на данные единой корпоративной аналитической платформы · предметная область «Торговое финансирование». Обязанности · работа с DWH, проектирование схем для хранения данных · загрузка данных в DWH из различных источников - как внутренние БД, так и внешние аналитические платформы и т.д. · развитие и поддержка существующих в рамках проекта DevOps процессов · проектирование и разработка ETL процессов · поиск ошибок и аномалий в данных, автоматизация проверок качества данных · подготовка и поддержание в актуальном состоянии каталога метаданных · разработка кода в соответствии с требованиями к предоставлению данных подотчетной предметной области единого семантического слоя · ревью и оптимизация ранее разработанного (автосгенерированного) кода · разработка и прогон автотестов на разработанный/доработанный код · подготовка дистрибутивов и описания в соответствии с требованиями оформления Банка · участие в проектировании логической модели данных. Требования · понимание принципов организации хранилищ данных, внутренней архитектуры Hadoop и Spark. Опыт работы с DWH · опыт работы в роли разработчика в проектах по разработке и/или модификации и/или внедрению ПО · понимание методологии DevOps · навыки работы с BitBucket, Jenkins, Nexus или аналогичными инструментами DevOps · опыт работы с ETL инструментами · уверенное знание SQL, умение строить сложные запросы и оптимизировать производительность опыт командной разработки с использованием программных продуктов Confluence и Jira (желательно). Ключевые навыки: · Hadoop, Spark, Jenkins,Bitbucket, Nexus, Git, ETL, SQL, DWH, Jira. Условия · работа в офисе в центре Москвы · возможность внести свои инициативы и увидеть результат работы · уровень заработной платы по результатам собеседования, существенный годовой бонус · гибкое начало рабочего дня · ДМС с первого дня работы."
"70087754","МТС","DevOps engineer на продукт BI Tools (Big Data)","False","None","None","От 3 до 6 лет","Удаленная работа","['Docker', 'Git', 'Python', 'SQL', 'Linux', 'Terraform', 'Ansible', 'Openstack', 'Prometheus']","Big Data МТС – место, где телеком данные превращаются в реально работающие IT-продукты. Мы создали и протестировали несколько десятков сервисов. Самые успешные из них уже стали частью экосистемы МТС. Например, МТС Маркетолог, рекомендации в KION (МТС ТВ), услуга “Кто звонит?” или Спам blacklist. Кого мы ищем? Обязательно:  Облачные сервисы и принципы их работы Администрирование Linux/Windows CI/CD Контейнеризация Оркестрация и балансировка Скриптовые языки Очереди сообщений Мониторинг Свободное чтение технической документации на английском  Что предстоит делать?  Создание self-service инструментария для аналитики и визуализации данных Интеграция с сервисами других команд Участие в развитии, доработке, деплоя Opensource инструментов Мониторинг, логирование микросервисных приложений, разработка экспортеров Деплой микросервисов как в облака так и baremetal  Сейчас мы ищем DevOps engineer на продукт BI Tools Под BI мы подразумеваем инструменты визуализации данных и предоставление актуальной аналитики, в том числе проведения ad-hoc запросов и пилотных исследований для пользователей с низким порогом вхождения в аналитику, интеграцию с ETL в рамках существующих инструментов. Организация основной добавленной ценности от использования данных при принятии решения. Наша команда занимается внедрением, развитием и разработкой решений бизнес-аналитики в компании. Из бонусов: - Мы всей коммандой на удалёнке. - В нашей команде имеется множество сервисов на поддержке, можно по ближе познакомиться с работой облаков, сетевых хранилок в том числе объектных и т.д. Что вы найдете в команде Big Data? Стек технологий: Openstack, k8s, Docker, Packer, Prometheus\VM, ELK\EFK, SQL, Python, SAST, GIT, Ansible, Terraform Команда: на данным момент в Центре BigData 10 java разработчиков. Каждый работает на своем продукте в кроссфункциональной команде (где есть аналитики, QA инженеры, DevOps, DE, DS и т.д.). Условия: каждый месяц - аванс и зарплата, дважды в год - премия. ДМС + стоматология, корпоративная связь, специальные предложения от партнеров и друзей МТС, отпуск 31 день в год. Выдаем 16” MacBook Pro или Dell на выбор. Есть ли обучение?  Конференции, митапы. Корпоративный университет МТС и масштабная виртуальная библиотека. А ещё мы регулярно обмениваемся опытом на совместных синках с лидами экспертизы  Какой график? Гибкое начало рабочего дня в промежутке с 8 до 11. Есть возможность работать несколько дней вне офиса по договоренности с командой. Сколько этапов при отборе? Не более трех:  HR + первое тех. интервью с лидом направления Тестовое задание/второе интервью - по необходимости Собеседование с PO и командой, выбор кандидатом проекта     "
"70047110","Новео","Data Engineer Middle+/Senior, Remote","False","None","None","От 1 года до 3 лет","Полный день","['Python', 'AWS', 'Athena', 'Hadoop']","НОВЕО - международная компания, с 2002 года предоставляющая услуги по заказной разработке ПО для западноевропейского рынка. Где можно работать: удаленно из любой страны либо в одном из офисов компании. Как можно работать: по трудовому договору с оплатой в валюте страны или через договор с ИП/самозанятым с оплатой в евро. Рабочие языки на проектах: английский, русский, французский. Что мы готовы предложить?   Достойный уровень заработной платы и регулярный ее пересмотр по результатам performance review;   трудоустройство по трудовому договору страны или через договор с ИП без привязки к определенному местонахождению;   оплачиваемый отпуск и больничные;   работа в профессиональной распределенной команде над интересными проектами;   возможность изучения новых технологий и их применение на практике, внутреннее обучение;   участие во внутренних мит-апах, постоянный обмен опытом с коллегами;   четко выстроенные процессы и методологии разработки;   бесплатное изучение иностранных языков в рабочее время с преподавателями компании (английский, французский);   компенсацию медицинских услуг, массажа или спорта (внутренний аналог ДМС).   Какой стек мы используем?  AWS, Athena, Glue, Ploty, Spotfire.  Чем ты будешь заниматься?   Сбор и формализация потребностей пользователей в анализе; подготавливать, использовать и исследовать все источники данных в хранилище данных; контролировать качества данных; проектирование и разработка наборов данных, отвечающих потребностям пользователей; создание и поддержка общей архитектуры данных; создание и разработка конвейеров данных с помощью инструмента планирования и оркестровки; создание и обслуживание озера данных и хранилища данных; обеспечение качества данных путем создания автоматизированных тестов для наших конвейеров и данных; создание решений по мониторингу и оповещению для наших конвейеров данных; поддержка и совершенствование архитектуры данных, инструментария данных, определение схемы базы данных, создание карт данных и конвейеров; взаимодействие с другими командами для анализа их потребностей в данных, их решения и общей поддержки.  Требования к кандидатам:  Хорошее понимание дисциплин: статистика, информатика, инженерия, прикладная математика или любые другие смежные дисциплины; опыт работы от 1,5 лет с большими объемами реальных данных с использованием SQL и / или NoSQL; опыт работы от 1,5 лет с Node.js / Python/ и Scala / или другими языками и инструментами программного обеспечения для анализа; возможность трансформировать бизнес-цели в действенный анализ;  уровень разговорного английского языка достаточный для общения с заказчиком на технические темы. О компании Новео:  20 лет на рынке; производственные отделы в странах Восточной Европы;  Присоединяйся к команде разработчиков НОВЕО для решения интересных задач!  среди наших клиентов: Decathlon, DIOR, Renault, BMW, Societe Generale и др. "
"69027863","VK","Big Data Platform Engineer","False","None","None","От 3 до 6 лет","Полный день","['Spark', 'hadoop', 'Linux', 'kafka', 'airflow', 'grafana']","Наша платформа работы с данными построена как на проверенных решениях с открытым исходным кодом (Hadoop, Kafka, Spark, Zeppelin и не только), так и на решениях собственной разработки, заточенных под работу 24/7 в условиях высоких нагрузок. Мы ищем человека, который поможет развивать эту платформу внедряя новые решения и дорабатывая существующие. Особенности:  big data: 4 hdfs кластера общим объемом ~150PB; high load: обслуживаем десятки тысяч серверов; high available: все сервера расположены в 5 разных дата центрах; не enterprise: разрабатываем решения, которые сами же и используем; stack: hadoop-3.1.x, kafka-2.4.x, spark-2.3.x, grafana 7.x, clickhouse, airflow; lang: java, scala, python, bash.  Задачи:  развивать платформу хранения и обработки больших данных; предлагать, отстаивать и реализовывать архитектурные решения; оптимизировать и настраивать различные компоненты: hdfs, kafka, clickhouse, spark, zeppelin, etc; разрабатывать инструменты для хранения и обработки данных на java/scala/kotlin.  Мы ожидаем:  хорошее знание java/scala; опыт работы и понимание внутреннего устройства: hadoop, hdfs, kafka, spark, zeppelin, airflow, zookeeper, clickhouse.  Условия:  сложные и интересные задачи: высоконагруженные быстрорастущие сервисы, которые задают уровень для конкурентов качеством и технологиями; оборудование: мощное железо, десятки петабайт данных, GPU-кластера и облачный инструментарий; команда: с нами работают профессионалы экстра класса, каждый из которых может поделиться своей экспертизой; профессиональное развитие: прямо в офисе мы организуем митапы, конференции, семинары и тренинги, куда открыт доступ каждому сотруднику, а также регулярно посещаем лучшие мировые конференции; новый опыт: лучшие сотрудники преподают в наших образовательных проектах, выступают на российских и международных конференциях; социальный пакет (спорт, ДМС, английский язык); комфортный офис с парковкой, душем и зонами отдыха в паре минут от м. «Аэропорт»; возможность работать в гибридном режиме (совмещение удаленки с работой в офисе совместно с командой); дополнительное обучение за счет компании, участие в профессиональных конференциях и форумах по всему миру; корпоративные мероприятия и Team Building Events в России и за рубежом. "
"69972583","Бринго","Senior Data Scientist / ML Engineer","False","None","None","От 3 до 6 лет","Полный день","['ML', 'Python', 'Tensorflow', 'Scikit-learn', 'Pandas']","«Бринго» — разработчик уникального программного обеспечения для интеграции корпоративных клиентов с мобильными операторами и мобильными приложениями (WeChat, Viber, WhatsApp) для осуществления SMS информирования, VOIP и систем биллинга. Основная задача компании — разработка и поддержка современных программных решений в различных отраслях, помогая корпоративным клиентам развивать свой бизнес. Мы ищем Senior/Lead ML инженера, который сможет вывести качество наших сервисов на новый уровень. Bringo имеет свой блог на Habr. Требования к кандидатам:  Глубокое понимание ML алгоритмов машинного обучения; Огромный опыт в построении и оптимизации МНОГОСЛОЙНЫХ и РЕКУРРЕНТНЫХ нейросетей; Хорошее знание SQL, опыт работы с БД, опыт написания сложных запросов; Уверенное знание Python (scikit-learn, tensorflow, pandas, numpy, matplotlib); Опыт построения Production ML.  Будет плюсом:   Опыт работы с ценами, например, с недвижимостью, анализ цен в магазинах и т.д.   Опыт работы ml teamlead/руководителем отдела аналитики/техническим директором.   Обязанности:  Извлечение и обработка массивов данных, а также по возможности автоматизация данных процессов; Обучение и тестирование моделей машинного обучения; Участие в разработке/проверке гипотез, интерпретации результатов и оценке качества разработанных моделей.  Условия:  Светлый офис А-класса с панорамным видом, кухней, кафе, столовой и пр.; Современное оборудование; График работы 5\2, гибкое начало дня в офисе (возможен формат удаленной работы после испытательного срока); Полностью &quot;Белая&quot; конкурентная заработная плата.  Преимущества работы в Bringo:  Корпоративные выезды на профильные конференции и выставки, оформление виз и все накладные расходы оплачивает работодатель; Корпоративы и развлекательные мероприятия для сотрудников (по желанию сотрудника); Рабочая станция и весь необходимый лицензионный программный функционал собирается по рекомендациям сотрудника; Работа в команде профессионалов и единомышленников.  United Kingdom representation [ Bringo Group Ltd. ]"
"68924527","Henkel Russia","Инженер по поддержке ИТ инфраструктуры / Data Center – Infrastructure Expert","False","None","None","От 3 до 6 лет","Полный день","['Linux', 'Microsoft', 'Системная инфраструктура', 'Серверное оборудование']","Инженер по поддержке ИТ инфраструктуры / Data Center – Infrastructure Expert  Обязанности:   Проектирование, развертывание и настройка системной инфраструктуры (on premise &amp; cloud) для продуктов на базе продуктов Microsoft, Linux   Обеспечение непрерывной работоспособности и эффективности системной инфраструктуры   Восстановление работоспособности сервисов при сбоях   Настройка серверного программного обеспечения, эксплуатация систем резервного копирования   Проведение исследований новых технологий, инструментов и платформ в сфере системной инфраструктуры   Ведение проектной и рабочей документации   Требования:   Высшее техническое образование в сфере Информационных технологий   Опыт работы по поддержке системной инфраструктуры от 5-х лет   Знание английского языка на уровне Intermediate и выше   Опыт администрирования ОС Microsoft/Debian Linux/Astra Linux/Alt Linux и понимание принципов работы базовых технологических сервисов (AD, DNS, DHCP, NTP, SNMP, SMTP, LDAP, RADIUS, SAMBA)   Знание продуктов Microsoft (Active Directory, DNS, DHCP, Hyper-V, Exchange, Office 365)   Понимание принципов построения частных / гибридных облаков   Опыт администрирования и знание принципов работы платформ виртуализации   Опыт взаимодействия с поставщиками услуг: постановка задач и контроль исполнения, координация работ   Опыт работы с проектной документацией, оформление по ГОСТ   Знание рынка отечественного корпоративного ПО (операционные системы, почтовые системы, системы виртуализации, системы управления корпоративной инфраструктурой, офисные приложения)   Условия:  Уровень компенсации обсуждается индивидуально (оклад + бонус) ДМС, страхование жизни 31 день отпуска Питание в офисе Мобильная связь Гибридный формат работы Программа поддержки сотрудников (well-being) Дружная и профессиональная команда Возможность присоединиться к яркой культуре доверия и сопричастности "
"69904411","Лаборатория Касперского","Senior Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Cassandra', 'Работа с базами данных', 'C#', 'Go', 'Linux']","О нас:Команда занимается разработкой информационных систем и сервисов как для внутренних, так и для внешних заказчиков. Используем C#\Scala\Go, разворачиваем сервисы в контейнерах, используем Kafka, RabbitMQ, Elasticsearch, ScyllaDB, MSSQL, HBase. А еще мы занимаемся обработкой больших объемов данных на самом современном стэке. Нам важно создавать надежные масштабируемые решения. Мы ищем специалиста для участия в создании новых продуктов, а также развития и поддержки существующих проектов. Чем предстоит заниматься:  Создание масштабируемых и высоконагруженных сервисов обработки больших объемов данных; Разработка, внедрение и поддержка сервисов на C#\Scala\Go; Участие в проектировании архитектуры новых решений; Настройка и улучшение CI/CD; Документирование разрабатываемых решений; Проведение code review; Анализ требований, планирование и оценка реализации; Проектирование сервисов на основе функциональных и нефункциональных требований; Совместная работа с экспертами предметной области с целью извлечения из данных новых знаний.  Что мы ждём от Вас:  Опыт работы с базами данных (реляционные/NoSQL): ScyllaDB\Cassandra или HBase; Опыт работы с контейнерами (Docker/Kubernetes/OpenShift); Опыт разработки распределённых систем: работа с распределенными хранилищами данных, кэшами, очередями; Хорошие знания в области объектно-ориентированного программирования, модели акторов, паттернов проектирования и архитектуры; Опыт разработки многопоточных приложений; Опыт работы с Git или любой системой контроля версий Внимательность, аккуратность, чувство ответственности и умение работать в команде; Знание английского языка достаточное для чтения технической документации.  Дополнительным плюсом будет:  Опыт разработки на платформе JVM; Akka, Akka Streams, Alpakka; Elasticsearch; Kafka; RabbitMQ; NoSQL БД (HBase, Cassandra, ScyllaDB, Redis и т.п.); Реляционные БД (MSSQL, PostgreSQL и т.п.); Опыт работы с Linux на уровне опытного пользователя; Опыт работы с любым функциональным языком программирования, например, Scala, F#, OCaml, Haskell; Непрерывное профессиональное развитие: изучение новых языков программирования, онлайн курсы, контрибьюты в открытые проекты, чтение книг, блогов и т.д. "
"68572777","WILDBERRIES","Middle/Senior Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Python', 'SQL', 'Базы данных', 'Greenplum', 'ClickHouse', 'Kafka', 'Redis']","Сейчас активно разрабатываются и внедряются множество DS моделей, затрагивающие самые разные части WB. Перед выводом в работу их нужно оценивать и тестировать с точки зрения производительности и масштабируемости, применять или создавать инструменты, обеспечивающие надежную и стабильную работу в таком highload-проекте, как WB. Что предстоит делать:  организовать сбор, хранение и доступ данных из различных источников (базы данных, объектные хранилища, API, разработка клея (веб-серверов)).  Мы ожидаем:   отличное знание SQL; хорошие навыки прикладной разработки на Python.  Предлагаем:   гибкое начало дня, удаленка или гибридный график; сложные интересные проекты; свободу в принятии решений; профессиональный рост в команде увлеченных инженеров; красивый офис, но работайте откуда удобно; минимум формализма; ротация между проектами; конференции, митапы.  Стек: Hadoop, Greenplum, ClickHouse, PostgreSQL, MS SQL Server, Redis, NATS, Kafka."
"69819329","Ozon","Руководитель группы Data Engineer, платформа интеграции данных","False","None","None","От 3 до 6 лет","Полный день","['Хранение и обработка данных', 'IT']","Команда DE Platform строит инструменты интеграции данных, которые выступают как мост между бизнесом и BI. Эти данные используются во множестве процессов внутри компании. Экспертиза команды также позволяет решать нетривиальные задачи по гарантиям доставки, технологическому развитию как инструментов, так и инфраструктуры вокруг. Наш стек:  Scala Vertica, MS SQL, ClickHouse, Hadoop (hadoop3, hdfs, yarn, hive) и др. Airflow (Celery Executor, k8s); Gitlab CI/CD, Kubernetes Kafka, Prometheus, Grafana Gitlab, GIT  Вам предстоит:  Формирование и развитие команды (найм, менторство, мотивация), команда 5-10 человек Оперативное и стратегическое планирование разработки (спринт, месяц, квартал) Организация работы со смежными командами Внедрение практик поддержки пользователей в контексте интеграции данных: реакция, анализ проблемы, фикс или задача на фикс Внедрение практик анализа возникающих проблем: мониторинг, алертинг, анализ логов сервисов, анализ и чтение логов подов в k8s, анализ метрик БД и сервисов Оптимизация запросов, таблиц и загрузок в БД Verica и Clickhouse; AirFlow: написание / рефакторинг обобщенного кода для привоза данных Развитие и поддержка транспортов данных Архитектурное планирование развития проектов команды    Мы ожидаем: Уверенное владение Scala Опыт работы с big data (Hadoop, hive, spark, kafka) Опыт работы с аналитическими БД (Vertica/Greenplum/ClickHouse) от года Опыт работы с СУБД (Postgre SQL/MS SQL) Опыт работы с Kubernetes, GIT Будет плюсом: Опыт работы с Prometheus, Grafana; Владение Java Владение Python Опыт работы с Airflow Мы предлагаем:   Динамичный и быстроразвивающийся бизнес, ресурсы, возможность сделать вместе сделать лучший продукт на рынке e-commerce Свободу действий в принятии решений Достойный уровень заработной платы Профессиональную команду, которой мы гордимся Возможность развиваться вместе с нашим бизнесом "
"69774595","Ozon","Data Engineer, инфраструктура, обработка и хранение данных","False","None","None","От 3 до 6 лет","Полный день","['IT', 'Хранение и обработка данных']","Команда DE-infra занимается разработкой инструмента управления главного двигателя автоматизации в компании Ozon – Airflow, по модели Airflow as a Service, а также разработкой инструмента интеграции данных. Мы помогаем нашим пользователям разрабатывать пайплайны движения данных и предосталяем им окружение для работы. В процессе работы мы используем практики DevOps, разнообразные инструменты и технологии: Python, Kubernetes, Airflow, Hadoop, Spark, MySQL, PostgreSql, Vertica, ClickHouse, Prometheus, Hive, Kafka, Grafana, Git, Helm. Наш стек:  Python; Vertica, MS SQL, ClickHouse, Hadoop (hadoop3, hdfs, yarn, hive) и др.; Airflow (Celery Executor, k8s); Gitlab CI/CD, Kubernetes; Kafka, Prometheus, Grafana; Gitlab, GIT.  Вам предстоит:  Поддержка пользователей в контексте подвоза данных: реакция, анализ проблемы, фикс или задача на фикс; Анализ возникающих проблем: мониторинг, алертинг, анализ логов сервисов, анализ и чтение логов подов в k8s, анализ метрик БД и сервисов; Помощь пользователям в оптимизации запросов, таблиц и загрузок в БД Verica; AirFlow: написание/рефакторинг обобщенного кода для привоза данных; Рефакторинг ДАГов AirFlow; Реализация фабрики AirFlow: сетап, грейсфул апдейт; Поддержка работоспособности AirFlow: введение метрик, борды, алертов; Развитие AirFlow: кастомные операторы, common code; Внедрение dev/stage контура AirFlow, локальная отладка, e2e тесты; Документирование разрабатываемых решений; Проведение code review; Развивать базу знаний по работе с базой данных для бизнес-пользователей.  Мы ожидаем:  Уверенное владение Python; Опыт работы с Airflow (Celery Executor, k8s); Опыт работы с аналитическими БД (Vertica/Greenplum/BigQuery/Redshift/Synapse); Опыт работы с СУБД (Postgre SQL/MS SQL); Опыт работы с Kubernetes, GIT; Опыт работы с KV-хранилищами.  Будет плюсом:  Опыт работы с Kafka, Prometheus, Grafana; Владение Scala; Опыт работы с Hadoop (hdfs, yarn, hive).  Мы предлагаем:  Динамичный и быстроразвивающийся бизнес, ресурсы, возможность сделать вместе сделать лучший продукт на рынке e-commerce; Свободу действий в принятии решений; Достойный уровень заработной платы; Профессиональную команду, которой мы гордимся; Возможность развиваться вместе с нашим бизнесом. "
"68038362","Ozon","Data Engineer, Платформа, Обработка и хранение данных","False","None","None","От 3 до 6 лет","Полный день","['IT', 'Хранение и обработка данных']","Команда DE Platform строит инструменты интеграции данных, которые выступают как мост между бизнесом и BI. Эти данные используются во множестве процессов внутри компании. Экспертиза команды также позволяет решать нетривиальные задачи по гарантиям доставки, технологическому развитию как инструментов, так и инфраструктуры вокруг. Наш стек:  Scala, Akka-Stream, Cats, Spark, Python; Vertica, MS SQL, ClickHouse, Hadoop (hadoop3, hdfs, yarn, hive) и др.; Airflow (Celery Executor, k8s); Gitlab CI/CD, Kubernetes; Kafka, Prometheus, Grafana; Gitlab, GIT.  Вам предстоит:   Поддержка и развитие существующих инструментов на базе Hadoop (hadoop3, hdfs, yarn, hive);   Разработка spark приложений для обработки данных;   Поддержка и развитие существующих решений на Kafka;   Внедрение полноценного dev/stage контура для сервисов;   Переписать/отрефакторить легаси код по довозу данных;   Рефакторинг метрик, борды, алертов;   Внедрение синка и подвоза данных multiDC;   Внедрение/рефакторинг e2e тестирования сервисов;   Участие в проектировании архитектуры разрабатываемых решений;   Участие в выборе используемых технологий и компонентов;   Внедрение и сопровождение технических решений;   Документирование разрабатываемых решений;   Проведение code review.   Мы ожидаем:  Уверенное владение Scala; Опыт работы с Hadoop (hdfs, yarn, hive); Опыт разработки многопоточных приложений; Опыт работы с СУБД (Postgre SQL/MS SQL); Опыт работы с аналитическими БД (Vertica/Greenplum/BigQuery/Redshift/Synapse); Опыт работы с Kubernetes, Kafka, git;  Опыт работы с KV-хранилищами.   Будет плюсом:  Опыт работы с Prometheus, Grafana; Опыт работы с Airflow (Celery Executor, k8s).  Мы предлагаем:  Динамичный и быстроразвивающийся бизнес, ресурсы, возможность сделать вместе лучший продукт на рынке e-commerce; Свободу действий в принятии решений; Достойный уровень заработной платы; Профессиональную команду, которой мы гордимся; Возможность развиваться вместе с нашим бизнесом. "
"68784510","МТС","DevOps engineer в МТС Метрику (Big Data)","False","None","None","От 3 до 6 лет","Полный день","['Python', 'Docker', 'Big Data', 'CI/CD', 'Go', 'Linux']","Big Data МТС – место, где телеком данные превращаются в реально работающие IT-продукты. Мы создали и протестировали несколько десятков сервисов. Самые успешные из них уже стали частью экосистемы МТС. Например, МТС Маркетолог, рекомендации в KION (МТС ТВ), услуга “Кто звонит?” или Спам blacklist. Кого мы ищем? Обязательно:  знание python или go; знание реляционных и нереляционных баз данных: работа под нагрузкой, репликация, резервирование, профилирование; опыт работы с веб-серверами и балансировщиками нагрузки; опыт работы с системами управления конфигурацией; опыт работы с системами CI/CD и Git; дистрибутив Linux; сети (понимание работы IP/TCP/HTTP), умение диагностировать и решать проблемы; docker.  Желательно:  иметь опыт построения высоконагруженных сервисов.  Что предстоит делать?  обеспечивать оснащение необходимыми мониторингами, графиками и CI/CD для новых и/или существующих компонентов платформы; развертывать кластера хранилищ данных и очередей сообщений; масштабировать всю систему в связи с постоянно растущей нагрузкой; автоматизировать ручную работу чтобы не тратить время на рутину.  Сейчас мы ищем DevOps engineer в МТС Метрику МТС Метрика - это real-time business intelligence (RTBI)-платформа, предназначенная для анализа поведения пользователей в клиентских приложениях. Единая система сбора поведенческих данных о пользователях со всех сайтов и приложений экосистемы МТС. Помогает оценивать и улучшать клиентский опыт. Для оценки клиентского опыта платформа предоставляет инструмент для интерактивной аналитики. Для улучшения может предоставлять потребителям (рекомендательные движки и ML-модели) уточненную и обогащенную информацию. Мы ищем DevOps-инженеров, которые помогут нам с «нуля» создать и развивать RTBI-платформу. У нас вы сможете принять участие в разработки event-based архитектуры платформы, поработать с streaming технологиями, технологиями Big Data, in-memory базами данных и аналитическими хранилищами. Наши преимущества:  опытные и отзывчивые коллеги, которые готовы ответить на любые вопросы; платформа создается с «нуля», никакого легаси; сложный и большой проект, в котором можно многому научиться.  Что вы найдете в команде Big Data? Стек технологий: Мы пишем на Java 17. Используем Apache Kafka как очередь сообщений. В качестве in-memory базы – Aerospike. Аналитическое хранилище - ClickHouse, а накопление истории происходит в Apache Hive. В целевом виде вся инфраструктура должна работать в k8s. Системы мониторинга - Prometheus, Grafana и логирования - ELK/EFK, системы управления конфигурацией - Ansible, Helm, Vault. Команда: на данным момент в Центре BigData 10 java разработчиков. Каждый работает на своем продукте в кроссфункциональной команде (где есть аналитики, QA инженеры, DevOps, DE, DS и т.д.). Условия: каждый месяц - аванс и зарплата, дважды в год - премия. ДМС + стоматология, корпоративная связь, специальные предложения от партнеров и друзей МТС, отпуск 31 день в год. Выдаем 16” MacBook Pro или Dell на выбор. Есть ли обучение?  Локальные и международные конференции, митапы. Корпоративный университет МТС и масштабная виртуальная библиотека. А ещё мы регулярно обмениваемся опытом на совместных синках с лидами экспертизы  Какой график? Гибкое начало рабочего дня в промежутке с 8 до 11. Есть возможность работать несколько дней вне офиса по договоренности с командой. Сколько этапов при отборе? Не более трех:  HR + первое тех. интервью с лидом направления Тестовое задание/второе интервью - по необходимости Собеседование с PO и командой, выбор кандидатом проекта "
"67924416","СБЕР","Senior Data Scientist/ML Engineer SberDevices","False","None","None","От 3 до 6 лет","Полный день","['Python', 'NLP', 'Deep Learning', 'Linux', 'Docker']","SberDevices - инновационное направление компании, которое создает умные устройства, виртуальные ассистенты и другие продукты в области NLP, gamedev, computer vision.Команда поиска ищет Senior Data Scientist/ML Engineer. Мы создаем поисковый движок, который работает со множеством различных данных. Например, у нас есть поиски по видео, музыке и другим доменам. Мы работаем с самыми совеременными технологиями, используем Hadoop, применяем разнообразные методы машинного обучения, улучшаем множество продуктов экосистемы Сбербанка, помогая пользователям найти то, что они хотели.Если ты всегда хотел работать на стыке науки и технологии, и готов не просто решать технически сложные задачи, но и глубоко погрузиться в тематику машинного обучения, начиная от внедрения уже обученных моделей и заканчивая самостоятельной реализацией новых алгоритмов из последних статей -- то ты наш идеальный кандидатОбязанности:  участие в разработке поискового движка разработка и оптимизация алгоритмов машинного обучения внедрение в продакшн ML решений  Требования:  уверенное владение Python на рабочем уровне хорошее знание классических алгоритмов и структур данных базовые знания и желание развиваться в области машинного обучения и анализа данных  Будет плюсом:  владение C++ на рабочем уровне опыт реализации и внедрения алгоритмов машинного обучения понимание принципов работы высоконагруженных интернет-сервисов знакомство с алгоритмами обработки больших данных и парадигмой MapReduce  Условия:  Самые инновационные, амбициозные проекты и задачи Профессиональное обучение, семинары, тренинги, конференции, корпоративная библиотека ДМС, страхование жизни Свободный дресс-код Гибкий график для оптимального баланса работы и личной жизни Льготные кредиты и корпоративные скидки Конкурентная компенсация (оклад и премии по результатам деятельности).   "
"68666164","Ozon","Data Engineer (Логистика, Отдел инфраструктуры и качества данных)","False","None","None","От 3 до 6 лет","Полный день","['Python', 'SQL', 'Аналитическое мышление', 'ClickHouse', 'Spark', 'it', 'разработка логистики']","Мы ищем опытного дата-инженера в команду аналитики логистики.Наша логистика доставляет миллионы товаров в сутки. И чтобы делать это быстро и качественно, всем точкам нашей сети доставки важно получать оперативную и качественную аналитику. Это одна из наших задач – создавать realtime аналитику для тысяч наших пользователей на сортировочных центрах.Но также мы создаем сервис мониторинга доступности доставки на основании статистики, разрабатываем сервис оценки вероятного распределения посылок по паллетам, и в целом решаем большое количество разнообразных аналитических задач, которые появляются с удивительной скоростью. Стек технологий:Python, Spark, Pandas, MS SQL, Airflow, Clickhouse, Grafana, Superset Что предстоит делать:  Создавать realtime отчетность для сортцентров; Настраивать интеграцию данных из большего числа разношерстных источников данных; Придумывать архитектуры для решения новых аналитический задач; Создавать прототипы решений; Создавать стабильные отказоустойчивые и масштабируемы аналитические сервисы.  Для нас важно:  Отличное знание SQL; Опыт в оптимизации производительности запросов и ETL процессов; Знание Python+Pandas; Уверенных технический бекграунд, ты представляешь себе что такое контейнеры, k8s, kafka и т.д.; Опыт от 3 лет.  Будет плюсом:  Опыт работы с Airflow; Вы чувствуете себя комфортно с Clickhouse; Вы делали проекты на Spark. "
"69613826","МТС Банк","Middle/Senior Data Engineer","False","None","None","От 3 до 6 лет","Полный день","['Python', 'SQL', 'Git', 'Atlassian Confluence', 'Atlassian Jira']","Мы собираем команду, которая будет заниматься разработкой платформы клиентских данных (CDP) с нуля. Для построения Customer Data Platform мы планируем использовать СУБД Greenplum и ClickHouse. Наша идея - создать платформу клиентских данных, которая будет помогать всем без исключения людям в компании делать их работу проще и эффективнее. Наша цель - превратить хаос в порядок, автоматизировать рутинные задачи и подготовить данные для удобного анализа и аргументированного принятия бизнес-решений, будь то решение о стратегии компании по отношению к клиентам или о развитии конкретного продукта. Подразделение Core-разработки будет заниматься написанием ядра платформы, разработкой фреймворков и инструментов, которые в дальнейшем помогут системным аналитикам просто и быстро загружать данные в платформу, а пользователям - эффективно использовать его и узнавать информацию о данных не только внутри CDP, но и за его пределами. Ты будешь писать те самые механизмы, которые являются сердцем платформы. Для воплощения этой идеи мы ищем людей по-настоящему влюбленных в инженерию данных. СТЭК:  Database Systems: ClickHouse, GreenPlum; DataFlow/ETL: Airflow, DBT; Continuous Integration: Gitlab CI; IssueTracking/KnowledgeBase: Jira, Confluence.  ТЕБЕ ПРЕДСТОИТ:  Создание и доработка фреймворков, обеспечивающих загрузку данных в клиентскую платформу данных. МЫ ОЖИДАЕМ, ЧТО ТЫ:  Имеешь опыт промышленного программирования на Python; Имеешь опыт создания архитектуры потоков данных и инструментов для управления ими; Знаешь SQL как родной язык; Имеешь опыт работы с Airflow; Имеешь продвинутые навыки работы с Git; Имеешь опыт покрытия кода unit-тестами; Понимаешь принципы работы СУБД.  БУДЕТ ЗДОРОВО, ЕСЛИ ТЫ: Имеешь опыт работы с DBT, MPP-системами (ClickHouse, GreenPlum), Django, Flask/FastApi. В МТС Финтех тебя ждет культура творчества и идей: мы всегда ищем новые решения, задаем тренды и не боимся быть не похожими, постоянная прокачка экспертизы, профессиональное развитие и свобода действовать и нести ответственность за результат. А еще:  Достойная заработная плата; Аgile-культура: продуктовые команды, Scrum и Kanban, демо-дни, внутренние митапы и таунхоллы; Доступ к корпоративной библиотеке Alpina, Корпоративный Университет и программы развития; Отсутствие дресс-кода, гибкий формат работы; Специальный тариф на мобильную связь для тебя и близких и предложения от партнеров банка и МТС; Забота о тебе и твоей семье: ДМС со стоматологией, сессиями с психологом и госпитализацией, страхование от несчастных случаев и болезней 24/7 и страхование выезжающих за рубеж. К ДМС можно подключить детей и родственников. А еще доступ к телемедицине BestDoctor; Мы за спорт: вместе тренируемся в MTS Running Club. Ты можешь присоединиться к корпоративным спортивным командам и занятиям йогой; Поддержка сотрудников: программа материальной помощи в различных жизненных ситуациях.   "
"68357777","Элемент Лизинг","Middle Data Scientist / Middle Machine Learning Engineer","True","150000","None","От 1 года до 3 лет","Полный день","['Python', 'Английский язык', 'SQL', 'Математическая статистика', 'Статистика']","Обязанности:  Разработка скоринговых и других моделей, применимых к задачам лизинга. Участие в процессе внедрения скоринговой модели в бизнес-процессы компании. Мониторинг и контроль за результатами работы скоринговой модели после внедрения, принеобходимости - дообучение модели.  Требования:  Опыт индустриальной разработки моделей от 2-х лет Умение писать структурированный и читабельный код на Python, использование линтеров иавтоформаттеров, готовность рефакторить уже имеющиеся проектные наработки, владение ООП. Понимание и умение применять алгоритмы классического ML (линейные модели, деревья) Английский язык (B2 и выше в области чтения и восприятия на слух; письмо и говорениене используются в рабочей практике) Математическая статистика (включая проверку статистических гипотез) Теория вероятностей (от комбинаторики до Теоремы Байеса) SQL (JOINs, GROUPBY, WHERE / HAVING, subqueries, CASE, WINDOW)  Условия:  Официальное трудоустройство по ТК РФ; ДМС (включая стоматологию + выезд за границу); Зарплата по результатам собеседования; Место работы: г. Москва, Кутузовский пр. 36 стр. 41. "
"67786460","SberAutoTech","MLOps Engineer (AI Driving Data)","False","None","None","От 3 до 6 лет","Полный день","['Linux', 'Python', 'Обучение и развитие', 'Организация мероприятий', 'Деловое общение', 'MLFlow', 'Kuberflow']","У команды SberAutoTech есть цель, и это – революция в мире автопрома в самом ближайшем будущем. Нам срочно нужны единомышленники! Мы осознаем, что прорыв всегда делает меньшинство, потому что не все готовы рискнуть творить историю и двигать вперед целую индустрию. Общающиеся между собой автомобили, сервисы и приложения, беспилотный транспорт и электрокары – это даже не будущее, а настоящее! Следующий шаг – сделать такие автомобили обычным явлением в нашей жизни. Если интересны технологии на стыке автопрома и интернета, если мечтаешь быть причастным к созданию чего-то нового и масштабного – присоединяйся к нашей команде. Мы ищем MLOps~a, который поможет нам улучшить качество ML-решений. В твоей зоне ответственности будет выбор и внедрение инструментов MLOps, поддержка инфраструктуры для обучения моделей, создание и поддержка ML-пайплайнов. Ты будешь тесно сотрудничать с командой разработки моделей для self-driving. Результаты твоей работы будут напрямую влиять на скорость развития беспилотной технологии. Наш стек: MLFlow, Kubeflow, DVC, ETL, CI/CD/CT Проект: AI Driving Data. Тебе предстоит организовать обработку данных, которые генерируют наши беспилотники, а также участвовать в создании хранилища для этих данных. Классы задач, которые необходимо будет выполнять:  Внедрить и развивать платформу полного жизненного цикла ML-моделей; Собирать и запускать ML-пайплайны; Поддерживать модели, наборы данных и параметры в системах CI/CD/CT; Поддерживать модели в проде, решать сложные проблемы.  Что мы ждем от кандидата:  Опыт разработки на Python от 3х лет; Знание алгоритмов и методов ML; Опыт построения инфраструктуры работы с данными; Опыт организации CI/CD.  Будет плюсом++  Опыт разработки на С++; Опыт разработки ML моделей в роли DS/DE; Знание лучших практик DevOps.  Мы ценим своих сотрудников и предлагаем:  Крутой технологический домен; Работа в команде топовых разработчиков, возможность разрабатывать уникальные и крупные проекты масштаба нашей страны; Конкурентные условия труда (белая индексируемая заработная плата, оклад+годовая премия); График работы – стандартный, но с гибким подходом к началу/окончанию рабочего дня. На период онбоардинга - офисный формат; Доступ к огромным возможностям повышения квалификации в СберУниверситете и Виртуальной школе, а также к другим формам обучения; Возможность посещения (как в качестве слушателя, так и в качестве выступающего) всероссийских и международных конференций; ДМС со стоматологией для сотрудников с первого дня и скидки на медицинскую страховку для родственников; Обеды для сотрудников, бесплатный кофе и другие напитки в кафетерии; Зарплатный проект, бесплатная подписка Сберпрайм+, субсидия на ипотеку и другие продукты Экосистемы Сбера на особых условиях; Широкий спектр дисконт–программ, скидок и привилегий от компаний-партнеров; Возможность уже сейчас использовать беспилотный транспорт, чтоб добраться до работы от МЦК ЗИЛ. Большой и комфортный офис со спортзалом, кинозалом, библиотекой, столами для пинг-понга, кафе для сотрудников; Льготное кредитование в Сбербанке – возможность пользоваться премиальными продуктами Банка на специальных условиях. "
"69570270","Актив Матрикс","Программист С++ (data engineer)","True","210000","None","От 1 года до 3 лет","Полный день","['Python', 'Linux', 'Big Data', 'ClickHouse', 'C++', 'GCC', 'Networking']","Компания уже семь лет занимается разработкой торговых роботов для наших клиентов, которые торгуют на крупнейших биржах мира. Мы анализируем рыночные данные, строим модели, автоматизируем процессы. Для поиска успешных торговых стратегий используются последние методы машинного обучения и топовое оборудование. Расширяющаяся ИТ-инфраструктура требует использования самых современных технологий и решения амбициозных задач связанных с высокопроизводительными вычислениями, низкими задержками и большими данными. Мы небольшая команда квантов и программистов с неформальной культурой. Занимаем офис в центре Москвы в одном из зданий около Красного Октября.   Что предстоит делать:   Разрабатывать высокопроизводительные программы для обработки и конвертации больших данных   Создавать новые и поддерживать существующие приложения для построения аналитики по большому объему рыночных данных   Проектировать и разрабатывать хранилища данных и средства их визуализации   Следить за целостностью и качеством данных   Отвечать за хранение, выгрузку и миграцию данных     Требования к кандидату:   Знание C++ (работаем с 17 стандартом)   Опыт работы с gcc/clang (понимание основных принципов работы, компиляция, линковка, оптимизации, поиск ошибок с помощью GDB и санитайзеров)   Опыт работы с Linux, умение писать приложения для него   Знание основных принципов работы сетевых протоколов (Ethernet, IP, TCP, UDP)   Знание базовых алгоритмов и структур данных (оценка сложности, подбор структуры данных для конкретной задачи)   Английский язык на уровне чтения технической литературы     Будет плюсом:   Знание основ SQL, опыт работы с базами данных, в частности ClickHouse   Знание Python, Rust   Знание CMake   Опыт работы с Docker   Опыт в написании парсеров/анализаторов бинарных данных   Знание форматов и способов хранения Big Data (в частности временных рядов)   Опыт удалённой работы     Что мы предлагаем:   конкурентная заработная плата и ежегодный бонус   после прохождения испытательного срока свободный график посещения офиса и варианты удаленной работы   комфортный офис в 5 минутах пешком от метро Полянка   техника на ваш выбор (Mac/Windows/Linux)  "
"69560595","ЗащитаИнфоТранс, ФГУП","Data Scientist / ML Engineer","False","None","None","От 3 до 6 лет","Полный день","['Python', 'SQL', 'Математическая статистика', 'Анализ данных', 'Базы данных']","ФГУП «ЗащитаИнфоТранс» — ведущий системный интегратор транспортной отрасли России в области транспортной и информационной безопасности, а также внедрения информационных технологий. Компания решает задачи, связанные с обеспечением безопасности на транспорте, информационной и физической защитой объектов, их аттестацией по требованиям безопасности, аудитом и обслуживанием. Головной офис компании расположен в Москве, также имеются филиалы в г. Санкт-Петербурге, Екатеринбурге, Хабаровске. Обязанности:   Разработка и внедрение решений с использованием алгоритмов машинного обучения.   Разработка и внедрение решений с использованием методов линейного программирования, целочисленного программирования, программирования в ограничениях.   Самостоятельная постановка гипотез, анализ данных, дизайн фич, проведение и оценка экспериментов;   Вывод моделей в продакшн (совместно с другими специалистами).   Требования:   Математическая база в области основ линейной алгебры, методов оптимизации, теории вероятностей и математической статистики.   Опыт программирования на Python и знание основных библиотек для работы с данными. (sklearn, pandas, numpy, scipy, matplotlib, XGBoost, LightGBM и т.д.).   Уверенное знание и опыт работы с основными алгоритмами классического ML (линейные модели, деревья, ансамблевые алгоритмы и тд).   Опыт построения оптимизационных моделей в CPLEX, Gurobi или аналогичных решениях;   Опыт работы с SQL, понимание аналитики данных.   Опыт полного цикла разработки аналитических моделей: от проектирования архитектуры решения до выведения в продакшен.   Условия:  Официальное оформление в соответствии с ТК РФ (оплата больничных и отпусков). Дополнительное материальное вознаграждение к отпуску ежегодно. График работы 5/2 (гибрид), возможность выбрать время начала рабочего дня. Возможность дополнительного обучения за счет компании и карьерного роста. Уютный офис с зонами отдыха и кухней, комфортное рабочее место, офис находится в пешей доступности от метро Тульская на территории Даниловской Мануфактуры. "
